{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import operator as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math, itertools\n",
    "import statistics\n",
    "import json\n",
    "import hdbscan\n",
    "\n",
    "# Sklearn imports\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Succ_Call_Tracker():\n",
    "    def __init__(self):\n",
    "        self.calls = [0]\n",
    "        self.succ = [0]\n",
    "        self.curr_s = 0\n",
    "        self.curr_c = 0\n",
    "        \n",
    "    def add_calls(self, num_calls):\n",
    "        self.curr_c += num_calls\n",
    "        \n",
    "    def add_succ(self):\n",
    "        self.curr_s += 1\n",
    "    \n",
    "    # The following TWO methods might not be useful ..\n",
    "    def add_call_save(self, num_calls):\n",
    "        self.add_calls(int(num_calls))\n",
    "        self.calls.append(self.curr_c)\n",
    "    def add_succ_save(self):\n",
    "        self.add_succ()\n",
    "        self.succ.append(self.curr_s)\n",
    "        \n",
    "    def save(self):\n",
    "        self.calls.append(self.curr_c)\n",
    "        self.succ.append(self.curr_s)\n",
    "        \n",
    "    def get_calls(self):\n",
    "        return self.curr_c\n",
    "        \n",
    "    def get_auc(self):\n",
    "        return metrics.auc(self.calls, self.succ)\n",
    "    \n",
    "    def get_succ_calls_score(self):\n",
    "        return (self.calls, self.succ, self.get_auc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 10, 55], [0, 1, 2], 72.5)\n"
     ]
    }
   ],
   "source": [
    "t = Succ_Call_Tracker()\n",
    "t.add_calls(10)\n",
    "t.add_succ()\n",
    "t.save()\n",
    "t.add_call_save(45)\n",
    "t.add_succ_save()\n",
    "print(t.get_succ_calls_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions Across All Methods\n",
    "\n",
    "def load_file(data_file_path):\n",
    "    data_df = pd.read_csv(data_file_path, delimiter=\";\")\n",
    "    return data_df\n",
    "  \n",
    "    \n",
    "def plot_graph_new(results, max_calls, list_passed, title, name = \"1\"):\n",
    "    x_pts = [i+1 for i in range(0, max_calls)]\n",
    "    if list_passed:\n",
    "        y_pts = results\n",
    "    else:    \n",
    "        y_pts = [results[i]['expected'] for i in range(0, max_calls)]\n",
    "    print(y_pts)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_pts, y_pts, linewidth=2)\n",
    "    plt.xlabel(\"Call Number\")\n",
    "    plt.ylabel(\"Success Per Call Rate\")\n",
    "    plt.ylim(0, 0.4)\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))\n",
    "#     plt.axvline(x=0, color =\"black\", linewidth=1)\n",
    "#     plt.axhline(y=0, color =\"black\", linewidth=1)\n",
    "    plt.xticks(np.arange(1, max_calls+1, 1))\n",
    "#     plt.show()\n",
    "    plt.savefig(str(name) + \".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def div(a,b):\n",
    "    if int(b) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return a/b\n",
    "    \n",
    "\n",
    "# Used for creating all possible combinations of the features.\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = iterable\n",
    "    return itertools.chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "\n",
    "\n",
    "def construct_dict(feature_comb):\n",
    "    new_dict = {}\n",
    "    new_dict['education'] = convert(feature_comb[0])\n",
    "    new_dict['job'] = convert(feature_comb[2])\n",
    "    new_dict['marital'] = convert(feature_comb[1])\n",
    "    new_dict['default'] = convert(feature_comb[3])\n",
    "    new_dict['loan'] = convert(feature_comb[4])\n",
    "    new_dict['housing'] = convert(feature_comb[5])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# This is the new metric (success per call rate).\n",
    "def compute_expected_succ_per_call_rate_feature_set(fs_df, no_calls_considered):\n",
    "    expected_values_call_nums = []\n",
    "    for i in range(1, no_calls_considered + 1):\n",
    "        expected_values_call_nums.append({'succ':0, 'total_calls':0, 'expected':0.0})\n",
    "        for index, row in fs_df.iterrows():\n",
    "            no_calls = row['campaign']\n",
    "            if no_calls <= i:\n",
    "                if row['y'] == \"yes\":\n",
    "                    expected_values_call_nums[i-1]['succ'] += 1\n",
    "                expected_values_call_nums[i-1]['total_calls'] += no_calls\n",
    "            else:\n",
    "                expected_values_call_nums[i-1]['total_calls'] += i\n",
    "    for loc, item in enumerate(expected_values_call_nums):\n",
    "        expected_values_call_nums[loc]['expected'] = div(item['succ'], item['total_calls'])\n",
    "    return expected_values_call_nums\n",
    "\n",
    "\n",
    "def compute_optimal_call_no(results):\n",
    "    max_loc = max(range(len(results)), key=lambda index: results[index]['expected'])\n",
    "    if max_loc == 0 and results[max_loc]['expected'] == 0.0:\n",
    "        return -1\n",
    "    return max_loc\n",
    "\n",
    "\n",
    "# Given a dictionary of what attributes comprise a feature set, we can get all rows corresponding to this feature set.\n",
    "def extract_rows_feature_set(fs_df, feature_labels = {'education':['tertiary', 'unknown'], \n",
    "                                                      'job':['management', 'technician', 'blue-collar'], \n",
    "                                                      'marital':['single'], 'default':['no'], \n",
    "                                                      'housing':['no'], 'loan':['no']}):\n",
    "    for key in feature_labels:\n",
    "        feature_labels_query_str = ''\n",
    "        arr = feature_labels[key]\n",
    "        for label in arr:\n",
    "            feature_labels_query_str += (key + ' == \"'+ label + '\" | ')\n",
    "        feature_labels_query_str = feature_labels_query_str[:-3]\n",
    "        fs_df = fs_df.query(feature_labels_query_str)\n",
    "    return fs_df\n",
    "\n",
    "\n",
    "def find_matching_attribute_comb(row_value, all_combs):\n",
    "    query = None\n",
    "    for comb in all_combs:\n",
    "        for item in comb:\n",
    "            if item == row_value:\n",
    "                query = comb\n",
    "    return query\n",
    "\n",
    "\n",
    "def compute_metric(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += row['campaign']\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "def compute_metric_2(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += min(row['campaign'], )\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} == '{1}'\".format(attrib, value)\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute_range(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    query_strings = []\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} >= {1} & {2} < {3}\".format(attrib, value[0], attrib, value[1])\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "        query_strings.append(v_query)\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals, query_strings\n",
    "\n",
    "\n",
    "def find_combinations(sub_attributes, ratios):\n",
    "    num_iter = len(ratios)\n",
    "    sil_scores = []\n",
    "    # Making use of the K-Means algorithm ... number of centroids are from 2 to n-1.\n",
    "    for clust_num in range(2, num_iter):\n",
    "        kmeans = KMeans(n_clusters = clust_num)\n",
    "        kmeans.fit(ratios.reshape(-1,1))\n",
    "        results = kmeans.labels_\n",
    "        sil_scores.append((silhouette_score(ratios.reshape(-1,1), results, metric='euclidean'), results, clust_num))\n",
    "#     print(sil_scores)\n",
    "    # We make use of the silhouette score to determine the ideal number of centroids.\n",
    "    sorted_sil_scores = sorted(sil_scores, key=lambda x: x[0], reverse = True)\n",
    "    # We then use this ideal number of centroids to determine which sub attributes should be aggregated.\n",
    "    joined_sub_attributes = []\n",
    "    for i in range(0, sorted_sil_scores[0][2]):\n",
    "        joined_sub_attributes.append([])\n",
    "    join_list = sorted_sil_scores[0][1]\n",
    "    for index, value in enumerate(join_list):\n",
    "        pos = join_list[index]\n",
    "        joined_sub_attributes[pos].append(sub_attributes[index])\n",
    "    return_joined_sub_attributes = []\n",
    "    for arr in joined_sub_attributes:\n",
    "        similar_els_gp = []\n",
    "        for item in arr:\n",
    "            similar_els_gp.append(str(item))\n",
    "        return_joined_sub_attributes.append(similar_els_gp)\n",
    "#     print(return_joined_sub_attributes)\n",
    "    return return_joined_sub_attributes\n",
    "\n",
    "# The following is the format of the way in which this method should be called.\n",
    "# find_combinations(['a', 'b', 'c', 'd'], np.array([1, 4, 7, 90]), \"job\").\n",
    "\n",
    "def find_all_cust_feature_set(fs, df):\n",
    "    comb = {\n",
    "        'education':fs[0], \n",
    "         'job':fs[1], \n",
    "         'marital':fs[2], \n",
    "         'default':fs[3], \n",
    "         'loan':fs[4], \n",
    "         'housing':fs[5]\n",
    "    }\n",
    "    res_1 = df.query(fs[6])\n",
    "    res_2 = res_1.query(fs[7])\n",
    "    res_final = extract_rows_feature_set(res_2, comb)\n",
    "    return res_final\n",
    "\n",
    "\n",
    "def construct_hull_points(results, max_calls):\n",
    "    pts = []\n",
    "    for x in range(0, max_calls):\n",
    "        s = results[x]['succ']\n",
    "        c = results[x]['total_calls']\n",
    "        pts.append([c,s])\n",
    "#     print(\"Num points is \", len(pts))\n",
    "    pts = np.array(pts)\n",
    "    try:\n",
    "        hull = ConvexHull(pts)\n",
    "        verts = hull.vertices\n",
    "#         print(pts)\n",
    "#         plt.plot(pts[:,0], pts[:,1], 'o')\n",
    "#         for simplex in hull.simplices:\n",
    "#             plt.plot(pts[simplex, 0], pts[simplex, 1], 'k-')\n",
    "        if not np.isin(max_calls - 1, verts):\n",
    "            verts = np.append(max_calls - 1, verts)\n",
    "        verts = np.sort(verts)\n",
    "        return verts.tolist()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def gradient_update(key, fs_pick):\n",
    "    fs = fs_pick[key]\n",
    "    fs_results = fs['results']\n",
    "    hull_pts = fs['hull_points']\n",
    "    loc = fs['loc']\n",
    "    max_loc = fs['max_num_pts']\n",
    "    grad = 0.0\n",
    "    if loc <= max_loc:\n",
    "        if loc == 0:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'], fs_results[hull_pts[loc]]['total_calls'])\n",
    "        else:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'] - fs_results[hull_pts[loc-1]]['succ'] , fs_results[hull_pts[loc]]['total_calls'] - fs_results[hull_pts[loc-1]]['total_calls'])\n",
    "        fs_pick[key]['grad'] = grad\n",
    "    else:\n",
    "        fs_pick[key]['finished'] = True\n",
    "\n",
    "        \n",
    "def get_features(row, feature_names):\n",
    "    fs = []\n",
    "    for index, val in enumerate(feature_names):\n",
    "        if int(row[index]) == 1:\n",
    "            fs.append(val)\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell holds functions that are utilized by each of the methods defined.\n",
    "\n",
    "def group_age(row, age_ranges):\n",
    "#     print(age_ranges)\n",
    "    age = int(row['age'])\n",
    "    age_val = None\n",
    "    for index, age_range in enumerate(age_ranges):\n",
    "        if op.ge(age, age_range[0]) and op.le(age, age_range[1]):\n",
    "            age_val = index + 1\n",
    "    if age_val == None:\n",
    "        print(\"Failed Assignment for age: \", age)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'age'] = age_val\n",
    "    return age_val\n",
    "        \n",
    "\n",
    "def group_balance(row, balance_ranges):\n",
    "#     print(balance_ranges)\n",
    "    bal = int(row['balance'])\n",
    "    bal_val = None\n",
    "    for index, balance_range in enumerate(balance_ranges):\n",
    "        if op.ge(bal, balance_range[0]) and op.le(bal, balance_range[1]):\n",
    "            bal_val = index + 1\n",
    "    if bal_val == None:\n",
    "        print(\"Failed Assignment for balance: \", bal)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'balance'] = bal_val\n",
    "    return bal_val\n",
    "\n",
    "\n",
    "def group_feature(df, col_name, func, ranges):\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, col_name] = func(row, ranges)\n",
    "\n",
    "def compute_ratio_all_users(df, train_indicies):\n",
    "    ratio_values = []\n",
    "    for val in train_indicies:\n",
    "        row = df.iloc[val]\n",
    "        if row['y'] == \"yes\":\n",
    "            ratio_values.append((val, div(1, row['campaign'])))\n",
    "        else:\n",
    "            ratio_values.append((val, 0.0))\n",
    "    return ratio_values\n",
    "\n",
    "\n",
    "def compute_freq_percentage(mappings):\n",
    "    total = 0\n",
    "    for user_mapping in mappings.keys():\n",
    "        total += mappings[user_mapping]['freq']\n",
    "    for user_mapping in mappings.keys():\n",
    "        mappings[user_mapping]['percentage'] = div(mappings[user_mapping]['freq'], total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each function represents each method attempted.\n",
    "\n",
    "def call_everyone(test_df):\n",
    "    print(\"Call all Customers Approach\")\n",
    "    call_check_points = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, 96500, 97000, 97500, 98000, 98500, 99000, 99500, 100000, 100500, 101000, 101500, 102000, 102500, 103000, 103500, 104000, 104500, 105000, 105500, 106000, 106500, 107000, 107500, 108000, 108500, 109000, 109500, 110000, 110500, 111000, 111500, 112000, 112500, 113000, 113500, 114000, 114500, 115000, 115500, 116000, 116500, 117000, 117500, 118000, 118500, 119000, 119500, 120000, 120500, 121000, 121500, 122000, 122500, 123000, 123500, 124000, 124500, 125000, 125500, 126000, 126500, 127000, 127500, 128000, 128500, 129000, 129500, 130000, 130500, 131000, 131500, 132000, 132500, 133000, 133500, 134000, 134500, 135000, 135500, 136000, 136500, 137000, 137500, 138000, 138500, 139000, 139500]\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    cp_loc = 0\n",
    "    res = test_df.reindex(np.random.permutation(test_df.index))\n",
    "    for loc, row in res.iterrows():\n",
    "        if sc_tracker.get_calls() >= call_check_points[cp_loc]:\n",
    "            cp_loc += 1\n",
    "            sc_tracker.save()\n",
    "        sc_tracker.add_calls(row['campaign'])\n",
    "        if row['y'] == \"yes\":\n",
    "            sc_tracker.add_succ()\n",
    "    sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score(), sc_tracker.get_calls()\n",
    "\n",
    "\n",
    "def greedy_approach(combs_to_consider):\n",
    "    print(\"Greedy Approach\")\n",
    "    persons_to_call_overall = {k: v for k, v in sorted(combs_to_consider.items(), key=lambda fs: fs[1]['overall_rate'], reverse = True)}\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    result_ratios = []\n",
    "    for key in persons_to_call_overall.keys():\n",
    "        for loc, cust in persons_to_call_overall[key]['fs_customers'].iterrows():\n",
    "            sc_tracker.add_calls(cust['campaign'])\n",
    "            if cust['y'] == \"yes\":\n",
    "                sc_tracker.add_succ()\n",
    "        sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score()\n",
    "\n",
    "\n",
    "def convex_hull(fs_pick, num_calls):\n",
    "    print(\"Gradient Ascent Approach\")\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    print(\"Performing initial update .. \")\n",
    "    for key in fs_pick.keys():\n",
    "        gradient_update(key, fs_pick)\n",
    "    print(\"Perforiming sort .. \")\n",
    "    # Sort based on gradient.\n",
    "    optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    # Call best feature set, update gradient for this feature set and re-sort all feature sets.\n",
    "    # Rinse and repeat!\n",
    "    print(\"Finished sort .. in while loop \")\n",
    "    while(sc_tracker.get_calls() <= num_calls):\n",
    "        best_loc = 0\n",
    "        while(best_loc < len(optimal_choices) and optimal_choices[best_loc][1]['finished'] == True):\n",
    "            best_loc += 1\n",
    "        if best_loc == len(optimal_choices):\n",
    "            break\n",
    "        fs_key = optimal_choices[best_loc][0]\n",
    "        fs_data = optimal_choices[best_loc][1]\n",
    "        if fs_data['finished'] == False:\n",
    "            loc = fs_data['loc']\n",
    "            if loc == 0:\n",
    "                call_start = 1\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            else:\n",
    "                call_start = fs_data['hull_points'][loc-1] + 2\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            for call in range(call_start, call_end + 1, 1):\n",
    "                for loc, row in fs_pick[fs_key]['fs_customers'].iterrows():\n",
    "                    if row['campaign'] == call:\n",
    "                        sc_tracker.add_calls(1)\n",
    "                        if row['y'] == \"yes\":\n",
    "                            sc_tracker.add_succ()\n",
    "                    elif row['campaign'] > call:\n",
    "                        sc_tracker.add_calls(1)\n",
    "            sc_tracker.save()\n",
    "            fs_pick[fs_key]['loc'] += 1\n",
    "            gradient_update(fs_key, fs_pick)\n",
    "            optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    return sc_tracker.get_succ_calls_score()\n",
    "\n",
    "\n",
    "def upper_bound(test_df):\n",
    "    print(\"Upper Bound Approach\")\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    res_df = test_df.query(\"y == 'yes'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        for i in range(0, num_cust):\n",
    "            sc_tracker.add_succ()\n",
    "        sc_tracker.add_calls((num_cust * x))\n",
    "        sc_tracker.save()\n",
    "    res_df = test_df.query(\"y == 'no'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        sc_tracker.add_calls((num_cust * x))\n",
    "        sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score()\n",
    "    \n",
    "    \n",
    "def new_approach_ratio_grouping_percentage(df, train_indicies, test_indicies, group_size, age_groupings, balance_groupings):\n",
    "    # Ensuring that we can binary encode any row in our dataset. We also group age and balance values \n",
    "    # from each row into ranges.\n",
    "    print(\"Step 1\")\n",
    "    df_copy = df.copy(deep=True)\n",
    "    group_feature(df_copy, \"age\", group_age, age_groupings)\n",
    "    group_feature(df_copy, \"balance\", group_balance, balance_groupings)\n",
    "    # Build the customers for each group.\n",
    "    print(\"Step 2\")\n",
    "    ratio_arr = compute_ratio_all_users(df_copy, train_indicies)\n",
    "    ratio_arr_sorted = sorted(ratio_arr, key=lambda tup: tup[1], reverse = True)\n",
    "    train_size = len(ratio_arr_sorted)\n",
    "    groupings = {}\n",
    "    for loc in range(0, train_size):\n",
    "        group_key = str(int(loc/group_size))\n",
    "        if group_key not in groupings.keys():\n",
    "            groupings[group_key] = {'indicies':[], 'mappings':{}, 'results':None} \n",
    "        groupings[group_key]['indicies'].append(ratio_arr_sorted[loc][0])\n",
    "    print(len(groupings.keys()))\n",
    "    # For each group, we find the unique feature combinations and store them in a list. \n",
    "    # We also store the results - s/c ratio for call numbers from 1-20.\n",
    "    test_calls = {}\n",
    "    print(\"Step 3\")\n",
    "    for group_key in groupings.keys():\n",
    "        users_df = df_copy.iloc[groupings[group_key]['indicies']]\n",
    "        mappings = groupings[group_key]['mappings']\n",
    "        for row in users_df.itertuples():\n",
    "            user_mapping = str((row.job, row.marital, row.education, row.default,\n",
    "                               row.housing, row.loan, row.age, row.balance))\n",
    "            if user_mapping not in mappings.keys():\n",
    "                mappings[user_mapping] = {'freq':0, 'percentage':0.0}\n",
    "            else:\n",
    "                mappings[user_mapping]['freq'] += 1\n",
    "        groupings[group_key]['results'] = compute_expected_succ_per_call_rate_feature_set(users_df, 20)\n",
    "        compute_freq_percentage(mappings)\n",
    "        test_calls[group_key] = {'locs_to_call':[], 'overall_rate':groupings[group_key]['results'][19]}\n",
    "    # print(test_calls)\n",
    "    # For the test set, we need to map each user to the most appropriate cluster.\n",
    "    print(\"Step 4\")\n",
    "    missed = 0\n",
    "    for loc in test_indicies:\n",
    "        row = df_copy.iloc[loc]\n",
    "        user_mapping = str((row['job'], row['marital'], row['education'], row['default'],\n",
    "                            row['housing'], row['loan'], row['age'], row['balance']))\n",
    "        all_groupings_keys = list(groupings.keys())\n",
    "        best_group_key = None\n",
    "        best_ratio = -1.0\n",
    "        for group_key in all_groupings_keys:\n",
    "            if user_mapping in groupings[group_key]['mappings']:\n",
    "                if best_group_key is None:\n",
    "                    best_group_key = group_key\n",
    "                    best_ratio = groupings[group_key]['mappings'][user_mapping]['percentage']\n",
    "                else:\n",
    "                    if groupings[best_group_key]['mappings'][user_mapping]['percentage'] > best_ratio:\n",
    "                        best_group_key = group_key\n",
    "                        best_ratio = groupings[group_key]['mappings'][user_mapping]['percentage']\n",
    "        if best_group_key is not None:\n",
    "            test_calls[best_group_key]['locs_to_call'].append(loc)\n",
    "        else:\n",
    "            missed += 1\n",
    "    print(\"We missed:\", missed)\n",
    "    # Call users ... those with the highest ratios are called first.\n",
    "    test_calls_sorted = sorted(test_calls.items(), key=lambda fs: fs[1]['overall_rate']['expected'], reverse = True)\n",
    "    print(\"Step 5\")\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    result_ratios = []\n",
    "    for test_call in test_calls_sorted:\n",
    "        for cust_loc in test_call[1]['locs_to_call']:\n",
    "            row = df_copy.iloc[cust_loc]\n",
    "            if row['y'] == \"yes\":\n",
    "                num_succ += 1\n",
    "            num_calls += int(row['campaign'])\n",
    "        result_ratios.append((num_succ, num_calls))\n",
    "    return result_ratios, groupings\n",
    "\n",
    "\n",
    "def clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded):\n",
    "    groupings = {}\n",
    "    predictions = clusterer.labels_\n",
    "    # We assign to each group, the similar indicies. This was based on the clustering approach.\n",
    "    for index, group in enumerate(predictions):\n",
    "        if str(group) not in groupings.keys():\n",
    "            groupings[str(group)] = {'train_indicies':[], 'unique_keys':{}, 'results':None, 'test_indicies':[]}\n",
    "        groupings[str(group)]['train_indicies'].append(index)\n",
    "    print(\"Check 1\")\n",
    "    # For all customers belonging to each grouping, we find the unique keys and compute the success per call\n",
    "    # ratio for call numbers 1-20.\n",
    "    for group in groupings.keys():\n",
    "        for index in groupings[group]['train_indicies']:\n",
    "            cust_info = train_df.iloc[index]\n",
    "            cust_features = get_features(train_df_encoded[index], feature_names)\n",
    "            # cust_features = cust_features[0:8]\n",
    "            if str(cust_features) not in groupings[group]['unique_keys'].keys():\n",
    "                groupings[group]['unique_keys'][str(cust_features)] = {'#_ocurr': 1}\n",
    "            else:\n",
    "                groupings[group]['unique_keys'][str(cust_features)]['#_ocurr'] += 1\n",
    "        results = compute_expected_succ_per_call_rate_feature_set(train_df.iloc[groupings[group]['train_indicies']], 20)\n",
    "        groupings[group]['results'] = results\n",
    "    print(\"Check 2\")\n",
    "    # This process makes use of the test set and determines the ideal cluster for a customer.\n",
    "    for index in range(0, len(test_df_encoded), 1):\n",
    "        encoded_customer_data = test_df_encoded[index]\n",
    "        test_labels, strengths = hdbscan.approximate_predict(clusterer, [encoded_customer_data])\n",
    "        groupings[str(test_labels[0])]['test_indicies'].append(index)\n",
    "    print(\"Check 3\")\n",
    "    # Perform sorting of groups based on success per call rate.\n",
    "    sorted_final_call = {k: v for k, v in sorted(groupings.items(), key=lambda item: item[1]['results'][19]['expected'], reverse = True)}\n",
    "    print(\"Check 4\")\n",
    "    # Go about calling customers, keep track of the success per call rate as we switch from group to group.\n",
    "    total_s = 0\n",
    "    total_c = 0\n",
    "    result_ratios = []\n",
    "    for group in sorted_final_call:\n",
    "        for cust_index in sorted_final_call[group]['test_indicies']:\n",
    "            row = test_df.iloc[cust_index]\n",
    "            if row['y'] == \"yes\":\n",
    "                total_s += 1\n",
    "            total_c += int(row['campaign'])\n",
    "        result_ratios.append((total_s, total_c))\n",
    "    return result_ratios, groupings\n",
    "        \n",
    "        \n",
    "# The encoding process can also be varied to not include age and balance.\n",
    "def clustering_age_balance_grouped(mkt_df_filtered, train_index, test_index, min_cluster_size, balance_groupings, age_groupings):\n",
    "    print(\"HDBScan Clustering - Groupings\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "    group_feature(mkt_df_filtered_cp, \"age\", group_age, age_groupings)\n",
    "    group_feature(mkt_df_filtered_cp, \"balance\", group_balance, balance_groupings)\n",
    "    train_df = mkt_df_filtered_cp.iloc[train_index]\n",
    "    test_df = mkt_df_filtered_cp.iloc[test_index]\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(mkt_df_filtered_cp.drop(columns = ['y', 'campaign']))\n",
    "    train_df_encoded = encoder.transform(train_df.drop(columns = ['y', 'campaign'])).toarray()\n",
    "    test_df_encoded = encoder.transform(test_df.drop(columns = ['y', 'campaign'])).toarray()\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "    clusterer.fit(train_df_encoded)\n",
    "#     train_df_copy = train_df.copy(deep = True)\n",
    "#     test_df_copy = test_df.copy(deep = True)\n",
    "#     group_feature(train_df_copy, \"age\", group_age, age_groupings)\n",
    "#     group_feature(train_df_copy, \"balance\", group_balance, balance_groupings)\n",
    "# #     print(train_df_copy.head(10))\n",
    "#     encoder = OneHotEncoder()\n",
    "#     train_df_encoded = encoder.fit_transform(train_df_copy.drop(columns=['y', 'campaign'])).toarray()\n",
    "#     clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "# #     print(len(train_df_encoded[0]))\n",
    "#     clusterer.fit(train_df_encoded)\n",
    "#     group_feature(test_df_copy, \"age\", group_age, age_groupings)\n",
    "#     group_feature(test_df_copy, \"balance\", group_balance, balance_groupings)\n",
    "#     test_df_encoded = encoder.fit_transform(test_df_copy.drop(columns=['y', 'campaign'])).toarray()\n",
    "#     feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "    return clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded)\n",
    "\n",
    "\n",
    "# The encoding process can also be varied to not include age and balance.\n",
    "def clustering_age_balance_not_grouped(mkt_df_filtered, train_index, test_index, min_cluster_size, balance_groupings, age_groupings):\n",
    "    print(\"HDBScan Clustering - No Groupings\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "    train_df = mkt_df_filtered_cp.iloc[train_index]\n",
    "    test_df = mkt_df_filtered_cp.iloc[test_index]\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(mkt_df_filtered_cp.drop(columns = ['y', 'campaign', 'age', 'balance']))\n",
    "    train_df_encoded = encoder.transform(train_df.drop(columns = ['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "    train_df_encoded = np.column_stack((train_df_encoded, train_df['age'].to_numpy()))\n",
    "    train_df_encoded = np.column_stack((train_df_encoded, train_df['balance'].to_numpy()))\n",
    "    test_df_encoded = encoder.transform(test_df.drop(columns = ['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "    test_df_encoded = np.column_stack((test_df_encoded, test_df['age'].to_numpy()))\n",
    "    test_df_encoded = np.column_stack((test_df_encoded, test_df['balance'].to_numpy()))\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan'])\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "    clusterer.fit(train_df_encoded)\n",
    "#     train_df_copy = train_df.copy(deep = True)\n",
    "#     test_df_copy = test_df.copy(deep = True)\n",
    "#     encoder = OneHotEncoder()\n",
    "#     train_df_encoded = encoder.fit_transform(train_df_copy.drop(columns=['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "#     train_df_encoded = np.column_stack((train_df_encoded, train_df_copy['age'].to_numpy()))\n",
    "#     train_df_encoded = np.column_stack((train_df_encoded, train_df_copy['balance'].to_numpy()))\n",
    "#     clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "#     clusterer.fit(train_df_encoded)\n",
    "#     test_df_encoded = encoder.fit_transform(test_df_copy.drop(columns=['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "#     test_df_encoded = np.column_stack((test_df_encoded, test_df_copy['age'].to_numpy()))\n",
    "#     test_df_encoded = np.column_stack((test_df_encoded, test_df_copy['balance'].to_numpy()))\n",
    "#     feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan'])\n",
    "    return clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded)\n",
    "\n",
    "# def clustering_age_balance_grouped_ratio_gradient_ascent(train_df, test_df, min_cluster_size, balance_groupings, age_groupings, num_calls):\n",
    "#     print(\"In P8\")\n",
    "#     group_age(train_df, age_groupings)\n",
    "#     group_balance(train_df, balance_groupings)\n",
    "#     train_df, ratio_df = compute_ratio_all_users(train_df)\n",
    "#     encoder = OneHotEncoder()\n",
    "#     train_df_encoded = encoder.fit_transform(train_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "#     train_df_encoded = np.column_stack((train_df_encoded, ratio_df['ratio'].to_numpy()))\n",
    "#     feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "#     clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "#     clusterer.fit(train_df_encoded)\n",
    "#     predictions = clusterer.labels_\n",
    "#     test_df = mkt_df_filtered.iloc[test_index]\n",
    "#     group_age(test_df, age_groupings)\n",
    "#     group_balance(test_df, balance_groupings)\n",
    "#     test_df_encoded = encoder.fit_transform(test_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "# #     return None, None, (None, None)\n",
    "#     return abstraction_new_approach_gradient_ascent(predictions, feature_names, train_df, test_df, train_df_encoded, test_df_encoded, num_calls)\n",
    "\n",
    "def tree_approaches_abstracted(model, mkt_df_filtered_cp, train_index, test_index):\n",
    "    # Encode Features.\n",
    "    features_to_transform = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'y']\n",
    "    for feature in features_to_transform:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(mkt_df_filtered_cp[feature])\n",
    "        mkt_df_filtered_cp[feature] = le.transform(mkt_df_filtered_cp[feature])\n",
    "    # Split into train/test.\n",
    "    train_df = mkt_df_filtered_cp.iloc[train_index]\n",
    "    test_df = mkt_df_filtered_cp.iloc[test_index]\n",
    "    # Split in to X and y.\n",
    "    feature_y = \"campaign\"\n",
    "    train_y = train_df[feature_y]\n",
    "    train_X = train_df.drop(columns = [feature_y])\n",
    "    test_y = test_df[feature_y]\n",
    "    test_X = test_df.drop(columns = [feature_y])\n",
    "    # Fit model and get predictions.\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions = model.predict(test_X)\n",
    "    # The rest is the approach to compute points for AUC metric. \n",
    "    total_c = 0\n",
    "    total_s = 0\n",
    "    results = []\n",
    "#     associations = []\n",
    "    j = 0\n",
    "    for index, row in test_X.iterrows():\n",
    "#         associations.append((j, index, predictions[j]))\n",
    "        if int(predictions[j]) <= test_y.loc[index]:\n",
    "            if row['y'] == 1:\n",
    "                total_s += 1\n",
    "        total_c += int(predictions[j])\n",
    "        results.append((total_c, total_s ))\n",
    "        j += 1\n",
    "#     total_c = 0\n",
    "#     total_s = 0\n",
    "#     results2 = []\n",
    "#     sorted_assoc = sorted(associations, key=lambda tup: tup[2])\n",
    "#     print(sorted_assoc[0])\n",
    "#     for tup in sorted_assoc:\n",
    "#         real_calls = test_y.loc[tup[1]]\n",
    "#         real_data = test_X.loc[tup[1]]\n",
    "#         if real_data['balance'] != mkt_df_filtered_cp.loc[tup[1]]['balance']:\n",
    "#             print(\"BADDDD\")\n",
    "#         if tup[2] <= int(real_calls):\n",
    "#             if real_data['y'] == 1:\n",
    "#                 total_s += 1\n",
    "#         total_c += int(tup[2])\n",
    "#         results2.append((total_c, total_s ))\n",
    "    return results\n",
    "    \n",
    "\n",
    "def decision_tree_multiclass(mkt_df_filtered, train_index, test_index):\n",
    "    print(\"Decision Tree\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "#     criterion='entropy', max_depth= 28, min_impurity_decrease= 0.00005\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    return tree_approaches_abstracted(model, mkt_df_filtered_cp, train_index, test_index)\n",
    "\n",
    "\n",
    "def xgboost_multiclass(mkt_df_filtered, train_index, test_index):\n",
    "    print(\"XGBoost\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "    model = XGBClassifier()\n",
    "    return tree_approaches_abstracted(model, mkt_df_filtered_cp, train_index, test_index)\n",
    "    \n",
    "\n",
    "\n",
    "# def decision_tree_multiclass(mkt_df_filtered, train_index, test_index):\n",
    "#     mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "    \n",
    "#     features_to_transform = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'y']\n",
    "#     for feature in features_to_transform:\n",
    "#         le = LabelEncoder()\n",
    "#         le.fit(mkt_df_filtered_cp[feature])\n",
    "#         mkt_df_filtered_cp[feature] = le.transform(mkt_df_filtered_cp[feature])\n",
    "    \n",
    "#     train_df = mkt_df_filtered_cp.iloc[train_index]\n",
    "#     test_df = mkt_df_filtered_cp.iloc[test_index]\n",
    "    \n",
    "#     model = tree.DecisionTreeClassifier()\n",
    "#     train_y = train_df['campaign']\n",
    "#     train_X = train_df.drop(columns = ['campaign'])\n",
    "#     test_y = test_df['campaign']\n",
    "#     test_X = test_df.drop(columns = ['campaign'])\n",
    "#     model.fit(train_X, train_y)\n",
    "#     predictions = model.predict(test_X)\n",
    "    \n",
    "#     total_c = 0\n",
    "#     total_s = 0\n",
    "#     results = []\n",
    "#     associations = []\n",
    "    \n",
    "#     j = 0\n",
    "#     for index, row in test_X.iterrows():\n",
    "#         associations.append((j, index, predictions[j]))\n",
    "#         if int(predictions[j]) <= test_y.loc[index]:\n",
    "#             if row['y'] == 1:\n",
    "#                 total_s += 1\n",
    "#         total_c += int(predictions[j])\n",
    "#         results.append((total_c, total_s ))\n",
    "#         j += 1\n",
    "    \n",
    "#     total_c = 0\n",
    "#     total_s = 0\n",
    "#     results2 = []\n",
    "#     sorted_assoc = sorted(associations, key=lambda tup: tup[2])\n",
    "#     for tup in sorted_assoc:\n",
    "#         real_calls = test_y.loc[tup[1]]\n",
    "#         real_data = test_X.loc[tup[1]]\n",
    "#         if tup[2] <= int(real_calls):\n",
    "#             if real_data['y'] == 1:\n",
    "#                 total_s += 1\n",
    "#         total_c += int(real_calls)\n",
    "#         results2.append((total_c, total_s ))\n",
    "    \n",
    "#     return results, results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell represents the logic required for the inital approach taken.\n",
    "# Yields really good results thus far.\n",
    "# This should remain untouched.\n",
    "\n",
    "def construct_feature_combs(train_df, min_row_fs, age_query_strings, balance_query_strings):\n",
    "    # At this point, we can run computations for the success rate of each sub attribute and join\n",
    "    # the sub-attributes based on the output of k-means.\n",
    "    poss = []\n",
    "\n",
    "    # Education.\n",
    "    all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "    education_cmbs = find_combinations(all_ed, metric_vals)\n",
    "\n",
    "    # Occupation.\n",
    "    all_jobs = ['student', 'retired', 'unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_jobs, train_df, 'job')\n",
    "    job_cmbs = find_combinations(all_jobs, metric_vals)\n",
    "\n",
    "    # Marital.\n",
    "    all_ms = ['married', 'single', 'divorced']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ms, train_df, 'marital')\n",
    "    marital_cmbs = find_combinations(all_ms, metric_vals)\n",
    "\n",
    "    # Default\n",
    "    all_def = ['no', 'yes']\n",
    "    default_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Loan\n",
    "    all_ln = ['no', 'yes']\n",
    "    loan_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Housing\n",
    "    all_hs = ['no', 'yes']\n",
    "    housing_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    poss.append(education_cmbs)\n",
    "    poss.append(marital_cmbs)\n",
    "    poss.append(job_cmbs)\n",
    "    poss.append(default_cmbs)\n",
    "    poss.append(loan_cmbs)\n",
    "    poss.append(housing_cmbs)\n",
    "    all_combs = list(itertools.product(*poss))\n",
    "\n",
    "    # print(\"Number of combinations: \", len(all_combs)* len(age_query_strings) * len(balance_query_strings))\n",
    "\n",
    "    # We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "    num_iter = 0\n",
    "    combs_to_consider = {}\n",
    "    fs_pick = {}\n",
    "\n",
    "    # Setting up looping structures to generate all possibilities.\n",
    "    for age_query in age_query_strings:\n",
    "        df_filtered_final = train_df.query(age_query)\n",
    "        for bal_query in balance_query_strings:\n",
    "            df_filtered_final_2 = df_filtered_final.query(bal_query)\n",
    "            for comb in all_combs:\n",
    "                dict_final_query = construct_dict(comb)\n",
    "                num_iter += 1\n",
    "                extracted_df = extract_rows_feature_set(df_filtered_final_2, dict_final_query)\n",
    "                key = (dict_final_query['education'], dict_final_query['job'], \n",
    "                       dict_final_query['marital'], dict_final_query['default'], \n",
    "                       dict_final_query['loan'], dict_final_query['housing'], \n",
    "                       bal_query, age_query)\n",
    "                n_rows = extracted_df.shape[0]\n",
    "                if n_rows >= min_row_fs:\n",
    "                    results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "#                             max_loc = compute_optimal_call_no(results)\n",
    "#                             if max_loc != -1:\n",
    "                    combs_to_consider[key] = {\n",
    "                                                'max_loc':0,\n",
    "                                                'best_rate':0, \n",
    "                                                'overall_rate':results[max_calls-1]['expected'], \n",
    "                                                'n_rows':n_rows, \n",
    "                                                'results':results,\n",
    "                                                'fs_customers':None\n",
    "                                             }\n",
    "                    fs_pick[key] = {\n",
    "                                    'grad': 0.0, \n",
    "                                    'loc':0, \n",
    "                                    'finished':False, \n",
    "                                    'hull_points':None,\n",
    "                                    'max_num_pts': -1,\n",
    "                                    'results':None,\n",
    "                                    'fs_customers' :None\n",
    "                                   }\n",
    "#               else:\n",
    "#                   print(\"Invalid FS ! -> \", n_rows)\n",
    "    for fs_key in combs_to_consider.keys():\n",
    "        fs_customers = find_all_cust_feature_set(fs_key, test_df)\n",
    "        combs_to_consider[fs_key]['fs_customers'] = fs_customers\n",
    "        res = construct_hull_points(combs_to_consider[fs_key]['results'], max_calls)\n",
    "        fs_pick[fs_key]['results'] = combs_to_consider[fs_key]['results']\n",
    "        fs_pick[fs_key]['fs_customers'] = fs_customers\n",
    "        if res is False:\n",
    "            # print(\"Invalid Convex Hull Assignment\")\n",
    "            fs_pick[fs_key]['finished'] = True\n",
    "        else:\n",
    "            fs_pick[fs_key]['hull_points'] = res\n",
    "            fs_pick[fs_key]['max_num_pts'] = len(res) - 1\n",
    "    return combs_to_consider, fs_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 17)\n",
      "CPU times: user 46.2 ms, sys: 14 Âµs, total: 46.2 ms\n",
      "Wall time: 44 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Code that sets up values to construct all possible feature combinations.\n",
    "\n",
    "# Query Strings with filename being 'mod_1' \n",
    "# age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "# age_ranges_2 = [(10, 34), (35, 45), (46, 100)]\n",
    "# balance_query_strings = ['balance <= 450',' balance > 450']\n",
    "# balance_ranges_2 = [(-10000, 450), (451, 105000)]\n",
    "\n",
    "# Query Strings with filename being 'mod_2' \n",
    "age_query_strings_1 = ['age >= 18 & age <= 30', 'age >= 31 & age <= 47', 'age >= 48 & age <= 64', 'age >= 65']\n",
    "age_ranges_1 = [(18,30), (31, 47), (48,64), (65,100)]\n",
    "balance_query_strings_1 = ['balance <= 450',' balance > 450']\n",
    "balance_ranges_1 = [(-10000, 450), (451, 105000)]\n",
    "\n",
    "age_balance_ranges = []\n",
    "age_balance_ranges.append((age_query_strings_1, age_ranges_1, balance_query_strings_1, balance_ranges_1))\n",
    "\n",
    "# Max call number to consider.\n",
    "max_calls = 20\n",
    "\n",
    "# Pull and filter all calls <= 20.\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "mkt_df_filtered = mkt_df_filtered[['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance', 'campaign', 'y']]\n",
    "print(mkt_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "fold_data = []\n",
    "\n",
    "for train_index, test_index in kf.split(mkt_df_filtered):\n",
    "    fold_data.append((train_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([    0,     1,     2, ..., 44964, 44965, 44966]),\n",
       "  array([   12,    21,    30, ..., 44956, 44957, 44958])),\n",
       " (array([    1,     2,     3, ..., 44961, 44963, 44965]),\n",
       "  array([    0,     6,    13, ..., 44962, 44964, 44966])),\n",
       " (array([    0,     1,     2, ..., 44964, 44965, 44966]),\n",
       "  array([    4,     5,    10, ..., 44954, 44955, 44961])),\n",
       " (array([    0,     2,     3, ..., 44962, 44964, 44966]),\n",
       "  array([    1,     8,     9, ..., 44943, 44963, 44965])),\n",
       " (array([    0,     1,     4, ..., 44964, 44965, 44966]),\n",
       "  array([    2,     3,     7, ..., 44927, 44931, 44960]))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "CPU times: user 6min 12s, sys: 669 ms, total: 6min 13s\n",
      "Wall time: 6min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Main code ... orchestrates everything!\n",
    "phase_batch = {}\n",
    "i = 0\n",
    "\n",
    "for fold in fold_data:\n",
    "    \n",
    "    train_index = fold[0]\n",
    "    test_index = fold[1]\n",
    "    \n",
    "    result_ratios_p1 = None\n",
    "    result_ratios_p2 = None\n",
    "    result_ratios_p3 = None\n",
    "    result_ratios_p4 = None\n",
    "    result_ratios_p5 = None\n",
    "    result_ratios_p6 = None\n",
    "    result_ratios_p7 = None\n",
    "    result_ratios_p8 = None\n",
    "    result_ratios_p9 = None\n",
    "\n",
    "    i += 1\n",
    "    print(\"At fold number: \", i)\n",
    "\n",
    "    train_df = mkt_df_filtered.iloc[train_index]\n",
    "    test_df = mkt_df_filtered.iloc[test_index]\n",
    "\n",
    "    combs_to_consider, fs_pick = construct_feature_combs(train_df, 0, age_query_strings_1, balance_query_strings_1)\n",
    "\n",
    "    # Testing Phase 1 -> Baseline test with shuffling of all customers and calling them ..\n",
    "    results_p1, num_calls = call_everyone(test_df)\n",
    "\n",
    "    # Testing Phase 2 -> Order how we call customers - based on the overall s/c ratio ..\n",
    "    results_p2 = greedy_approach(combs_to_consider)\n",
    "\n",
    "    # Testing Phase 4 -> Convex Hull - Gradient Ascent Approach ..\n",
    "    results_p4 = convex_hull(fs_pick, num_calls)\n",
    "\n",
    "#     # Testing Phase 5 -> If we were godlike and knew all ..\n",
    "#     result_ratios_p5 = upper_bound(test_df)\n",
    "    \n",
    "#     # New Stuff\n",
    "#     result_ratios_p6, groups1 = clustering_age_balance_grouped(mkt_df_filtered, train_index, test_index, 20, balance_ranges_1, age_ranges_1)\n",
    "\n",
    "#     # result_ratios_p7, groups2 = clustering_age_balance_not_grouped(mkt_df_filtered, train_index, test_index, 20, balance_ranges_2, age_ranges_2)\n",
    "\n",
    "#     # result_ratios_p8, groups3 = new_approach_ratio_grouping_percentage(mkt_df_filtered, train_index, test_index, 500, age_ranges_2, balance_ranges_2)\n",
    "    \n",
    "#     result_ratios_p7 = xgboost_multiclass(mkt_df_filtered, train_index, test_index)\n",
    "    \n",
    "#     result_ratios_p8 = decision_tree_multiclass(mkt_df_filtered, train_index, test_index)\n",
    "    \n",
    "    # Add all results together for this fold.\n",
    "    phase_batch[i] = {'p1':results_p1, 'p2':results_p2, \n",
    "                      'p4':results_p4, 'p5':result_ratios_p5,\n",
    "                      'p6':result_ratios_p6, 'p7':result_ratios_p7, \n",
    "                      'p8':result_ratios_p8, 'p9':result_ratios_p9\n",
    "                     }\n",
    "\n",
    "# with open('full_max20.json', 'w') as fp:\n",
    "#     json.dump(phase_batch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = {'1':{'ratio':0.25}, '2':{'ratio':0.59}}\n",
    "sorted(me.items(), key=lambda fs: fs[1]['ratio'], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  50,\n",
       "  102,\n",
       "  150,\n",
       "  201,\n",
       "  251,\n",
       "  300,\n",
       "  350,\n",
       "  400,\n",
       "  450,\n",
       "  504,\n",
       "  1003,\n",
       "  1502,\n",
       "  2001,\n",
       "  2500,\n",
       "  3000,\n",
       "  3504,\n",
       "  4000,\n",
       "  4500,\n",
       "  5014,\n",
       "  5500,\n",
       "  6004,\n",
       "  6501,\n",
       "  7005,\n",
       "  7503,\n",
       "  8001,\n",
       "  8503,\n",
       "  9000,\n",
       "  9507,\n",
       "  10004,\n",
       "  10500,\n",
       "  11005,\n",
       "  11501,\n",
       "  12000,\n",
       "  12502,\n",
       "  13001,\n",
       "  13500,\n",
       "  14001,\n",
       "  14501,\n",
       "  15002,\n",
       "  15500,\n",
       "  16010,\n",
       "  16509,\n",
       "  17000,\n",
       "  17501,\n",
       "  18002,\n",
       "  18500,\n",
       "  19001,\n",
       "  19505,\n",
       "  20003,\n",
       "  20500,\n",
       "  21001,\n",
       "  21501,\n",
       "  22003,\n",
       "  22503,\n",
       "  23003,\n",
       "  23501,\n",
       "  23751],\n",
       " [0,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  9,\n",
       "  11,\n",
       "  16,\n",
       "  19,\n",
       "  24,\n",
       "  25,\n",
       "  28,\n",
       "  50,\n",
       "  72,\n",
       "  98,\n",
       "  125,\n",
       "  149,\n",
       "  174,\n",
       "  212,\n",
       "  225,\n",
       "  257,\n",
       "  278,\n",
       "  297,\n",
       "  326,\n",
       "  347,\n",
       "  375,\n",
       "  390,\n",
       "  423,\n",
       "  453,\n",
       "  475,\n",
       "  507,\n",
       "  531,\n",
       "  554,\n",
       "  573,\n",
       "  599,\n",
       "  620,\n",
       "  641,\n",
       "  659,\n",
       "  684,\n",
       "  717,\n",
       "  731,\n",
       "  750,\n",
       "  769,\n",
       "  787,\n",
       "  809,\n",
       "  835,\n",
       "  861,\n",
       "  889,\n",
       "  910,\n",
       "  923,\n",
       "  936,\n",
       "  959,\n",
       "  985,\n",
       "  1012,\n",
       "  1039,\n",
       "  1059,\n",
       "  1076,\n",
       "  1102,\n",
       "  1117],\n",
       " 13635489.0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xg_dt_correct1'+'.json', 'w') as fp:\n",
    "    json.dump(phase_batch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for key in phase_batch.keys():\n",
    "    print(key)\n",
    "    new_ratios = []\n",
    "    for el in phase_batch[key]['p9']:\n",
    "        new_ratios.append((int(el[0]), int(el[1])))\n",
    "    phase_batch[key]['p9'] = new_ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in phase_batch.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45174\n",
      "45175\n",
      "45176\n",
      "45177\n",
      "45178\n",
      "45179\n",
      "45180\n",
      "45181\n",
      "45182\n",
      "45183\n",
      "45184\n",
      "45185\n",
      "45186\n",
      "45187\n",
      "45188\n",
      "45189\n",
      "45190\n",
      "45191\n",
      "45192\n",
      "45193\n",
      "45194\n",
      "45195\n",
      "45196\n",
      "45197\n",
      "45198\n",
      "45199\n",
      "45200\n",
      "45201\n",
      "45202\n",
      "45203\n",
      "45204\n",
      "45205\n",
      "45206\n",
      "45207\n",
      "45208\n",
      "45209\n",
      "45210\n"
     ]
    }
   ],
   "source": [
    "for index, row in mkt_df_filtered.iterrows():\n",
    "    if index > 45173:\n",
    "        print(index)\n",
    "    res = mkt_df_filtered.loc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45173, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mkt_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dc0a1f8adf41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmkt_df_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m45174\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "mkt_df_filtered.iloc[45174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
