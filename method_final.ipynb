{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math, itertools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "\n",
    "from ipynb.fs.full.helper_fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that sets up values to construct all possible feature combinations.\n",
    "\n",
    "poss = []\n",
    "\n",
    "poss.append([['tertiary', 'unknown'],['primary','secondary']])\n",
    "poss.append([['single'],['married'],['divorced']])\n",
    "poss.append([['student','retired','unemployed'],['admin', 'management', 'self-employed'],['technician', 'unknown', 'services'],['housemaid', 'blue-collar', 'entrepreneur']])\n",
    "poss.append([['no'],['yes']])\n",
    "poss.append([['no'],['yes']])\n",
    "poss.append([['no'],['yes']])\n",
    "all_combs = list(itertools.product(*poss))\n",
    "\n",
    "# Age query strings.\n",
    "age_query_strings = ['age < 26','age >= 26 & age <=60','age >60']\n",
    "\n",
    "# Balance query strings.\n",
    "balance_query_strings = ['balance <= 5000',' balance > 5000']\n",
    "\n",
    "# Max call number to consider.\n",
    "# We obtained this by dividing the overall reward we have in the dataset by the number of successes.\n",
    "max_calls = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_combinations(sub_attributes, ratios, att_name):\n",
    "    num_iter = len(ratios)\n",
    "    sil_scores = []\n",
    "    # Making use of the K-Means algorithm ... number of centroids are from 2 to n-1.\n",
    "    for clust_num in range(2, num_iter):\n",
    "        kmeans = KMeans(n_clusters = clust_num)\n",
    "        kmeans.fit(ratios.reshape(-1,1))\n",
    "        results = kmeans.labels_\n",
    "        sil_scores.append((silhouette_score(ratios.reshape(-1,1), results, metric='euclidean'), results, clust_num))\n",
    "    # We make use of the silhouette score to determine the ideal number of centroids.\n",
    "    sorted_sil_scores = sorted(sil_scores, key=lambda x: x[0], reverse = True)\n",
    "    # We then use this ideal number of centroids to determine which sub attributes should be aggregated.\n",
    "    joined_sub_attributes = []\n",
    "    for i in range(0, sorted_sil_scores[0][2]):\n",
    "        joined_sub_attributes.append([])\n",
    "    join_list = sorted_sil_scores[0][1]\n",
    "    for index, value in enumerate(join_list):\n",
    "        pos = join_list[index]\n",
    "        joined_sub_attributes[pos].append(sub_attributes[index])\n",
    "    return_joined_sub_attributes = []\n",
    "    for arr in joined_sub_attributes:\n",
    "        query_str = \"\"\n",
    "        for item in arr:\n",
    "            single_str = att_name + \" == \" + item + \" & \"\n",
    "            query_str += single_str\n",
    "        query_str = query_str[:-2]\n",
    "        return_joined_sub_attributes.append(query_str)\n",
    "#     print(return_joined_sub_attributes)\n",
    "    return return_joined_sub_attributes\n",
    "\n",
    "# The following is the format of the way in which this method should be called.\n",
    "# find_combinations(['a', 'b', 'c', 'd'], np.array([1, 4, 7, 90]), \"job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44967, 17)\n",
      "(8994,)\n",
      "(8994,)\n",
      "(8993,)\n",
      "(8993,)\n",
      "(8993,)\n"
     ]
    }
   ],
   "source": [
    "# Main code ... orchestrates everything!\n",
    "\n",
    "# Pull and filter all calls <= 20.\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "\n",
    "print(mkt_df_filtered.shape)\n",
    "\n",
    "\n",
    "# Splitting dataframe into data and result dataframes.\n",
    "X = mkt_df_filtered.iloc[:,0:len(mkt_df_filtered.columns)-1]\n",
    "y = mkt_df_filtered.iloc[:,-1]   \n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Obtaining relevant dataframes based on the splitting defined by cross validation.\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # At this point, we can run computations for the success rate of each sub attribute and join\n",
    "    # the sub-attributes based on the output of k-means.\n",
    "    \n",
    "    \n",
    "    # We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "    \n",
    "    \n",
    "    num_iter = 0\n",
    "    num_non_zero_combs = 0\n",
    "    combs_to_consider = []\n",
    "    # Setting up looping structures to generate all possibilities.\n",
    "    # All that has to be done now is to change 'df_train' to 'X_train'.\n",
    "    # for age_query in age_query_strings:\n",
    "    #     df_filtered_final = df_train.query(age_query)\n",
    "    #     for bal_query in balance_query_strings:\n",
    "    #         df_filtered_final = df_filtered_final.query(bal_query)\n",
    "    #         for comb in all_combs:\n",
    "    #             dict_final_query = construct_dict(comb)\n",
    "    #             num_iter += 1\n",
    "    #             extracted_df = extract_rows_feature_set(df_filtered_final, dict_final_query)\n",
    "    #             if extracted_df.shape[0] != 0:\n",
    "    #                 num_non_zero_combs += 1\n",
    "    #                 results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "    #                 max_loc = compute_optimal_call_no(results)\n",
    "    #                 if max_loc >= 0:\n",
    "    #                     combs_to_consider.append({'age':age_query, 'bal':bal_query, 'comb':comb, 'consider':True, 'max_loc':max_loc, 'rate':results[max_loc]['expected']})\n",
    "    #                 else:\n",
    "    #                     combs_to_consider.append({'age':age_query, 'bal':bal_query, 'comb':comb, 'consider':False})\n",
    "    #                 print(age_query)\n",
    "    #                 print(bal_query)\n",
    "    #                 print(dict_final_query)\n",
    "    #                 print(\"Max Loc is: \", max_loc+1)\n",
    "    #                 plot_graph_new(results, max_calls)\n",
    "    #                 print(\"\\n\\n\\n\")\n",
    "    #\n",
    "    #When we are finished creating the feature combinations .... we can now use the hold out set for validation of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += row['campaign']\n",
    "    return div(total_successes, total_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All combs:  0.3513100131570743\n",
      "All combs(ratio >=0.20):  0.5514576239792396\n"
     ]
    }
   ],
   "source": [
    "# Metric using our method.\n",
    "filtered_combs_valid = [comb for comb in combs_to_consider if comb['consider'] == True]\n",
    "filtered_combs_valid_sorted = sorted(filtered_combs_valid, key=lambda k: k['rate'], reverse = True) \n",
    "print(\"All combs: \", mean([comb['rate'] for comb in combs_to_consider]))\n",
    "print(\"All combs(ratio >=0.20): \", mean([comb['rate'] for comb in combs_to_consider if comb['rate'] >= 0.20]))\n",
    "# The ratio will be increased further when we filter the unwanted feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04469269018705815\n"
     ]
    }
   ],
   "source": [
    "# Metric for the filtered dataset (only up to 20 calls).\n",
    "print(compute_metric(mkt_df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042326899068472104\n"
     ]
    }
   ],
   "source": [
    "# Metric for the entire dataset (up to 56 calls).\n",
    "print(compute_metric(mkt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section computes the metric for the purpose of determining the groupings within each feature (  ..... need to finish for the remainder of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_for_each_attribute(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} == '{1}'\".format(attrib, value)\n",
    "        metric_val = compute_metric(mkt_df_filtered.query(v_query))\n",
    "        metric_vals[index] = metric_val\n",
    "        print(v_query, metric_val)\n",
    "    return metric_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age >= 10 & age <= 19 0.14634146341463414\n",
      "age >= 20 & age <= 29 0.07631601041054488\n",
      "age >= 30 & age <= 39 0.040097307272879794\n",
      "age >= 40 & age <= 49 0.03357253501090633\n",
      "age >= 50 & age <= 59 0.03437390389337075\n",
      "age >= 60 & age <= 69 0.12427647259107934\n",
      "age >= 70 & age <= 79 0.20594965675057209\n",
      "age >= 80 0.1950354609929078\n"
     ]
    }
   ],
   "source": [
    "# Age.\n",
    "all_age_query_strings = ['age >= 10 & age <= 19', 'age >= 20 & age <= 29', 'age >= 30 & age <= 39', 'age >= 40 & age <= 49', 'age >= 50 & age <= 59','age >= 60 & age <= 69', 'age >= 70 & age <= 79', 'age >= 80']\n",
    "for age_query in all_age_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(age_query)\n",
    "    print(age_query, compute_metric(df_filtered_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job == 'student' 0.12907869481765835\n",
      "job == 'retired' 0.09875598086124401\n",
      "job == 'unemployed' 0.06631648063033486\n",
      "job == 'admin' 0.0\n",
      "job == 'management' 0.05079848502596541\n",
      "job == 'self-employed' 0.04410377358490566\n",
      "job == 'technician' 0.04040647274128299\n",
      "job == 'unknown' 0.0379041248606466\n",
      "job == 'services' 0.03470001880759827\n",
      "job == 'housemaid' 0.0319243275199527\n",
      "job == 'blue-collar' 0.02749087520385183\n",
      "job == 'entrepreneur' 0.030182090296832127\n"
     ]
    }
   ],
   "source": [
    "# Occupation.\n",
    "all_jobs = ['student', 'retired', 'unemployed', 'admin', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "compute_metric_for_each_attribute(all_jobs, mkt_df_filtered, 'job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital == 'married' 0.03762389773737097\n",
      "marital == 'single' 0.05973928537934915\n",
      "marital == 'divorced' 0.04741576459826193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['marital == married & marital == divorced ', 'marital == single ']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current_dir = os.getcwd()\n",
    "# mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "# mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "# Marital Status\n",
    "all_ms = ['married', 'single', 'divorced']\n",
    "metric_vals = compute_metric_for_each_attribute(all_ms, mkt_df_filtered, 'marital')\n",
    "find_combinations(all_ms, metric_vals, 'marital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education == 'tertiary' 0.05604090002528161\n",
      "education == 'secondary' 0.0410958904109589\n",
      "education == 'primary' 0.03222282905516111\n",
      "education == 'unknown' 0.05277486910994764\n"
     ]
    }
   ],
   "source": [
    "# Education\n",
    "all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "compute_metric_for_each_attribute(all_ed, mkt_df_filtered, 'education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default == 'no' 0.04515566754107414\n",
      "default == 'yes' 0.021996615905245348\n",
      "default == 'unknown' 0.0\n"
     ]
    }
   ],
   "source": [
    "# Default\n",
    "all_def = ['no', 'yes', 'unknown']\n",
    "compute_metric_for_each_attribute(all_def, mkt_df_filtered, 'default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing == 'no' 0.061309197293838\n",
      "housing == 'yes' 0.030395519335452\n",
      "housing == 'unknown' 0.0\n"
     ]
    }
   ],
   "source": [
    "# Housing\n",
    "all_hs = ['no', 'yes', 'unknown']\n",
    "compute_metric_for_each_attribute(all_hs, mkt_df_filtered, 'housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan == 'no' 0.048405306237651706\n",
      "loan == 'yes' 0.025356992860142796\n",
      "loan == 'unknown' 0.0\n"
     ]
    }
   ],
   "source": [
    "# Loan\n",
    "all_ln = ['no', 'yes', 'unknown']\n",
    "compute_metric_for_each_attribute(all_ln, mkt_df_filtered, 'loan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance >= -100000 & age <= -1 0.0\n",
      "balance >= 0 & balance < 1000 0.03984136840916789\n",
      "balance >= 1000 & balance < 2000 0.05351934051997464\n",
      "balance >= 2000 & balance < 3000 0.0680968858131488\n",
      "balance >= 3000 & balance < 4000 0.0689893862482695\n",
      "balance >= 4000 & balance < 5000 0.06461086637298091\n",
      "balance >= 5000 & balance < 6000 0.06534547402249598\n",
      "balance >= 6000 & balance < 7000 0.04906542056074766\n",
      "balance >= 7000 & balance < 8000 0.0671217292377702\n",
      "balance >= 8000 & balance < 9000 0.052269601100412656\n",
      "balance >= 9000 & balance < 10000 0.058823529411764705\n",
      "balance >= 10000 & balance < 11000 0.09217877094972067\n",
      "balance >= 11000 & balance < 12000 0.08365019011406843\n",
      "balance >= 12000 & balance < 13000 0.09821428571428571\n",
      "balance >= 13000 & balance < 14000 0.035897435897435895\n",
      "balance >= 14000 & balance < 15000 0.058823529411764705\n",
      "balance >= 15000 & balance < 16000 0.03418803418803419\n",
      "balance >= 16000 & balance < 17000 0.0\n",
      "balance >= 17000 & balance < 18000 0.028985507246376812\n",
      "balance >= 18000 & balance < 19000 0.11904761904761904\n",
      "balance >= 19000 & balance < 19000 0.0\n",
      "balance >= 20000 0.05742574257425743\n"
     ]
    }
   ],
   "source": [
    "# Balance\n",
    "all_bal_query_strings = ['balance >= -100000 & balance <= -1', 'balance >= 0 & balance < 1000', 'balance >= 1000 & balance < 2000', 'balance >= 2000 & balance < 3000', 'balance >= 3000 & balance < 4000','balance >= 4000 & balance < 5000', 'balance >= 5000 & balance < 6000', 'balance >= 6000 & balance < 7000', 'balance >= 7000 & balance < 8000', 'balance >= 8000 & balance < 9000', 'balance >= 9000 & balance < 10000','balance >= 10000 & balance < 11000', 'balance >= 11000 & balance < 12000', 'balance >= 12000 & balance < 13000', 'balance >= 13000 & balance < 14000', 'balance >= 14000 & balance < 15000', 'balance >= 15000 & balance < 16000', 'balance >= 16000 & balance < 17000','balance >= 17000 & balance < 18000', 'balance >= 18000 & balance < 19000', 'balance >= 19000 & balance < 19000', 'balance >= 20000']\n",
    "for bal_query in all_bal_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(bal_query)\n",
    "    print(bal_query, compute_metric(df_filtered_final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
