{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import ipynb.fs\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import operator as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math, itertools\n",
    "import statistics\n",
    "import json\n",
    "import hdbscan\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Sklearn imports\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Succ_Call_Tracker():\n",
    "    def __init__(self):\n",
    "        self.calls = [0]\n",
    "        self.succ = [0]\n",
    "        self.curr_s = 0\n",
    "        self.curr_c = 0\n",
    "        \n",
    "    def add_calls(self, num_calls):\n",
    "        self.curr_c += num_calls\n",
    "        \n",
    "    def add_succ(self):\n",
    "        self.curr_s += 1\n",
    "    \n",
    "    # The following TWO methods might not be useful ..\n",
    "    def add_call_save(self, num_calls):\n",
    "        self.add_calls(int(num_calls))\n",
    "        self.calls.append(self.curr_c)\n",
    "    def add_succ_save(self):\n",
    "        self.add_succ()\n",
    "        self.succ.append(self.curr_s)\n",
    "        \n",
    "    def save(self):\n",
    "        self.calls.append(self.curr_c)\n",
    "        self.succ.append(self.curr_s)\n",
    "        \n",
    "    def get_calls(self):\n",
    "        return self.curr_c\n",
    "        \n",
    "    def get_auc(self):\n",
    "        return metrics.auc(self.calls, self.succ)\n",
    "    \n",
    "    def get_all_calls_succ(self):\n",
    "        return (self.calls, self.succ)\n",
    "    \n",
    "    def get_succ_calls_score(self):\n",
    "        return (self.calls, self.succ, self.get_auc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 10, 55], [0, 1, 2])\n",
      "([0, 10, 55], [0, 1, 2], 72.5)\n"
     ]
    }
   ],
   "source": [
    "t = Succ_Call_Tracker()\n",
    "t.add_calls(10)\n",
    "t.add_succ()\n",
    "t.save()\n",
    "t.add_call_save(45)\n",
    "t.add_succ_save()\n",
    "print(t.get_all_calls_succ())\n",
    "print(t.get_succ_calls_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions Across All Methods\n",
    "\n",
    "def load_file(data_file_path):\n",
    "    data_df = pd.read_csv(data_file_path, delimiter=\";\")\n",
    "    return data_df\n",
    "  \n",
    "    \n",
    "def plot_graph_new(results, max_calls, list_passed, title, name = \"1\"):\n",
    "    x_pts = [i+1 for i in range(0, max_calls)]\n",
    "    if list_passed:\n",
    "        y_pts = results\n",
    "    else:    \n",
    "        y_pts = [results[i]['expected'] for i in range(0, max_calls)]\n",
    "    print(y_pts)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_pts, y_pts, linewidth=2)\n",
    "    plt.xlabel(\"Call Number\")\n",
    "    plt.ylabel(\"Success Per Call Rate\")\n",
    "    plt.ylim(0, 0.4)\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))\n",
    "#     plt.axvline(x=0, color =\"black\", linewidth=1)\n",
    "#     plt.axhline(y=0, color =\"black\", linewidth=1)\n",
    "    plt.xticks(np.arange(1, max_calls+1, 1))\n",
    "#     plt.show()\n",
    "    plt.savefig(str(name) + \".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def div(a,b):\n",
    "    if int(b) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return a/b\n",
    "    \n",
    "\n",
    "# Used for creating all possible combinations of the features.\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = iterable\n",
    "    return itertools.chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "\n",
    "\n",
    "def construct_dict(feature_comb):\n",
    "    new_dict = {}\n",
    "    new_dict['education'] = convert(feature_comb[0])\n",
    "    new_dict['job'] = convert(feature_comb[2])\n",
    "    new_dict['marital'] = convert(feature_comb[1])\n",
    "    new_dict['default'] = convert(feature_comb[3])\n",
    "    new_dict['loan'] = convert(feature_comb[4])\n",
    "    new_dict['housing'] = convert(feature_comb[5])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# This is the new metric (success per call rate).\n",
    "def compute_expected_succ_per_call_rate_feature_set(fs_df, no_calls_considered):\n",
    "    expected_values_call_nums = []\n",
    "    for i in range(1, no_calls_considered + 1):\n",
    "        expected_values_call_nums.append({'succ':0, 'total_calls':0, 'expected':0.0})\n",
    "        for index, row in fs_df.iterrows():\n",
    "            no_calls = row['campaign']\n",
    "            if no_calls <= i:\n",
    "                if row['y'] == \"yes\":\n",
    "                    expected_values_call_nums[i-1]['succ'] += 1\n",
    "                expected_values_call_nums[i-1]['total_calls'] += no_calls\n",
    "            else:\n",
    "                expected_values_call_nums[i-1]['total_calls'] += i\n",
    "    for loc, item in enumerate(expected_values_call_nums):\n",
    "        expected_values_call_nums[loc]['expected'] = div(item['succ'], item['total_calls'])\n",
    "    return expected_values_call_nums\n",
    "\n",
    "\n",
    "def compute_optimal_call_no(results):\n",
    "    max_loc = max(range(len(results)), key=lambda index: results[index]['expected'])\n",
    "    if max_loc == 0 and results[max_loc]['expected'] == 0.0:\n",
    "        return -1\n",
    "    return max_loc\n",
    "\n",
    "\n",
    "# Given a dictionary of what attributes comprise a feature set, we can get all rows corresponding to this feature set.\n",
    "def extract_rows_feature_set(fs_df, feature_labels = {'education':['tertiary', 'unknown'], \n",
    "                                                      'job':['management', 'technician', 'blue-collar'], \n",
    "                                                      'marital':['single'], 'default':['no'], \n",
    "                                                      'housing':['no'], 'loan':['no']}):\n",
    "    for key in feature_labels:\n",
    "        feature_labels_query_str = ''\n",
    "        arr = feature_labels[key]\n",
    "        for label in arr:\n",
    "            feature_labels_query_str += (key + ' == \"'+ label + '\" | ')\n",
    "        feature_labels_query_str = feature_labels_query_str[:-3]\n",
    "        fs_df = fs_df.query(feature_labels_query_str)\n",
    "    return fs_df\n",
    "\n",
    "\n",
    "def find_matching_attribute_comb(row_value, all_combs):\n",
    "    query = None\n",
    "    for comb in all_combs:\n",
    "        for item in comb:\n",
    "            if item == row_value:\n",
    "                query = comb\n",
    "    return query\n",
    "\n",
    "\n",
    "def compute_metric(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += row['campaign']\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "def compute_metric_2(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += min(row['campaign'], )\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} == '{1}'\".format(attrib, value)\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute_range(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    query_strings = []\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} >= {1} & {2} < {3}\".format(attrib, value[0], attrib, value[1])\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "        query_strings.append(v_query)\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals, query_strings\n",
    "\n",
    "\n",
    "def find_combinations(sub_attributes, ratios):\n",
    "    num_iter = len(ratios)\n",
    "    sil_scores = []\n",
    "    # Making use of the K-Means algorithm ... number of centroids are from 2 to n-1.\n",
    "    for clust_num in range(2, num_iter):\n",
    "        kmeans = KMeans(n_clusters = clust_num)\n",
    "        kmeans.fit(ratios.reshape(-1,1))\n",
    "        results = kmeans.labels_\n",
    "        sil_scores.append((silhouette_score(ratios.reshape(-1,1), results, metric='euclidean'), results, clust_num))\n",
    "#     print(sil_scores)\n",
    "    # We make use of the silhouette score to determine the ideal number of centroids.\n",
    "    sorted_sil_scores = sorted(sil_scores, key=lambda x: x[0], reverse = True)\n",
    "    # We then use this ideal number of centroids to determine which sub attributes should be aggregated.\n",
    "    joined_sub_attributes = []\n",
    "    for i in range(0, sorted_sil_scores[0][2]):\n",
    "        joined_sub_attributes.append([])\n",
    "    join_list = sorted_sil_scores[0][1]\n",
    "    for index, value in enumerate(join_list):\n",
    "        pos = join_list[index]\n",
    "        joined_sub_attributes[pos].append(sub_attributes[index])\n",
    "    return_joined_sub_attributes = []\n",
    "    for arr in joined_sub_attributes:\n",
    "        similar_els_gp = []\n",
    "        for item in arr:\n",
    "            similar_els_gp.append(str(item))\n",
    "        return_joined_sub_attributes.append(similar_els_gp)\n",
    "#     print(return_joined_sub_attributes)\n",
    "    return return_joined_sub_attributes\n",
    "\n",
    "# The following is the format of the way in which this method should be called.\n",
    "# find_combinations(['a', 'b', 'c', 'd'], np.array([1, 4, 7, 90]), \"job\").\n",
    "\n",
    "def find_all_cust_feature_set(fs, df):\n",
    "    comb = {\n",
    "        'education':fs[0], \n",
    "         'job':fs[1], \n",
    "         'marital':fs[2], \n",
    "         'default':fs[3], \n",
    "         'loan':fs[4], \n",
    "         'housing':fs[5]\n",
    "    }\n",
    "    res_1 = df.query(fs[6])\n",
    "    res_2 = res_1.query(fs[7])\n",
    "    res_final = extract_rows_feature_set(res_2, comb)\n",
    "    return res_final\n",
    "\n",
    "\n",
    "def construct_hull_points(results, max_calls):\n",
    "    pts = []\n",
    "    for x in range(0, max_calls):\n",
    "        s = results[x]['succ']\n",
    "        c = results[x]['total_calls']\n",
    "        pts.append([c,s])\n",
    "#     print(\"Num points is \", len(pts))\n",
    "    pts = np.array(pts)\n",
    "    try:\n",
    "        hull = ConvexHull(pts)\n",
    "        verts = hull.vertices\n",
    "#         print(pts)\n",
    "#         plt.plot(pts[:,0], pts[:,1], 'o')\n",
    "#         for simplex in hull.simplices:\n",
    "#             plt.plot(pts[simplex, 0], pts[simplex, 1], 'k-')\n",
    "        if not np.isin(max_calls - 1, verts):\n",
    "            verts = np.append(max_calls - 1, verts)\n",
    "        verts = np.sort(verts)\n",
    "        return verts.tolist()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def gradient_update(key, fs_pick):\n",
    "    fs = fs_pick[key]\n",
    "    fs_results = fs['results']\n",
    "    hull_pts = fs['hull_points']\n",
    "    loc = fs['loc']\n",
    "    max_loc = fs['max_num_pts']\n",
    "    grad = 0.0\n",
    "    if loc <= max_loc:\n",
    "        if loc == 0:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'], fs_results[hull_pts[loc]]['total_calls'])\n",
    "        else:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'] - fs_results[hull_pts[loc-1]]['succ'] , fs_results[hull_pts[loc]]['total_calls'] - fs_results[hull_pts[loc-1]]['total_calls'])\n",
    "        fs_pick[key]['grad'] = grad\n",
    "    else:\n",
    "        fs_pick[key]['finished'] = True\n",
    "\n",
    "        \n",
    "def get_features(row, feature_names):\n",
    "    fs = []\n",
    "    for index, val in enumerate(feature_names):\n",
    "        if int(row[index]) == 1:\n",
    "            fs.append(val)\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell holds functions that are utilized by each of the methods defined.\n",
    "\n",
    "def group_age(row, age_ranges):\n",
    "#     print(age_ranges)\n",
    "    age = int(row['age'])\n",
    "    age_val = None\n",
    "    for index, age_range in enumerate(age_ranges):\n",
    "        if op.ge(age, age_range[0]) and op.le(age, age_range[1]):\n",
    "            age_val = index + 1\n",
    "    if age_val == None:\n",
    "        print(\"Failed Assignment for age: \", age)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'age'] = age_val\n",
    "    return age_val\n",
    "        \n",
    "\n",
    "def group_balance(row, balance_ranges):\n",
    "#     print(balance_ranges)\n",
    "    bal = int(row['balance'])\n",
    "    bal_val = None\n",
    "    for index, balance_range in enumerate(balance_ranges):\n",
    "        if op.ge(bal, balance_range[0]) and op.le(bal, balance_range[1]):\n",
    "            bal_val = index + 1\n",
    "    if bal_val == None:\n",
    "        print(\"Failed Assignment for balance: \", bal)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'balance'] = bal_val\n",
    "    return bal_val\n",
    "\n",
    "\n",
    "def group_feature(df, col_name, func, ranges):\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, col_name] = func(row, ranges)\n",
    "\n",
    "def compute_ratio_all_users(df, train_indicies):\n",
    "    ratio_values = []\n",
    "    for val in train_indicies:\n",
    "        row = df.iloc[val]\n",
    "        if row['y'] == \"yes\":\n",
    "            ratio_values.append((val, div(1, row['campaign'])))\n",
    "        else:\n",
    "            ratio_values.append((val, 0.0))\n",
    "    return ratio_values\n",
    "\n",
    "\n",
    "def compute_freq_percentage(mappings):\n",
    "    total = 0\n",
    "    for user_mapping in mappings.keys():\n",
    "        total += mappings[user_mapping]['freq']\n",
    "    for user_mapping in mappings.keys():\n",
    "        mappings[user_mapping]['percentage'] = div(mappings[user_mapping]['freq'], total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell represents the logic required for the inital approach taken.\n",
    "# Yields really good results thus far.\n",
    "# This should remain untouched.\n",
    "\n",
    "def construct_feature_combs(train_df, test_df, min_row_fs, age_ranges, balance_ranges, max_calls):\n",
    "    # At this point, we can run computations for the success rate of each sub attribute and join\n",
    "    # the sub-attributes based on the output of k-means.\n",
    "    poss = []\n",
    "\n",
    "    # Education.\n",
    "    all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "    education_cmbs = find_combinations(all_ed, metric_vals)\n",
    "\n",
    "    # Occupation.\n",
    "    all_jobs = ['student', 'retired', 'unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_jobs, train_df, 'job')\n",
    "    job_cmbs = find_combinations(all_jobs, metric_vals)\n",
    "\n",
    "    # Marital.\n",
    "    all_ms = ['married', 'single', 'divorced']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ms, train_df, 'marital')\n",
    "    marital_cmbs = find_combinations(all_ms, metric_vals)\n",
    "\n",
    "    # Default\n",
    "    all_def = ['no', 'yes']\n",
    "    default_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Loan\n",
    "    all_ln = ['no', 'yes']\n",
    "    loan_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Housing\n",
    "    all_hs = ['no', 'yes']\n",
    "    housing_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    poss.append(education_cmbs)\n",
    "    poss.append(marital_cmbs)\n",
    "    poss.append(job_cmbs)\n",
    "    poss.append(default_cmbs)\n",
    "    poss.append(loan_cmbs)\n",
    "    poss.append(housing_cmbs)\n",
    "    all_combs = list(itertools.product(*poss))\n",
    "\n",
    "    # print(\"Number of combinations: \", len(all_combs)* len(age_query_strings) * len(balance_query_strings))\n",
    "\n",
    "    # We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "    num_iter = 0\n",
    "    combs_to_consider = {}\n",
    "    fs_pick = {}\n",
    "\n",
    "    # Setting up looping structures to generate all possibilities.\n",
    "    for age_range in age_ranges:\n",
    "        age_query = \"age >= {0} & age <= {1}\".format(age_range[0], age_range[1])\n",
    "        df_filtered_final = train_df.query(age_query)\n",
    "        for bal_range in balance_ranges:\n",
    "            bal_query = \"balance >= {0} & balance <= {1}\".format(bal_range[0], bal_range[1])\n",
    "            df_filtered_final_2 = df_filtered_final.query(bal_query)\n",
    "            for comb in all_combs:\n",
    "                dict_final_query = construct_dict(comb)\n",
    "                num_iter += 1\n",
    "                extracted_df = extract_rows_feature_set(df_filtered_final_2, dict_final_query)\n",
    "                key = (dict_final_query['education'], dict_final_query['job'], \n",
    "                       dict_final_query['marital'], dict_final_query['default'], \n",
    "                       dict_final_query['loan'], dict_final_query['housing'], \n",
    "                       bal_query, age_query)\n",
    "                n_rows = extracted_df.shape[0]\n",
    "                if n_rows >= min_row_fs:\n",
    "                    results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "#                             max_loc = compute_optimal_call_no(results)\n",
    "#                             if max_loc != -1:\n",
    "                    combs_to_consider[key] = {\n",
    "                                                'max_loc':0,\n",
    "                                                'best_rate':0, \n",
    "                                                'overall_rate':results[max_calls-1]['expected'], \n",
    "                                                'n_rows':n_rows, \n",
    "                                                'results':results,\n",
    "                                                'fs_customers':None\n",
    "                                             }\n",
    "                    fs_pick[key] = {\n",
    "                                    'grad': 0.0, \n",
    "                                    'loc':0, \n",
    "                                    'finished':False, \n",
    "                                    'hull_points':None,\n",
    "                                    'max_num_pts': -1,\n",
    "                                    'results':None,\n",
    "                                    'fs_customers' :None,\n",
    "                                    'convex_hull_fail': False\n",
    "                                   }\n",
    "#               else:\n",
    "#                   print(\"Invalid FS ! -> \", n_rows)\n",
    "    for fs_key in combs_to_consider.keys():\n",
    "        fs_customers = find_all_cust_feature_set(fs_key, test_df)\n",
    "        combs_to_consider[fs_key]['fs_customers'] = fs_customers\n",
    "        res = construct_hull_points(combs_to_consider[fs_key]['results'], max_calls)\n",
    "        fs_pick[fs_key]['results'] = combs_to_consider[fs_key]['results']\n",
    "        fs_pick[fs_key]['fs_customers'] = fs_customers\n",
    "        if res is False:\n",
    "            # print(\"Invalid Convex Hull Assignment\")\n",
    "            fs_pick[fs_key]['finished'] = True\n",
    "            fs_pick[fs_key]['convex_hull_fail'] = True\n",
    "        else:\n",
    "            fs_pick[fs_key]['hull_points'] = res\n",
    "            fs_pick[fs_key]['max_num_pts'] = len(res) - 1\n",
    "    return combs_to_consider, fs_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each function represents each method attempted.\n",
    "\n",
    "def call_everyone(test_df):\n",
    "    print(\"Call all Customers Approach\")\n",
    "    call_check_points = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, 96500, 97000, 97500, 98000, 98500, 99000, 99500, 100000, 100500, 101000, 101500, 102000, 102500, 103000, 103500, 104000, 104500, 105000, 105500, 106000, 106500, 107000, 107500, 108000, 108500, 109000, 109500, 110000, 110500, 111000, 111500, 112000, 112500, 113000, 113500, 114000, 114500, 115000, 115500, 116000, 116500, 117000, 117500, 118000, 118500, 119000, 119500, 120000, 120500, 121000, 121500, 122000, 122500, 123000, 123500, 124000, 124500, 125000, 125500, 126000, 126500, 127000, 127500, 128000, 128500, 129000, 129500, 130000, 130500, 131000, 131500, 132000, 132500, 133000, 133500, 134000, 134500, 135000, 135500, 136000, 136500, 137000, 137500, 138000, 138500, 139000, 139500]\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    cp_loc = 0\n",
    "    res = test_df.reindex(np.random.permutation(test_df.index))\n",
    "    for loc, row in res.iterrows():\n",
    "        if sc_tracker.get_calls() >= call_check_points[cp_loc]:\n",
    "            cp_loc += 1\n",
    "            sc_tracker.save()\n",
    "        sc_tracker.add_calls(row['campaign'])\n",
    "        if row['y'] == \"yes\":\n",
    "            sc_tracker.add_succ()\n",
    "    sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score(), sc_tracker.get_calls(), sc_tracker.get_all_calls_succ()\n",
    "\n",
    "\n",
    "def greedy_approach(combs_to_consider):\n",
    "    print(\"Greedy Approach\")\n",
    "    persons_to_call_overall = {k: v for k, v in sorted(combs_to_consider.items(), key=lambda fs: fs[1]['overall_rate'], reverse = True)}\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    result_ratios = []\n",
    "    for key in persons_to_call_overall.keys():\n",
    "        for loc, cust in persons_to_call_overall[key]['fs_customers'].iterrows():\n",
    "            sc_tracker.add_calls(cust['campaign'])\n",
    "            if cust['y'] == \"yes\":\n",
    "                sc_tracker.add_succ()\n",
    "        sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score(), sc_tracker.get_all_calls_succ(), persons_to_call_overall\n",
    "\n",
    "\n",
    "def convex_hull(fs_pick, num_calls):\n",
    "    print(\"Gradient Ascent Approach\")\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    for key in fs_pick.keys():\n",
    "        gradient_update(key, fs_pick)\n",
    "    # Sort based on gradient.\n",
    "    optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    # Call best feature set, update gradient for this feature set and re-sort all feature sets.\n",
    "    # Rinse and repeat!\n",
    "    while(sc_tracker.get_calls() <= num_calls):\n",
    "        best_loc = 0\n",
    "        while(best_loc < len(optimal_choices) and optimal_choices[best_loc][1]['finished'] == True):\n",
    "            best_loc += 1\n",
    "        if best_loc == len(optimal_choices):\n",
    "            break\n",
    "        fs_key = optimal_choices[best_loc][0]\n",
    "        fs_data = optimal_choices[best_loc][1]\n",
    "        if fs_data['finished'] == False:\n",
    "            loc = fs_data['loc']\n",
    "            if loc == 0:\n",
    "                call_start = 1\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            else:\n",
    "                call_start = fs_data['hull_points'][loc-1] + 2\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            for call in range(call_start, call_end + 1, 1):\n",
    "                for loc, row in fs_pick[fs_key]['fs_customers'].iterrows():\n",
    "                    if row['campaign'] == call:\n",
    "                        sc_tracker.add_calls(1)\n",
    "                        if row['y'] == \"yes\":\n",
    "                            sc_tracker.add_succ()\n",
    "                    elif row['campaign'] > call:\n",
    "                        sc_tracker.add_calls(1)\n",
    "            sc_tracker.save()\n",
    "            fs_pick[fs_key]['loc'] += 1\n",
    "            gradient_update(fs_key, fs_pick)\n",
    "            optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    # In instances where the convex hull faield to generate, we simply call the remainder of cusomers.\n",
    "    for key in fs_pick.keys():\n",
    "        if fs_pick[key]['convex_hull_fail'] == True:\n",
    "            for index, row in fs_pick[key]['fs_customers'].iterrows():\n",
    "                if row['y'] == 'yes':\n",
    "                    sc_tracker.add_succ()\n",
    "                sc_tracker.add_calls(row['campaign'])\n",
    "            sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score(), sc_tracker.get_all_calls_succ(), optimal_choices\n",
    "\n",
    "\n",
    "def upper_bound(test_df, max_calls):\n",
    "    print(\"Upper Bound Approach\")\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    res_df = test_df.query(\"y == 'yes'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        for i in range(0, num_cust):\n",
    "            sc_tracker.add_succ()\n",
    "        sc_tracker.add_calls((num_cust * x))\n",
    "        sc_tracker.save()\n",
    "    res_df = test_df.query(\"y == 'no'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        sc_tracker.add_calls((num_cust * x))\n",
    "        sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score(), sc_tracker.get_all_calls_succ()\n",
    "    \n",
    "    \n",
    "def new_approach_ratio_grouping_percentage(df, train_indicies, test_indicies, group_size, age_groupings, balance_groupings):\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    # Ensuring that we can binary encode any row in our dataset. We also group age and balance values \n",
    "    # from each row into ranges.\n",
    "    df_copy = df.copy(deep=True)\n",
    "    group_feature(df_copy, \"age\", group_age, age_groupings)\n",
    "    group_feature(df_copy, \"balance\", group_balance, balance_groupings)\n",
    "    # Build the customers for each group.\n",
    "    ratio_arr = compute_ratio_all_users(df_copy, train_indicies)\n",
    "    ratio_arr_sorted = sorted(ratio_arr, key=lambda tup: tup[1], reverse = True)\n",
    "    train_size = len(ratio_arr_sorted)\n",
    "    groupings = {}\n",
    "    for loc in range(0, train_size):\n",
    "        group_key = str(int(loc/group_size))\n",
    "        if group_key not in groupings.keys():\n",
    "            groupings[group_key] = {'indicies':[], 'mappings':{}, 'results':None} \n",
    "        groupings[group_key]['indicies'].append(ratio_arr_sorted[loc][0])\n",
    "    # For each group, we find the unique feature combinations and store them in a list. \n",
    "    # We also store the results - s/c ratio for call numbers from 1-20.\n",
    "    test_calls = {}\n",
    "    for group_key in groupings.keys():\n",
    "        users_df = df_copy.iloc[groupings[group_key]['indicies']]\n",
    "        mappings = groupings[group_key]['mappings']\n",
    "        for row in users_df.itertuples():\n",
    "            user_mapping = str((row.job, row.marital, row.education, row.default,\n",
    "                               row.housing, row.loan, row.age, row.balance))\n",
    "            if user_mapping not in mappings.keys():\n",
    "                mappings[user_mapping] = {'freq':0, 'percentage':0.0}\n",
    "            else:\n",
    "                mappings[user_mapping]['freq'] += 1\n",
    "        groupings[group_key]['results'] = compute_expected_succ_per_call_rate_feature_set(users_df, 20)\n",
    "        compute_freq_percentage(mappings)\n",
    "        test_calls[group_key] = {'locs_to_call':[], 'overall_rate':groupings[group_key]['results'][19]}\n",
    "    # print(test_calls)\n",
    "    # For the test set, we need to map each user to the most appropriate cluster.\n",
    "    missed = 0\n",
    "    for loc in test_indicies:\n",
    "        row = df_copy.iloc[loc]\n",
    "        user_mapping = str((row['job'], row['marital'], row['education'], row['default'],\n",
    "                            row['housing'], row['loan'], row['age'], row['balance']))\n",
    "        all_groupings_keys = list(groupings.keys())\n",
    "        best_group_key = None\n",
    "        best_ratio = -1.0\n",
    "        for group_key in all_groupings_keys:\n",
    "            if user_mapping in groupings[group_key]['mappings']:\n",
    "                if best_group_key is None:\n",
    "                    best_group_key = group_key\n",
    "                    best_ratio = groupings[group_key]['mappings'][user_mapping]['percentage']\n",
    "                else:\n",
    "                    if groupings[best_group_key]['mappings'][user_mapping]['percentage'] > best_ratio:\n",
    "                        best_group_key = group_key\n",
    "                        best_ratio = groupings[group_key]['mappings'][user_mapping]['percentage']\n",
    "        if best_group_key is not None:\n",
    "            test_calls[best_group_key]['locs_to_call'].append(loc)\n",
    "        else:\n",
    "            missed += 1\n",
    "    print(\"We missed:\", missed)\n",
    "    # Call users ... those with the highest ratios are called first.\n",
    "    test_calls_sorted = sorted(test_calls.items(), key=lambda fs: fs[1]['overall_rate']['expected'], reverse = True)\n",
    "    print(\"Step 5\")\n",
    "    for test_call in test_calls_sorted:\n",
    "        for cust_loc in test_call[1]['locs_to_call']:\n",
    "            row = df_copy.iloc[cust_loc]\n",
    "            if row['y'] == \"yes\":\n",
    "                sc_tracker.add_succ()\n",
    "            sc_tracker.add_calls(row['campaign'])\n",
    "        sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score(), groupings, sc_tracker.get_all_calls_succ()\n",
    "\n",
    "\n",
    "def clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded):\n",
    "    groupings = {}\n",
    "    predictions = clusterer.labels_\n",
    "    # We assign to each group, the similar indicies. This was based on the clustering approach.\n",
    "    for index, group in enumerate(predictions):\n",
    "        if str(group) not in groupings.keys():\n",
    "            groupings[str(group)] = {'train_indicies':[], 'unique_keys':{}, 'results':None, 'test_indicies':[]}\n",
    "        groupings[str(group)]['train_indicies'].append(index)\n",
    "    # For all customers belonging to each grouping, we find the unique keys and compute the success per call\n",
    "    # ratio for call numbers 1-20.\n",
    "    for group in groupings.keys():\n",
    "        for index in groupings[group]['train_indicies']:\n",
    "            cust_info = train_df.iloc[index]\n",
    "            cust_features = get_features(train_df_encoded[index], feature_names)\n",
    "            # cust_features = cust_features[0:8]\n",
    "            if str(cust_features) not in groupings[group]['unique_keys'].keys():\n",
    "                groupings[group]['unique_keys'][str(cust_features)] = {'#_ocurr': 1}\n",
    "            else:\n",
    "                groupings[group]['unique_keys'][str(cust_features)]['#_ocurr'] += 1\n",
    "        results = compute_expected_succ_per_call_rate_feature_set(train_df.iloc[groupings[group]['train_indicies']], 20)\n",
    "        groupings[group]['results'] = results\n",
    "    # This process makes use of the test set and determines the ideal cluster for a customer.\n",
    "    for index in range(0, len(test_df_encoded), 1):\n",
    "        encoded_customer_data = test_df_encoded[index]\n",
    "        test_labels, strengths = hdbscan.approximate_predict(clusterer, [encoded_customer_data])\n",
    "        groupings[str(test_labels[0])]['test_indicies'].append(index)\n",
    "    # Perform sorting of groups based on success per call rate.\n",
    "    sorted_final_call = {k: v for k, v in sorted(groupings.items(), key=lambda item: item[1]['results'][19]['expected'], reverse = True)}\n",
    "    # Go about calling customers, keep track of the success per call rate as we switch from group to group.\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    for group in sorted_final_call:\n",
    "        for cust_index in sorted_final_call[group]['test_indicies']:\n",
    "            row = test_df.iloc[cust_index]\n",
    "            if row['y'] == \"yes\":\n",
    "                sc_tracker.add_succ()\n",
    "            sc_tracker.add_calls(row['campaign'])\n",
    "        sc_tracker.save()\n",
    "    return sc_tracker.get_succ_calls_score(), groupings, sc_tracker.get_all_calls_succ()\n",
    "\n",
    "\n",
    "def tree_approaches_abstracted(model, mkt_df_filtered_cp, train_index, test_index):\n",
    "    # Encode Features.\n",
    "    features_to_transform = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'y']\n",
    "    for feature in features_to_transform:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(mkt_df_filtered_cp[feature])\n",
    "        mkt_df_filtered_cp[feature] = le.transform(mkt_df_filtered_cp[feature])\n",
    "    # Split into train/test.\n",
    "    train_df = mkt_df_filtered_cp.iloc[train_index]\n",
    "    test_df = mkt_df_filtered_cp.iloc[test_index]\n",
    "    # Split in to X and y.\n",
    "    feature_y = \"campaign\"\n",
    "    feature_z = \"y\"\n",
    "    train_y = train_df[feature_y]\n",
    "    train_X = train_df.drop(columns = [feature_y, feature_z])\n",
    "    test_y = test_df[feature_y]\n",
    "    test_X = test_df.drop(columns = [feature_y, feature_z])\n",
    "    # Fit model and get predictions.\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions = model.predict(test_X)\n",
    "    # The rest is the approach to compute points for AUC metric.\n",
    "    sc_tracker = Succ_Call_Tracker()\n",
    "    j = 0\n",
    "    for index, row in test_X.iterrows():\n",
    "        if int(predictions[j]) >= test_y.iloc[j]:\n",
    "            if test_df.iloc[j]['y'] == 1:\n",
    "                sc_tracker.add_succ()\n",
    "        sc_tracker.add_calls(predictions[j])\n",
    "        sc_tracker.save()\n",
    "        j += 1\n",
    "    return sc_tracker.get_succ_calls_score(), sc_tracker.get_all_calls_succ()\n",
    "        \n",
    "        \n",
    "# The encoding process can also be varied to not include age and balance.\n",
    "def clustering_age_balance_grouped(mkt_df_filtered, train_index, test_index, min_cluster_size, balance_groupings, age_groupings):\n",
    "    print(\"HDBScan Clustering - Groupings\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "    group_feature(mkt_df_filtered_cp, \"age\", group_age, age_groupings)\n",
    "    group_feature(mkt_df_filtered_cp, \"balance\", group_balance, balance_groupings)\n",
    "    train_df = mkt_df_filtered_cp.iloc[train_index]\n",
    "    test_df = mkt_df_filtered_cp.iloc[test_index]\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(mkt_df_filtered_cp.drop(columns = ['y', 'campaign']))\n",
    "    train_df_encoded = encoder.transform(train_df.drop(columns = ['y', 'campaign'])).toarray()\n",
    "    test_df_encoded = encoder.transform(test_df.drop(columns = ['y', 'campaign'])).toarray()\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "    clusterer.fit(train_df_encoded)\n",
    "    return clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded)\n",
    "\n",
    "\n",
    "# The encoding process can also be varied to not include age and balance.\n",
    "def clustering_age_balance_not_grouped(mkt_df_filtered, train_index, test_index, min_cluster_size, balance_groupings, age_groupings):\n",
    "    print(\"HDBScan Clustering - No Groupings\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "    train_df = mkt_df_filtered_cp.iloc[train_index]\n",
    "    test_df = mkt_df_filtered_cp.iloc[test_index]\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(mkt_df_filtered_cp.drop(columns = ['y', 'campaign', 'age', 'balance']))\n",
    "    train_df_encoded = encoder.transform(train_df.drop(columns = ['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "    train_df_encoded = np.column_stack((train_df_encoded, train_df['age'].to_numpy()))\n",
    "    train_df_encoded = np.column_stack((train_df_encoded, train_df['balance'].to_numpy()))\n",
    "    test_df_encoded = encoder.transform(test_df.drop(columns = ['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "    test_df_encoded = np.column_stack((test_df_encoded, test_df['age'].to_numpy()))\n",
    "    test_df_encoded = np.column_stack((test_df_encoded, test_df['balance'].to_numpy()))\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan'])\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "    clusterer.fit(train_df_encoded)\n",
    "    return clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded)\n",
    "\n",
    "# def clustering_age_balance_grouped_ratio_gradient_ascent(train_df, test_df, min_cluster_size, balance_groupings, age_groupings, num_calls):\n",
    "#     print(\"In P8\")\n",
    "#     group_age(train_df, age_groupings)\n",
    "#     group_balance(train_df, balance_groupings)\n",
    "#     train_df, ratio_df = compute_ratio_all_users(train_df)\n",
    "#     encoder = OneHotEncoder()\n",
    "#     train_df_encoded = encoder.fit_transform(train_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "#     train_df_encoded = np.column_stack((train_df_encoded, ratio_df['ratio'].to_numpy()))\n",
    "#     feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "#     clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "#     clusterer.fit(train_df_encoded)\n",
    "#     predictions = clusterer.labels_\n",
    "#     test_df = mkt_df_filtered.iloc[test_index]\n",
    "#     group_age(test_df, age_groupings)\n",
    "#     group_balance(test_df, balance_groupings)\n",
    "#     test_df_encoded = encoder.fit_transform(test_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "# #     return None, None, (None, None)\n",
    "#     return abstraction_new_approach_gradient_ascent(predictions, feature_names, train_df, test_df, train_df_encoded, test_df_encoded, num_calls)\n",
    "\n",
    "\n",
    "def decision_tree_multiclass(mkt_df_filtered, train_index, test_index):\n",
    "    print(\"Decision Tree\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "#     criterion='entropy', max_depth= 28, min_impurity_decrease= 0.00005\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    return tree_approaches_abstracted(model, mkt_df_filtered_cp, train_index, test_index)\n",
    "\n",
    "\n",
    "def xgboost_multiclass(mkt_df_filtered, train_index, test_index):\n",
    "    print(\"XGBoost\")\n",
    "    mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "    model2 = XGBClassifier()\n",
    "    return tree_approaches_abstracted(model2, mkt_df_filtered_cp, train_index, test_index)\n",
    "\n",
    "\n",
    "# def decision_tree_multiclass2(mkt_df_filtered, train_index, test_index):\n",
    "#     print(\"Decision Tree\")\n",
    "#     mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "# #     criterion='entropy', max_depth= 28, min_impurity_decrease= 0.00005\n",
    "#     model = tree.DecisionTreeClassifier(criterion= 'gini', max_depth= 1, min_impurity_decrease= 5e-05)\n",
    "#     return tree_approaches_abstracted(model, mkt_df_filtered_cp, train_index, test_index)\n",
    "\n",
    "\n",
    "# def xgboost_multiclass2(mkt_df_filtered, train_index, test_index):\n",
    "#     print(\"XGBoost\")\n",
    "#     mkt_df_filtered_cp = mkt_df_filtered.copy(deep = True)\n",
    "#     model2 = XGBClassifier()\n",
    "#     return tree_approaches_abstracted(model2, mkt_df_filtered_cp, train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays average over all folds.\n",
    "def compute_score_folds(fold_batch):\n",
    "    average_area = {}\n",
    "\n",
    "    for fold_num in fold_batch.keys():\n",
    "        fold_data = fold_batch[fold_num]\n",
    "        for key in fold_data.keys():\n",
    "            average_area[key] = {'all_scores': [], 'avg':0.0, 'ratio':0.0}\n",
    "        break\n",
    "\n",
    "    for fold_num in fold_batch.keys():\n",
    "        fold_data = fold_batch[fold_num]\n",
    "        for key in fold_data.keys():\n",
    "            average_area[key]['all_scores'].append(fold_data[key][2])\n",
    "\n",
    "    for key in average_area.keys():\n",
    "        average_area[key]['avg'] = statistics.mean(average_area[key]['all_scores'])\n",
    "\n",
    "    baseline_val = average_area['BL']['avg']\n",
    "    for key in average_area.keys():\n",
    "        print(key)\n",
    "        print(average_area[key]['all_scores'])\n",
    "        print('Average: ', average_area[key]['avg'])\n",
    "        average_area[key]['ratio'] = average_area[key]['avg']/baseline_val\n",
    "        print('Ratio: ', average_area[key]['avg']/baseline_val)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return average_area\n",
    "\n",
    "\n",
    "# Generate and save graphs.\n",
    "def gen_save_graphs(fold_results, folder_name, test_name):\n",
    "    \n",
    "    average_area = compute_score_folds(fold_results)\n",
    "\n",
    "    dir = os.getcwd() + '/Plots/' + folder_name + '/' + test_name\n",
    "    main_dir = os.getcwd() + '/Plots/' + folder_name + '/'\n",
    "    if not os.path.exists(dir):\n",
    "        if not os.path.exists(main_dir):\n",
    "            os.mkdir(main_dir)\n",
    "        os.mkdir(os.getcwd() + '/Plots/' + folder_name + '/' + test_name)\n",
    "\n",
    "    for fold_num in fold_results.keys():\n",
    "        plt.ylabel(\"Number of Successes\")\n",
    "        plt.xlabel(\"Number of Calls\")\n",
    "        fold_data = fold_results[fold_num]\n",
    "        for key in fold_data.keys():\n",
    "            plt.plot(fold_data[key][0], fold_data[key][1], label=key, linewidth=1)\n",
    "        plt.legend()\n",
    "        plt.savefig(dir+'/'+str(fold_num)+'.pdf')\n",
    "        plt.clf()\n",
    "\n",
    "    with open(dir + '/data.json', 'w') as fp:\n",
    "        json.dump(average_area, fp)\n",
    "    fp.close()\n",
    "\n",
    "        \n",
    "def save_customer_segmentation_data(fold_data, folder_name, test_name):\n",
    "    dir = os.getcwd() + '/Plots/' + folder_name + '/' + test_name \n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    filehandler = open(dir + '/cs_data.json', 'wb')\n",
    "    pickle.dump(fold_data, filehandler)\n",
    "\n",
    "\n",
    "def get_succ_calls_score_limited(calls, succ, call_limit):\n",
    "    for index, call in enumerate(calls):\n",
    "        if call > call_limit:\n",
    "            break\n",
    "    calls_limited = calls[:index]\n",
    "    succ_limited = succ[:index]\n",
    "    return (calls_limited, succ_limited, metrics.auc(calls_limited, succ_limited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 17)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# Code that sets up values to construct all possible feature combinations.\n",
    "\n",
    "# Query Strings with filename being 'mod_1' \n",
    "# age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "# age_ranges_2 = [(10, 34), (35, 45), (46, 100)]\n",
    "# balance_query_strings = ['balance <= 450',' balance > 450']\n",
    "# balance_ranges_2 = [(-10000, 450), (451, 105000)]\n",
    "\n",
    "# Defaults based on separating groups into even bins.\n",
    "age_ranges_1 = [(18,30), (31, 47), (48,64), (65,100)]\n",
    "balance_ranges_1 = [(-10000, 450), (451, 105000)]\n",
    "\n",
    "# Decision Tree Splits ..\n",
    "age_ranges_2 = [(18, 60), (61, 100)]\n",
    "balance_ranges_2 = [(-10000, 798), (799, 105000)]\n",
    "\n",
    "age_ranges_3 = [(18, 29), (30, 89), (90,100)]\n",
    "balance_ranges_3 = [(-10000, 60), (61, 1578), (1579, 105000)]\n",
    "\n",
    "age_ranges_4 = [(18, 25), (26, 59), (60, 87), (88, 93), (94, 100)]\n",
    "balance_ranges_4 = [(-10000, -46), (-45, 194), (195, 804), (805, 13112), (13113, 105000)]\n",
    "\n",
    "age_ranges_5 = [(18, 22), (23, 28), (29, 36), (37, 83), (84, 94), (95, 100)]\n",
    "balance_ranges_5 = [(-10000, -167), (-166, -45), (-44, 170), (171, 776), (777, 800), (801, 1547), (1548, 10179),\n",
    "                    (10180, 17907), (17908, 105000)]\n",
    "\n",
    "age_ranges_6 = [(18, 24), (25, 26), (27, 30), (31, 58), (59, 82), (83, 86), (87, 100)]\n",
    "balance_ranges_6 = [(-10000, -192), (-191, -130), (-129, 5), (6, 62), (63, 179), (180, 773), (774, 790), \n",
    "                    (791, 799), (800, 803), (804, 1546), (1547, 1558), (1559, 5519), (5520, 10187), (10188, 15498),\n",
    "                    (15499, 18221), (18222, 105000)]\n",
    "\n",
    "\n",
    "age_balance_ranges = []\n",
    "\n",
    "# age_balance_ranges.append((age_ranges_1, balance_ranges_1, 'old'))\n",
    "\n",
    "# age_balance_ranges.append((age_ranges_2, balance_ranges_2, 'gp_1a'))\n",
    "# age_balance_ranges.append((age_ranges_2, balance_ranges_3, 'gp_1b'))\n",
    "# age_balance_ranges.append((age_ranges_2, balance_ranges_4, 'gp_1c'))\n",
    "\n",
    "# age_balance_ranges.append((age_ranges_3, balance_ranges_2, 'gp_2a'))\n",
    "# age_balance_ranges.append((age_ranges_3, balance_ranges_3, 'gp_2b'))\n",
    "# age_balance_ranges.append((age_ranges_3, balance_ranges_4, 'gp_2c'))\n",
    "\n",
    "# age_balance_ranges.append((age_ranges_4, balance_ranges_2, 'gp_3a'))\n",
    "# age_balance_ranges.append((age_ranges_4, balance_ranges_3, 'gp_3b'))\n",
    "# age_balance_ranges.append((age_ranges_4, balance_ranges_4, 'gp_3c'))\n",
    "\n",
    "age_balance_ranges.append((age_ranges_2, balance_ranges_5, 'gp_1d'))\n",
    "age_balance_ranges.append((age_ranges_3, balance_ranges_5, 'gp_2d'))\n",
    "age_balance_ranges.append((age_ranges_4, balance_ranges_5, 'gp_3d'))\n",
    "\n",
    "age_balance_ranges.append((age_ranges_5, balance_ranges_2, 'gp_4a'))\n",
    "age_balance_ranges.append((age_ranges_5, balance_ranges_3, 'gp_4b'))\n",
    "age_balance_ranges.append((age_ranges_5, balance_ranges_4, 'gp_4c'))\n",
    "age_balance_ranges.append((age_ranges_5, balance_ranges_5, 'gp_4d'))\n",
    "\n",
    "# age_balance_ranges.append((age_ranges_5, balance_ranges_5, 'nab_5'))\n",
    "# age_balance_ranges.append((age_ranges_6, balance_ranges_6, 'nab_6'))\n",
    "\n",
    "# Max call number to consider.\n",
    "max_calls = 34\n",
    "\n",
    "# Pull and filter all calls <= max_calls.\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "mkt_df_filtered = mkt_df_filtered[['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance', 'campaign', 'y']]\n",
    "print(mkt_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only set random state to ensure that we have the same train and test indicies in comparison notebook!\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "fold_data = []\n",
    "\n",
    "for train_index, test_index in kf.split(mkt_df_filtered):\n",
    "    fold_data.append((train_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gp_1d\n",
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "BL\n",
      "[12865255.0, 13386055.0, 12979048.0, 13271005.5, 12714743.0]\n",
      "Average:  13043221.3\n",
      "Ratio:  1.0\n",
      "\n",
      "\n",
      "GC\n",
      "[17392238.5, 17772657.5, 16892020.0, 17528299.5, 17205807.5]\n",
      "Average:  17358204.6\n",
      "Ratio:  1.330821903635109\n",
      "\n",
      "\n",
      "GA\n",
      "[17942983.0, 18145346.5, 17173096.5, 17822581.5, 17599956.5]\n",
      "Average:  17736792.8\n",
      "Ratio:  1.3598475707837603\n",
      "\n",
      "\n",
      "UB\n",
      "[24907304.5, 25896082.0, 24939222.5, 26071400.5, 25365304.5]\n",
      "Average:  25435862.8\n",
      "Ratio:  1.9501212327049913\n",
      "\n",
      "\n",
      "gp_2d\n",
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "BL\n",
      "[12942087.5, 13095298.0, 12786941.5, 13644887.0, 13021605.5]\n",
      "Average:  13098163.9\n",
      "Ratio:  1.0\n",
      "\n",
      "\n",
      "GC\n",
      "[17215186.5, 17725650.5, 16822107.5, 17404925.0, 17110408.5]\n",
      "Average:  17255655.6\n",
      "Ratio:  1.3174102669458885\n",
      "\n",
      "\n",
      "GA\n",
      "[17842076.5, 18074354.5, 17125339.0, 17712299.0, 17509386.5]\n",
      "Average:  17652691.1\n",
      "Ratio:  1.3477225689625094\n",
      "\n",
      "\n",
      "UB\n",
      "[24907304.5, 25896082.0, 24939222.5, 26071400.5, 25365304.5]\n",
      "Average:  25435862.8\n",
      "Ratio:  1.9419410990879415\n",
      "\n",
      "\n",
      "gp_3d\n",
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "BL\n",
      "[12629976.5, 13106479.5, 12780417.5, 13234588.5, 12862481.5]\n",
      "Average:  12922788.7\n",
      "Ratio:  1.0\n",
      "\n",
      "\n",
      "GC\n",
      "[17216801.5, 17797021.5, 16976210.0, 17547440.5, 17353552.5]\n",
      "Average:  17378205.2\n",
      "Ratio:  1.3447720614668877\n",
      "\n",
      "\n",
      "GA\n",
      "[17715072.0, 18140195.0, 17031209.5, 17764746.0, 17475739.0]\n",
      "Average:  17625392.3\n",
      "Ratio:  1.363900061292498\n",
      "\n",
      "\n",
      "UB\n",
      "[24907304.5, 25896082.0, 24939222.5, 26071400.5, 25365304.5]\n",
      "Average:  25435862.8\n",
      "Ratio:  1.9682951869359284\n",
      "\n",
      "\n",
      "gp_4a\n",
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "BL\n",
      "[12740271.0, 13286048.5, 12828155.5, 13771869.0, 12874358.0]\n",
      "Average:  13100140.4\n",
      "Ratio:  1.0\n",
      "\n",
      "\n",
      "GC\n",
      "[17024924.0, 17769748.0, 16611987.5, 17326128.0, 17076154.5]\n",
      "Average:  17161788.4\n",
      "Ratio:  1.3100461427115695\n",
      "\n",
      "\n",
      "GA\n",
      "[17672406.0, 18344188.5, 17104659.0, 17989077.0, 17683763.5]\n",
      "Average:  17758818.8\n",
      "Ratio:  1.3556204939605074\n",
      "\n",
      "\n",
      "UB\n",
      "[24907304.5, 25896082.0, 24939222.5, 26071400.5, 25365304.5]\n",
      "Average:  25435862.8\n",
      "Ratio:  1.941648106305792\n",
      "\n",
      "\n",
      "gp_4b\n",
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "BL\n",
      "[12693242.0, 13701408.0, 12382686.0, 13451637.5, 13109230.0]\n",
      "Average:  13067640.7\n",
      "Ratio:  1.0\n",
      "\n",
      "\n",
      "GC\n",
      "[17121569.5, 17703807.0, 16779644.5, 17653329.0, 16955298.5]\n",
      "Average:  17242729.7\n",
      "Ratio:  1.3194983008677306\n",
      "\n",
      "\n",
      "GA\n",
      "[17763556.5, 18203454.0, 17264217.0, 17807388.0, 17632854.0]\n",
      "Average:  17734293.9\n",
      "Ratio:  1.357115205960629\n",
      "\n",
      "\n",
      "UB\n",
      "[24907304.5, 25896082.0, 24939222.5, 26071400.5, 25365304.5]\n",
      "Average:  25435862.8\n",
      "Ratio:  1.9464770561069988\n",
      "\n",
      "\n",
      "gp_4c\n",
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "BL\n",
      "[12866417.5, 13172469.0, 12648581.5, 13184923.0, 13409433.0]\n",
      "Average:  13056364.8\n",
      "Ratio:  1.0\n",
      "\n",
      "\n",
      "GC\n",
      "[17308196.0, 17816564.0, 16859424.5, 17359866.0, 17062854.5]\n",
      "Average:  17281381.0\n",
      "Ratio:  1.3235982039962608\n",
      "\n",
      "\n",
      "GA\n",
      "[17446895.0, 18156311.0, 17171403.0, 17835252.5, 17636412.0]\n",
      "Average:  17649254.7\n",
      "Ratio:  1.3517740175274513\n",
      "\n",
      "\n",
      "UB\n",
      "[24907304.5, 25896082.0, 24939222.5, 26071400.5, 25365304.5]\n",
      "Average:  25435862.8\n",
      "Ratio:  1.9481580968080794\n",
      "\n",
      "\n",
      "gp_4d\n",
      "At fold number:  1\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  2\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  3\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  4\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "At fold number:  5\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Upper Bound Approach\n",
      "\n",
      "\n",
      "BL\n",
      "[12914506.5, 13413908.5, 12632174.0, 13491670.5, 12779866.5]\n",
      "Average:  13046425.2\n",
      "Ratio:  1.0\n",
      "\n",
      "\n",
      "GC\n",
      "[17061238.5, 17581007.5, 16852786.5, 17298838.5, 16760010.5]\n",
      "Average:  17110776.3\n",
      "Ratio:  1.311529866434217\n",
      "\n",
      "\n",
      "GA\n",
      "[17309509.5, 17873965.0, 17023959.0, 17234817.0, 17204698.5]\n",
      "Average:  17329389.8\n",
      "Ratio:  1.3282864489193562\n",
      "\n",
      "\n",
      "UB\n",
      "[24907304.5, 25896082.0, 24939222.5, 26071400.5, 25365304.5]\n",
      "Average:  25435862.8\n",
      "Ratio:  1.9496423280761999\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "# These methods are dependent on the age and balance groupings.\n",
    "# Main code ... orchestrates everything!\n",
    "\n",
    "folder_name = str(datetime.now().strftime(\"%d-%m-%Y_%H-%M\"))\n",
    "\n",
    "for ab_group in age_balance_ranges:\n",
    "    \n",
    "    age_ranges = ab_group[0]\n",
    "    balance_ranges = ab_group[1]\n",
    "\n",
    "    fold_results = {}\n",
    "    fold_cs_data = {}\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    print(ab_group[2])\n",
    "\n",
    "    for fold_d in fold_data:\n",
    "\n",
    "        train_index = fold_d[0]\n",
    "        test_index = fold_d[1]\n",
    "\n",
    "        results_p1 = ([0], [0], 0.0)\n",
    "        results_p2 = ([0], [0], 0.0)\n",
    "        results_p3 = ([0], [0], 0.0)\n",
    "        results_p4 = ([0], [0], 0.0)\n",
    "        results_p5 = ([0], [0], 0.0)\n",
    "        results_p6 = ([0], [0], 0.0)\n",
    "        results_p7 = ([0], [0], 0.0)\n",
    "        results_p8 = ([0], [0], 0.0)\n",
    "        results_p9 = ([0], [0], 0.0)\n",
    "\n",
    "        i += 1\n",
    "        print(\"At fold number: \", i)\n",
    "\n",
    "        train_df = mkt_df_filtered.iloc[train_index]\n",
    "        test_df = mkt_df_filtered.iloc[test_index]\n",
    "\n",
    "        combs_to_consider, fs_pick = construct_feature_combs(train_df, test_df, 0, age_ranges, balance_ranges, max_calls)\n",
    "\n",
    "        # Testing Phase 1 -> Baseline test with shuffling of all customers and calling them ..\n",
    "        results_p1, num_calls, p1_s_c = call_everyone(test_df)\n",
    "\n",
    "        # Testing Phase 2 -> Order how we call customers - based on the overall s/c ratio ..\n",
    "        results_p2, p2_s_c, cs_data_2 = greedy_approach(combs_to_consider)\n",
    "\n",
    "        # Testing Phase 3 -> Convex Hull - Gradient Ascent Approach ..\n",
    "        results_p3, p3_s_c, cs_data_3 = convex_hull(fs_pick, num_calls)\n",
    "\n",
    "        # Testing Phase 4 -> If we were Godlike and knew all ..\n",
    "        results_p4, p4_s_c = upper_bound(test_df, max_calls)\n",
    "\n",
    "        # Clustering Approaches \n",
    "#         results_p5, groups1, p5_s_c = clustering_age_balance_grouped(mkt_df_filtered, train_index, test_index, 20, balance_ranges, age_ranges)\n",
    "\n",
    "#         results_p6, groups2 = clustering_age_balance_not_grouped(mkt_df_filtered, train_index, test_index, 20, balance_ranges, age_ranges)\n",
    "#         These methods do not take in to account the age and balance ranges.\n",
    "#         results_p7, groups3 = new_approach_ratio_grouping_percentage(mkt_df_filtered, train_index, test_index, 500, age_ranges_2, balance_ranges_2)\n",
    "#         results_p8 = xgboost_multiclass(mkt_df_filtered, train_index, test_index)\n",
    "#         results_p9, p9_s_c = decision_tree_multiclass(mkt_df_filtered, train_index, test_index)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Add all results together for this fold.\n",
    "        fold_results[i] = {'BL':results_p1, \n",
    "                          'GC':results_p2, \n",
    "                          'GA':results_p3, \n",
    "                          'UB':results_p4\n",
    "#                           'HDbScan':results_p5,\n",
    "#                           'XGB': results_p8,\n",
    "#                           'DT': results_p9\n",
    "                         }\n",
    "        \n",
    "        fold_cs_data[i] = {\n",
    "                          'GC':cs_data_2, \n",
    "                          'GA':cs_data_3,\n",
    "                         }\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    gen_save_graphs(fold_results, folder_name, ab_group[2])\n",
    "    save_customer_segmentation_data(fold_cs_data, folder_name, ab_group[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays raw results\n",
    "for fold_num in phase_batch.keys():\n",
    "    print(\"Fold Number: \", fold_num)\n",
    "    fold_data = phase_batch[fold_num]\n",
    "    for key in fold_data.keys():\n",
    "        if fold_data[key]!=None:\n",
    "            print(key, fold_data[key][2])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
