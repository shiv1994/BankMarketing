{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import operator as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math, itertools\n",
    "import statistics\n",
    "import json\n",
    "import hdbscan\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions Across All Methods\n",
    "\n",
    "def load_file(data_file_path):\n",
    "    data_df = pd.read_csv(data_file_path, delimiter=\";\")\n",
    "    return data_df\n",
    "  \n",
    "    \n",
    "def plot_graph_new(results, max_calls, list_passed, title, name = \"1\"):\n",
    "    x_pts = [i+1 for i in range(0, max_calls)]\n",
    "    if list_passed:\n",
    "        y_pts = results\n",
    "    else:    \n",
    "        y_pts = [results[i]['expected'] for i in range(0, max_calls)]\n",
    "    print(y_pts)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_pts, y_pts, linewidth=2)\n",
    "    plt.xlabel(\"Call Number\")\n",
    "    plt.ylabel(\"Success Per Call Rate\")\n",
    "    plt.ylim(0, 0.4)\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))\n",
    "#     plt.axvline(x=0, color =\"black\", linewidth=1)\n",
    "#     plt.axhline(y=0, color =\"black\", linewidth=1)\n",
    "    plt.xticks(np.arange(1, max_calls+1, 1))\n",
    "#     plt.show()\n",
    "    plt.savefig(str(name) + \".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def div(a,b):\n",
    "    if int(b) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return a/b\n",
    "    \n",
    "\n",
    "# Used for creating all possible combinations of the features.\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = iterable\n",
    "    return itertools.chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "\n",
    "\n",
    "def construct_dict(feature_comb):\n",
    "    new_dict = {}\n",
    "    new_dict['education'] = convert(feature_comb[0])\n",
    "    new_dict['job'] = convert(feature_comb[2])\n",
    "    new_dict['marital'] = convert(feature_comb[1])\n",
    "    new_dict['default'] = convert(feature_comb[3])\n",
    "    new_dict['loan'] = convert(feature_comb[4])\n",
    "    new_dict['housing'] = convert(feature_comb[5])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# This is the new metric (success per call rate).\n",
    "def compute_expected_succ_per_call_rate_feature_set(fs_df, no_calls_considered):\n",
    "    expected_values_call_nums = []\n",
    "    for i in range(1, no_calls_considered + 1):\n",
    "        expected_values_call_nums.append({'succ':0, 'total_calls':0, 'expected':0.0})\n",
    "        for index, row in fs_df.iterrows():\n",
    "            no_calls = row['campaign']\n",
    "            if no_calls <= i:\n",
    "                if row['y'] == \"yes\":\n",
    "                    expected_values_call_nums[i-1]['succ'] += 1\n",
    "                expected_values_call_nums[i-1]['total_calls'] += no_calls\n",
    "            else:\n",
    "                expected_values_call_nums[i-1]['total_calls'] += i\n",
    "    for loc, item in enumerate(expected_values_call_nums):\n",
    "        expected_values_call_nums[loc]['expected'] = div(item['succ'], item['total_calls'])\n",
    "    return expected_values_call_nums\n",
    "\n",
    "\n",
    "def compute_optimal_call_no(results):\n",
    "    max_loc = max(range(len(results)), key=lambda index: results[index]['expected'])\n",
    "    if max_loc == 0 and results[max_loc]['expected'] == 0.0:\n",
    "        return -1\n",
    "    return max_loc\n",
    "\n",
    "\n",
    "# Given a dictionary of what attributes comprise a feature set, we can get all rows corresponding to this feature set.\n",
    "def extract_rows_feature_set(fs_df, feature_labels = {'education':['tertiary', 'unknown'], \n",
    "                                                      'job':['management', 'technician', 'blue-collar'], \n",
    "                                                      'marital':['single'], 'default':['no'], \n",
    "                                                      'housing':['no'], 'loan':['no']}):\n",
    "    for key in feature_labels:\n",
    "        feature_labels_query_str = ''\n",
    "        arr = feature_labels[key]\n",
    "        for label in arr:\n",
    "            feature_labels_query_str += (key + ' == \"'+ label + '\" | ')\n",
    "        feature_labels_query_str = feature_labels_query_str[:-3]\n",
    "        fs_df = fs_df.query(feature_labels_query_str)\n",
    "    return fs_df\n",
    "\n",
    "\n",
    "def find_matching_attribute_comb(row_value, all_combs):\n",
    "    query = None\n",
    "    for comb in all_combs:\n",
    "        for item in comb:\n",
    "            if item == row_value:\n",
    "                query = comb\n",
    "    return query\n",
    "\n",
    "\n",
    "def compute_metric(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += row['campaign']\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "def compute_metric_2(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += min(row['campaign'], )\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} == '{1}'\".format(attrib, value)\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute_range(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    query_strings = []\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} >= {1} & {2} < {3}\".format(attrib, value[0], attrib, value[1])\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "        query_strings.append(v_query)\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals, query_strings\n",
    "\n",
    "\n",
    "def find_combinations(sub_attributes, ratios):\n",
    "    num_iter = len(ratios)\n",
    "    sil_scores = []\n",
    "    # Making use of the K-Means algorithm ... number of centroids are from 2 to n-1.\n",
    "    for clust_num in range(2, num_iter):\n",
    "        kmeans = KMeans(n_clusters = clust_num)\n",
    "        kmeans.fit(ratios.reshape(-1,1))\n",
    "        results = kmeans.labels_\n",
    "        sil_scores.append((silhouette_score(ratios.reshape(-1,1), results, metric='euclidean'), results, clust_num))\n",
    "#     print(sil_scores)\n",
    "    # We make use of the silhouette score to determine the ideal number of centroids.\n",
    "    sorted_sil_scores = sorted(sil_scores, key=lambda x: x[0], reverse = True)\n",
    "    # We then use this ideal number of centroids to determine which sub attributes should be aggregated.\n",
    "    joined_sub_attributes = []\n",
    "    for i in range(0, sorted_sil_scores[0][2]):\n",
    "        joined_sub_attributes.append([])\n",
    "    join_list = sorted_sil_scores[0][1]\n",
    "    for index, value in enumerate(join_list):\n",
    "        pos = join_list[index]\n",
    "        joined_sub_attributes[pos].append(sub_attributes[index])\n",
    "    return_joined_sub_attributes = []\n",
    "    for arr in joined_sub_attributes:\n",
    "        similar_els_gp = []\n",
    "        for item in arr:\n",
    "            similar_els_gp.append(str(item))\n",
    "        return_joined_sub_attributes.append(similar_els_gp)\n",
    "#     print(return_joined_sub_attributes)\n",
    "    return return_joined_sub_attributes\n",
    "\n",
    "# The following is the format of the way in which this method should be called.\n",
    "# find_combinations(['a', 'b', 'c', 'd'], np.array([1, 4, 7, 90]), \"job\").\n",
    "\n",
    "def find_all_cust_feature_set(fs, df):\n",
    "    comb = {\n",
    "        'education':fs[0], \n",
    "         'job':fs[1], \n",
    "         'marital':fs[2], \n",
    "         'default':fs[3], \n",
    "         'loan':fs[4], \n",
    "         'housing':fs[5]\n",
    "    }\n",
    "    res_1 = df.query(fs[6])\n",
    "    res_2 = res_1.query(fs[7])\n",
    "    res_final = extract_rows_feature_set(res_2, comb)\n",
    "    return res_final\n",
    "\n",
    "\n",
    "def construct_hull_points(results, max_calls):\n",
    "    pts = []\n",
    "    for x in range(0, max_calls):\n",
    "        s = results[x]['succ']\n",
    "        c = results[x]['total_calls']\n",
    "        pts.append([c,s])\n",
    "#     print(\"Num points is \", len(pts))\n",
    "    pts = np.array(pts)\n",
    "    try:\n",
    "        hull = ConvexHull(pts)\n",
    "        verts = hull.vertices\n",
    "#         print(pts)\n",
    "#         plt.plot(pts[:,0], pts[:,1], 'o')\n",
    "#         for simplex in hull.simplices:\n",
    "#             plt.plot(pts[simplex, 0], pts[simplex, 1], 'k-')\n",
    "        if not np.isin(max_calls - 1, verts):\n",
    "            verts = np.append(max_calls - 1, verts)\n",
    "        verts = np.sort(verts)\n",
    "        return verts.tolist()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def gradient_update(key, fs_pick):\n",
    "    fs = fs_pick[key]\n",
    "    fs_results = fs['results']\n",
    "    hull_pts = fs['hull_points']\n",
    "    loc = fs['loc']\n",
    "    max_loc = fs['max_num_pts']\n",
    "    grad = 0.0\n",
    "    if loc <= max_loc:\n",
    "        if loc == 0:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'], fs_results[hull_pts[loc]]['total_calls'])\n",
    "        else:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'] - fs_results[hull_pts[loc-1]]['succ'] , fs_results[hull_pts[loc]]['total_calls'] - fs_results[hull_pts[loc-1]]['total_calls'])\n",
    "        fs_pick[key]['grad'] = grad\n",
    "    else:\n",
    "        fs_pick[key]['finished'] = True\n",
    "\n",
    "        \n",
    "def get_features(row, feature_names):\n",
    "    fs = []\n",
    "    for index, val in enumerate(feature_names):\n",
    "        if int(row[index]) == 1:\n",
    "            fs.append(val)\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell holds functions that are utilized by each of the methods defined.\n",
    "\n",
    "def group_age(row, age_ranges):\n",
    "#     print(age_ranges)\n",
    "    age = int(row['age'])\n",
    "    age_val = None\n",
    "    for index, age_range in enumerate(age_ranges):\n",
    "        if op.ge(age, age_range[0]) and op.le(age, age_range[1]):\n",
    "            age_val = index + 1\n",
    "    if age_val == None:\n",
    "        print(\"Failed Assignment for age: \", age)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'age'] = age_val\n",
    "    return age_val\n",
    "        \n",
    "\n",
    "def group_balance(row, balance_ranges):\n",
    "#     print(balance_ranges)\n",
    "    bal = int(row['balance'])\n",
    "    bal_val = None\n",
    "    for index, balance_range in enumerate(balance_ranges):\n",
    "        if op.ge(bal, balance_range[0]) and op.le(bal, balance_range[1]):\n",
    "            bal_val = index + 1\n",
    "    if bal_val == None:\n",
    "        print(\"Failed Assignment for balance: \", bal)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'balance'] = bal_val\n",
    "    return bal_val\n",
    "\n",
    "\n",
    "def group_feature(df, col_name, func, ranges):\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, col_name] = func(row, ranges)\n",
    "\n",
    "def compute_ratio_all_users(df, train_indicies):\n",
    "    ratio_values = []\n",
    "    for val in train_indicies:\n",
    "        row = df.iloc[val]\n",
    "        if row['y'] == \"yes\":\n",
    "            ratio_values.append((val, div(1, row['campaign'])))\n",
    "        else:\n",
    "            ratio_values.append((val, 0.0))\n",
    "    return ratio_values\n",
    "\n",
    "\n",
    "def compute_freq_percentage(mappings):\n",
    "    total = 0\n",
    "    for user_mapping in mappings.keys():\n",
    "        total += mappings[user_mapping]['freq']\n",
    "    for user_mapping in mappings.keys():\n",
    "        mappings[user_mapping]['percentage'] = div(mappings[user_mapping]['freq'], total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each function represents each method attempted.\n",
    "\n",
    "def call_everyone(test_df):\n",
    "    print(\"Call all Customers Approach\")\n",
    "    call_check_points = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, 96500, 97000, 97500, 98000, 98500, 99000, 99500, 100000, 100500, 101000, 101500, 102000, 102500, 103000, 103500, 104000, 104500, 105000, 105500, 106000, 106500, 107000, 107500, 108000, 108500, 109000, 109500, 110000, 110500, 111000, 111500, 112000, 112500, 113000, 113500, 114000, 114500, 115000, 115500, 116000, 116500, 117000, 117500, 118000, 118500, 119000, 119500, 120000, 120500, 121000, 121500, 122000, 122500, 123000, 123500, 124000, 124500, 125000, 125500, 126000, 126500, 127000, 127500, 128000, 128500, 129000, 129500, 130000, 130500, 131000, 131500, 132000, 132500, 133000, 133500, 134000, 134500, 135000, 135500, 136000, 136500, 137000, 137500, 138000, 138500, 139000, 139500]\n",
    "    result_ratios_p1 = []\n",
    "    cp_loc = 0\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    res = test_df.reindex(np.random.permutation(test_df.index))\n",
    "    for loc, row in res.iterrows():\n",
    "        if num_calls >= call_check_points[cp_loc]:\n",
    "            cp_loc += 1\n",
    "            result_ratios_p1.append((num_succ, num_calls))\n",
    "        num_calls += row['campaign']\n",
    "        if row['y'] == \"yes\":\n",
    "            num_succ += 1\n",
    "    result_ratios_p1.append((num_succ, num_calls))\n",
    "    return result_ratios_p1, num_calls\n",
    "\n",
    "\n",
    "def greedy_approach(combs_to_consider):\n",
    "    print(\"Greedy Approach\")\n",
    "    persons_to_call_overall = {k: v for k, v in sorted(combs_to_consider.items(), key=lambda fs: fs[1]['overall_rate'], reverse = True)}\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    result_ratios_p2 = []\n",
    "    # print(type(persons_to_call_overall))\n",
    "    result_ratios = []\n",
    "    for key in persons_to_call_overall.keys():\n",
    "        for loc, cust in persons_to_call_overall[key]['fs_customers'].iterrows():\n",
    "            num_calls += cust['campaign']\n",
    "            if cust['y'] == \"yes\":\n",
    "                num_succ +=1\n",
    "        result_ratios_p2.append((num_succ, num_calls))\n",
    "    return result_ratios_p2\n",
    "\n",
    "\n",
    "def convex_hull(fs_pick, num_calls):\n",
    "    print(\"Gradient Ascent Approach\")\n",
    "    result_ratios_p4 = []\n",
    "    total_s = 0\n",
    "    total_c = 0\n",
    "    print(\"Performing initial update .. \")\n",
    "    for key in fs_pick.keys():\n",
    "        gradient_update(key, fs_pick)\n",
    "    print(\"Perforiming sort .. \")\n",
    "    # Sort based on gradient.\n",
    "    optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    # Call best feature set, update gradient for this feature set and re-sort all feature sets.\n",
    "    # Rinse and repeat!\n",
    "    print(\"Finished sort .. in while loop \")\n",
    "    while(total_c <= num_calls):\n",
    "        best_loc = 0\n",
    "        while(best_loc < len(optimal_choices) and optimal_choices[best_loc][1]['finished'] == True):\n",
    "            best_loc += 1\n",
    "        if best_loc == len(optimal_choices):\n",
    "            break\n",
    "        fs_key = optimal_choices[best_loc][0]\n",
    "        fs_data = optimal_choices[best_loc][1]\n",
    "        if fs_data['finished'] == False:\n",
    "            loc = fs_data['loc']\n",
    "            if loc == 0:\n",
    "                call_start = 1\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            else:\n",
    "                call_start = fs_data['hull_points'][loc-1] + 2\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            for call in range(call_start, call_end + 1, 1):\n",
    "                for loc, row in fs_pick[fs_key]['fs_customers'].iterrows():\n",
    "                    if row['campaign'] == call:\n",
    "                        total_c += 1\n",
    "                        if row['y'] == \"yes\":\n",
    "                            total_s += 1\n",
    "                    elif row['campaign'] > call:\n",
    "                        total_c += 1\n",
    "            result_ratios_p4.append((total_s, total_c))\n",
    "            fs_pick[fs_key]['loc'] += 1\n",
    "            gradient_update(fs_key, fs_pick)\n",
    "            optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    return result_ratios_p4\n",
    "\n",
    "\n",
    "def upper_bound(test_df):\n",
    "    print(\"Upper Bound Approach\")\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    result_ratios_p5 = []\n",
    "    res_df = test_df.query(\"y == 'yes'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        num_succ += num_cust\n",
    "        num_calls = num_calls + (num_cust * x)\n",
    "        result_ratios_p5.append((num_succ, num_calls))\n",
    "    res_df = test_df.query(\"y == 'no'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        num_calls = num_calls + (num_cust * x)\n",
    "        result_ratios_p5.append((num_succ, num_calls))\n",
    "    return result_ratios_p5\n",
    "    \n",
    "    \n",
    "def new_approach_ratio_grouping_percentage(df, train_indicies, test_indicies, group_size, age_groupings, balance_groupings):\n",
    "    # Ensuring that we can binary encode any row in our dataset. We also group age and balance values \n",
    "    # from each row into ranges.\n",
    "    print(\"Step 1\")\n",
    "    df_copy = df.copy(deep=True)\n",
    "    group_feature(df_copy, \"age\", group_age, age_groupings)\n",
    "    group_feature(df_copy, \"balance\", group_balance, balance_groupings)\n",
    "    # Build the customers for each group.\n",
    "    print(\"Step 2\")\n",
    "    ratio_arr = compute_ratio_all_users(df_copy, train_indicies)\n",
    "    ratio_arr_sorted = sorted(ratio_arr, key=lambda tup: tup[1], reverse = True)\n",
    "    train_size = len(ratio_arr_sorted)\n",
    "    groupings = {}\n",
    "    for loc in range(0, train_size):\n",
    "        group_key = str(int(loc/group_size))\n",
    "        if group_key not in groupings.keys():\n",
    "            groupings[group_key] = {'indicies':[], 'mappings':{}, 'results':None} \n",
    "        groupings[group_key]['indicies'].append(ratio_arr_sorted[loc][0])\n",
    "    print(len(groupings.keys()))\n",
    "    # For each group, we find the unique feature combinations and store them in a list. \n",
    "    # We also store the results - s/c ratio for call numbers from 1-20.\n",
    "    test_calls = {}\n",
    "    print(\"Step 3\")\n",
    "    for group_key in groupings.keys():\n",
    "        users_df = df_copy.iloc[groupings[group_key]['indicies']]\n",
    "        mappings = groupings[group_key]['mappings']\n",
    "        for row in users_df.itertuples():\n",
    "            user_mapping = str((row.job, row.marital, row.education, row.default,\n",
    "                               row.housing, row.loan, row.age, row.balance))\n",
    "            if user_mapping not in mappings.keys():\n",
    "                mappings[user_mapping] = {'freq':0, 'percentage':0.0}\n",
    "            else:\n",
    "                mappings[user_mapping]['freq'] += 1\n",
    "        groupings[group_key]['results'] = compute_expected_succ_per_call_rate_feature_set(users_df, 20)\n",
    "        compute_freq_percentage(mappings)\n",
    "        test_calls[group_key] = {'locs_to_call':[], 'overall_rate':groupings[group_key]['results'][19]}\n",
    "    # print(test_calls)\n",
    "    # For the test set, we need to map each user to the most appropriate cluster.\n",
    "    print(\"Step 4\")\n",
    "    missed = 0\n",
    "    for loc in test_indicies:\n",
    "        row = df_copy.iloc[loc]\n",
    "        user_mapping = str((row['job'], row['marital'], row['education'], row['default'],\n",
    "                            row['housing'], row['loan'], row['age'], row['balance']))\n",
    "        all_groupings_keys = list(groupings.keys())\n",
    "        best_group_key = None\n",
    "        best_ratio = -1.0\n",
    "        for group_key in all_groupings_keys:\n",
    "            if user_mapping in groupings[group_key]['mappings']:\n",
    "                if best_group_key is None:\n",
    "                    best_group_key = group_key\n",
    "                    best_ratio = groupings[group_key]['mappings'][user_mapping]['percentage']\n",
    "                else:\n",
    "                    if groupings[best_group_key]['mappings'][user_mapping]['percentage'] > best_ratio:\n",
    "                        best_group_key = group_key\n",
    "                        best_ratio = groupings[group_key]['mappings'][user_mapping]['percentage']\n",
    "        if best_group_key is not None:\n",
    "            test_calls[best_group_key]['locs_to_call'].append(loc)\n",
    "        else:\n",
    "            missed += 1\n",
    "    print(\"We missed:\", missed)\n",
    "    # Call users ... those with the highest ratios are called first.\n",
    "    test_calls_sorted = sorted(test_calls.items(), key=lambda fs: fs[1]['overall_rate']['expected'], reverse = True)\n",
    "    print(\"Step 5\")\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    result_ratios = []\n",
    "    for test_call in test_calls_sorted:\n",
    "        for cust_loc in test_call[1]['locs_to_call']:\n",
    "            row = df_copy.iloc[cust_loc]\n",
    "            if row['y'] == \"yes\":\n",
    "                num_succ += 1\n",
    "            num_calls += int(row['campaign'])\n",
    "        result_ratios.append((num_succ, num_calls))\n",
    "    return result_ratios, groupings\n",
    "\n",
    "\n",
    "def clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded):\n",
    "    groupings = {}\n",
    "    predictions = clusterer.labels_\n",
    "    # We assign to each group, the similar indicies. This was based on the clustering approach.\n",
    "    for index, group in enumerate(predictions):\n",
    "        if str(group) not in groupings.keys():\n",
    "            groupings[str(group)] = {'train_indicies':[], 'unique_keys':{}, 'results':None, 'test_indicies':[]}\n",
    "        groupings[str(group)]['train_indicies'].append(index)\n",
    "    print(\"Check 1\")\n",
    "    # For all customers belonging to each grouping, we find the unique keys and compute the success per call\n",
    "    # ratio for call numbers 1-20.\n",
    "    for group in groupings.keys():\n",
    "        for index in groupings[group]['train_indicies']:\n",
    "            cust_info = train_df.iloc[index]\n",
    "            cust_features = get_features(train_df_encoded[index], feature_names)\n",
    "            # cust_features = cust_features[0:8]\n",
    "            if str(cust_features) not in groupings[group]['unique_keys'].keys():\n",
    "                groupings[group]['unique_keys'][str(cust_features)] = {'#_ocurr': 1}\n",
    "            else:\n",
    "                groupings[group]['unique_keys'][str(cust_features)]['#_ocurr'] += 1\n",
    "        results = compute_expected_succ_per_call_rate_feature_set(train_df.iloc[groupings[group]['train_indicies']], 20)\n",
    "        groupings[group]['results'] = results\n",
    "    print(\"Check 2\")\n",
    "    # This process makes use of the test set and determines the ideal cluster for a customer.\n",
    "    for index in range(0, len(test_df_encoded), 1):\n",
    "        encoded_customer_data = test_df_encoded[index]\n",
    "        test_labels, strengths = hdbscan.approximate_predict(clusterer, [encoded_customer_data])\n",
    "        groupings[str(test_labels[0])]['test_indicies'].append(index)\n",
    "    print(\"Check 3\")\n",
    "    # Perform sorting of groups based on success per call rate.\n",
    "    sorted_final_call = {k: v for k, v in sorted(groupings.items(), key=lambda item: item[1]['results'][19]['expected'], reverse = True)}\n",
    "    print(\"Check 4\")\n",
    "    # Go about calling customers, keep track of the success per call rate as we switch from group to group.\n",
    "    total_s = 0\n",
    "    total_c = 0\n",
    "    result_ratios = []\n",
    "    for group in sorted_final_call:\n",
    "        for cust_index in sorted_final_call[group]['test_indicies']:\n",
    "            row = test_df.iloc[cust_index]\n",
    "            if row['y'] == \"yes\":\n",
    "                total_s += 1\n",
    "            total_c += int(row['campaign'])\n",
    "        result_ratios.append((total_s, total_c))\n",
    "    return result_ratios, groupings\n",
    "        \n",
    "        \n",
    "# The encoding process can also be varied to not include age and balance.\n",
    "def clustering_age_balance_grouped(train_df, test_df, min_cluster_size, balance_groupings, age_groupings):\n",
    "    print(\"HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\")\n",
    "    train_df_copy = train_df.copy(deep = True)\n",
    "    test_df_copy = test_df.copy(deep = True)\n",
    "    group_feature(train_df_copy, \"age\", group_age, age_groupings)\n",
    "    group_feature(train_df_copy, \"balance\", group_balance, balance_groupings)\n",
    "#     print(train_df_copy.head(10))\n",
    "    encoder = OneHotEncoder()\n",
    "    train_df_encoded = encoder.fit_transform(train_df_copy.drop(columns=['y', 'campaign'])).toarray()\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "#     print(len(train_df_encoded[0]))\n",
    "    clusterer.fit(train_df_encoded)\n",
    "    group_feature(test_df_copy, \"age\", group_age, age_groupings)\n",
    "    group_feature(test_df_copy, \"balance\", group_balance, balance_groupings)\n",
    "    test_df_encoded = encoder.fit_transform(test_df_copy.drop(columns=['y', 'campaign'])).toarray()\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "    return clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded)\n",
    "\n",
    "\n",
    "# The encoding process can also be varied to not include age and balance.\n",
    "def clustering_age_balance_not_grouped(train_df, test_df, min_cluster_size, balance_groupings, age_groupings):\n",
    "    print(\"HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\")\n",
    "    train_df_copy = train_df.copy(deep = True)\n",
    "    test_df_copy = test_df.copy(deep = True)\n",
    "    encoder = OneHotEncoder()\n",
    "    train_df_encoded = encoder.fit_transform(train_df_copy.drop(columns=['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "    train_df_encoded = np.column_stack((train_df_encoded, train_df_copy['age'].to_numpy()))\n",
    "    train_df_encoded = np.column_stack((train_df_encoded, train_df_copy['balance'].to_numpy()))\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "    clusterer.fit(train_df_encoded)\n",
    "    test_df_encoded = encoder.fit_transform(test_df_copy.drop(columns=['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "    test_df_encoded = np.column_stack((test_df_encoded, test_df_copy['age'].to_numpy()))\n",
    "    test_df_encoded = np.column_stack((test_df_encoded, test_df_copy['balance'].to_numpy()))\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan'])\n",
    "    return clustering_approach_abstracted(clusterer, feature_names, train_df, test_df, train_df_encoded, test_df_encoded)\n",
    "\n",
    "# def clustering_age_balance_grouped_ratio_gradient_ascent(train_df, test_df, min_cluster_size, balance_groupings, age_groupings, num_calls):\n",
    "#     print(\"In P8\")\n",
    "#     group_age(train_df, age_groupings)\n",
    "#     group_balance(train_df, balance_groupings)\n",
    "#     train_df, ratio_df = compute_ratio_all_users(train_df)\n",
    "#     encoder = OneHotEncoder()\n",
    "#     train_df_encoded = encoder.fit_transform(train_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "#     train_df_encoded = np.column_stack((train_df_encoded, ratio_df['ratio'].to_numpy()))\n",
    "#     feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "#     clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "#     clusterer.fit(train_df_encoded)\n",
    "#     predictions = clusterer.labels_\n",
    "#     test_df = mkt_df_filtered.iloc[test_index]\n",
    "#     group_age(test_df, age_groupings)\n",
    "#     group_balance(test_df, balance_groupings)\n",
    "#     test_df_encoded = encoder.fit_transform(test_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "# #     return None, None, (None, None)\n",
    "#     return abstraction_new_approach_gradient_ascent(predictions, feature_names, train_df, test_df, train_df_encoded, test_df_encoded, num_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell represents the logic required for the inital approach taken.\n",
    "# Yields really good results thus far.\n",
    "# This should remain untouched.\n",
    "\n",
    "def construct_feature_combs(train_df):\n",
    "    # At this point, we can run computations for the success rate of each sub attribute and join\n",
    "    # the sub-attributes based on the output of k-means.\n",
    "    poss = []\n",
    "\n",
    "    # Education.\n",
    "    all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "    education_cmbs = find_combinations(all_ed, metric_vals)\n",
    "\n",
    "    # Occupation.\n",
    "    all_jobs = ['student', 'retired', 'unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_jobs, train_df, 'job')\n",
    "    job_cmbs = find_combinations(all_jobs, metric_vals)\n",
    "\n",
    "    # Marital.\n",
    "    all_ms = ['married', 'single', 'divorced']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ms, train_df, 'marital')\n",
    "    marital_cmbs = find_combinations(all_ms, metric_vals)\n",
    "\n",
    "    # Default\n",
    "    all_def = ['no', 'yes']\n",
    "    default_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Loan\n",
    "    all_ln = ['no', 'yes']\n",
    "    loan_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Housing\n",
    "    all_hs = ['no', 'yes']\n",
    "    housing_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    poss.append(education_cmbs)\n",
    "    poss.append(marital_cmbs)\n",
    "    poss.append(job_cmbs)\n",
    "    poss.append(default_cmbs)\n",
    "    poss.append(loan_cmbs)\n",
    "    poss.append(housing_cmbs)\n",
    "    all_combs = list(itertools.product(*poss))\n",
    "\n",
    "    # print(\"Number of combinations: \", len(all_combs)* len(age_query_strings) * len(balance_query_strings))\n",
    "\n",
    "    # We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "    num_iter = 0\n",
    "    combs_to_consider = {}\n",
    "    fs_pick = {}\n",
    "\n",
    "    # Setting up looping structures to generate all possibilities.\n",
    "    for age_query in age_query_strings:\n",
    "        df_filtered_final = train_df.query(age_query)\n",
    "        for bal_query in balance_query_strings:\n",
    "            df_filtered_final_2 = df_filtered_final.query(bal_query)\n",
    "            for comb in all_combs:\n",
    "                dict_final_query = construct_dict(comb)\n",
    "                num_iter += 1\n",
    "                extracted_df = extract_rows_feature_set(df_filtered_final_2, dict_final_query)\n",
    "                key = (dict_final_query['education'], dict_final_query['job'], \n",
    "                       dict_final_query['marital'], dict_final_query['default'], \n",
    "                       dict_final_query['loan'], dict_final_query['housing'], \n",
    "                       bal_query, age_query)\n",
    "                n_rows = extracted_df.shape[0]\n",
    "                if n_rows >= cp:\n",
    "                    results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "#                             max_loc = compute_optimal_call_no(results)\n",
    "#                             if max_loc != -1:\n",
    "                    combs_to_consider[key] = {\n",
    "                                                'max_loc':0,\n",
    "                                                'best_rate':0, \n",
    "                                                'overall_rate':results[max_calls-1]['expected'], \n",
    "                                                'n_rows':n_rows, \n",
    "                                                'results':results,\n",
    "                                                'fs_customers':None\n",
    "                                             }\n",
    "                    fs_pick[key] = {'grad': 0.0, \n",
    "                                    'loc':0, \n",
    "                                    'finished':False, \n",
    "                                    'hull_points':None,\n",
    "                                    'max_num_pts': -1,\n",
    "                                    'results':None,\n",
    "                                    'fs_customers' :None\n",
    "                                   }\n",
    "#               else:\n",
    "#                   print(\"Invalid FS ! -> \", n_rows)\n",
    "    for fs_key in combs_to_consider.keys():\n",
    "        fs_customers = find_all_cust_feature_set(fs_key, test_df)\n",
    "        combs_to_consider[fs_key]['fs_customers'] = fs_customers\n",
    "        res = construct_hull_points(combs_to_consider[fs_key]['results'], max_calls)\n",
    "        fs_pick[fs_key]['results'] = combs_to_consider[fs_key]['results']\n",
    "        fs_pick[fs_key]['fs_customers'] = fs_customers\n",
    "        if res is False:\n",
    "            print(\"Invalid Convex Hull Assignment\")\n",
    "            fs_pick[fs_key]['finished'] = True\n",
    "        else:\n",
    "            fs_pick[fs_key]['hull_points'] = res\n",
    "            fs_pick[fs_key]['max_num_pts'] = len(res) - 1\n",
    "    return combs_to_consider, fs_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 17)\n",
      "CPU times: user 40.1 ms, sys: 8.02 ms, total: 48.1 ms\n",
      "Wall time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Code that sets up values to construct all possible feature combinations.\n",
    "\n",
    "# Query Strings with filename being 'mod_1' \n",
    "# age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "# age_ranges_2 = [(10, 34), (35, 45), (46, 100)]\n",
    "# balance_query_strings = ['balance <= 450',' balance > 450']\n",
    "# balance_ranges_2 = [(-10000, 450), (451, 105000)]\n",
    "\n",
    "# Query Strings with filename being 'mod_2' \n",
    "age_query_strings = ['age >= 18 & age <= 30', 'age >= 31 & age <= 47', 'age >= 48 & age <= 64', 'age >= 65']\n",
    "age_ranges_2 = [(18,30), (31, 47), (48,64), (65,100)]\n",
    "balance_query_strings = ['balance <= 450',' balance > 450']\n",
    "balance_ranges_2 = [(-10000, 450), (451, 105000)]\n",
    "\n",
    "# Max call number to consider.\n",
    "max_calls = 20\n",
    "\n",
    "# Pull and filter all calls <= 20.\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "mkt_df_filtered = mkt_df_filtered[['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance', 'campaign', 'y']]\n",
    "mkt_df_filtered_kmeans = mkt_df_filtered[['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance']]\n",
    "print(mkt_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At fold number:  1\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "Upper Bound Approach\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "Step 1\n",
      "Step 2\n",
      "72\n",
      "Step 3\n",
      "Step 4\n",
      "We missed: 163\n",
      "Step 5\n",
      "At fold number:  2\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "Upper Bound Approach\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "Step 1\n",
      "Step 2\n",
      "72\n",
      "Step 3\n",
      "Step 4\n",
      "We missed: 186\n",
      "Step 5\n",
      "At fold number:  3\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "Upper Bound Approach\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "Step 1\n",
      "Step 2\n",
      "72\n",
      "Step 3\n",
      "Step 4\n",
      "We missed: 159\n",
      "Step 5\n",
      "At fold number:  4\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "Upper Bound Approach\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "Step 1\n",
      "Step 2\n",
      "72\n",
      "Step 3\n",
      "Step 4\n",
      "We missed: 142\n",
      "Step 5\n",
      "At fold number:  5\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Invalid Convex Hull Assignment\n",
      "Call all Customers Approach\n",
      "Greedy Approach\n",
      "Gradient Ascent Approach\n",
      "Performing initial update .. \n",
      "Perforiming sort .. \n",
      "Finished sort .. in while loop \n",
      "Upper Bound Approach\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "HDBScan Clustering No Ratio - Using Approximate Predict Function from HDBScan Library\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "Step 1\n",
      "Step 2\n",
      "72\n",
      "Step 3\n",
      "Step 4\n",
      "We missed: 141\n",
      "Step 5\n",
      "CPU times: user 32min 4s, sys: 753 ms, total: 32min 5s\n",
      "Wall time: 32min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Main code ... orchestrates everything!\n",
    "\n",
    "# Splitting dataframe into data and result dataframes.\n",
    "X = mkt_df_filtered.iloc[:,0:len(mkt_df_filtered.columns)-1]\n",
    "y = mkt_df_filtered.iloc[:,-1]\n",
    "\n",
    "cut_points = [20]\n",
    "\n",
    "for cp in cut_points:\n",
    "\n",
    "    for j in range(1,2):\n",
    "        \n",
    "        phase_batch = {}\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        i = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            \n",
    "            result_ratios_p1 = None\n",
    "            result_ratios_p2 = None\n",
    "            result_ratios_p3 = None\n",
    "            result_ratios_p4 = None\n",
    "            result_ratios_p5 = None\n",
    "            result_ratios_p6 = None\n",
    "            result_ratios_p7 = None\n",
    "            result_ratios_p8 = None\n",
    "            result_ratios_p9 = None\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            print(\"At fold number: \", i)\n",
    "\n",
    "            train_df = mkt_df_filtered.iloc[train_index]\n",
    "            test_df = mkt_df_filtered.iloc[test_index]\n",
    "            \n",
    "            combs_to_consider, fs_pick = construct_feature_combs(train_df)\n",
    "            \n",
    "            \n",
    "            # Testing Phase 1 -> Baseline test with shuffling of all customers and calling them ..\n",
    "            result_ratios_p1, num_calls = call_everyone(test_df)\n",
    "\n",
    "\n",
    "            # Testing Phase 2 -> Order how we call customers - based on the overall s/c ratio ..\n",
    "            result_ratios_p2 = greedy_approach(combs_to_consider)\n",
    "\n",
    "\n",
    "            # Testing Phase 4 -> Convex Hull - Gradient Ascent Approach ..\n",
    "            result_ratios_p4 = convex_hull(fs_pick, num_calls)\n",
    "            \n",
    "            \n",
    "            # Testing Phase 5 -> If we were godlike and knew all ..\n",
    "            result_ratios_p5 = upper_bound(test_df)\n",
    "            \n",
    "            result_ratios_p6, groups1 = clustering_age_balance_grouped(train_df, test_df, 20, balance_ranges_2, age_ranges_2)\n",
    "            \n",
    "            result_ratios_p7, groups2 = clustering_age_balance_not_grouped(train_df, test_df, 20, balance_ranges_2, age_ranges_2)\n",
    "\n",
    "            result_ratios_p8, groups3 = new_approach_ratio_grouping_percentage(mkt_df_filtered, train_index, test_index, 500, age_ranges_2, balance_ranges_2)\n",
    "            \n",
    "            # Add all results together for this fold.\n",
    "            \n",
    "            phase_batch_key = str(j) + \"_\" + str(i)\n",
    "            phase_batch[phase_batch_key] = {'p1':result_ratios_p1, 'p2':result_ratios_p2, \n",
    "                                            'p4':result_ratios_p4, 'p5':result_ratios_p5,\n",
    "                                            'p6':result_ratios_p6, 'p7':result_ratios_p7, \n",
    "                                            'p8':result_ratios_p8, 'p9':result_ratios_p9}\n",
    "\n",
    "        with open('mod_2_' + str(cp) +'.json', 'w') as fp:\n",
    "                json.dump(phase_batch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = {'1':{'ratio':0.25}, '2':{'ratio':0.59}}\n",
    "sorted(me.items(), key=lambda fs: fs[1]['ratio'], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('13082020' + str(cp) +'.json', 'w') as fp:\n",
    "    json.dump(phase_batch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1\n",
      "1_2\n",
      "1_3\n",
      "1_4\n",
      "1_5\n"
     ]
    }
   ],
   "source": [
    "for key in phase_batch.keys():\n",
    "    print(key)\n",
    "    new_ratios = []\n",
    "    for el in phase_batch[key]['p8']:\n",
    "        new_ratios.append((int(el[0]), int(el[1])))\n",
    "    phase_batch[key]['p8'] = new_ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1\n",
      "1_2\n",
      "1_3\n",
      "1_4\n",
      "1_5\n"
     ]
    }
   ],
   "source": [
    "for key in phase_batch.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
