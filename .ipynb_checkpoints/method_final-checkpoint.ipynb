{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math, itertools\n",
    "import statistics\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "# from ipynb.fs.full.helper_fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def load_file(data_file_path):\n",
    "    data_df = pd.read_csv(data_file_path, delimiter=\";\")\n",
    "    return data_df\n",
    "  \n",
    "    \n",
    "def plot_graph_new(results, max_calls, list_passed, title):\n",
    "    x_pts = [i+1 for i in range(0, max_calls)]\n",
    "    if list_passed:\n",
    "        y_pts = results\n",
    "    else:    \n",
    "        y_pts = [results[i]['expected'] for i in range(0, max_calls)]\n",
    "    plt.title(title)\n",
    "    plt.plot(x_pts, y_pts)\n",
    "    plt.axvline(x=0, color =\"black\")\n",
    "    plt.axhline(y=0, color =\"black\")\n",
    "    plt.xticks(np.arange(1, max_calls+1, 1))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# def plot_graph_both_axes(ratios, no_calls):\n",
    "    \n",
    "\n",
    "def div(a,b):\n",
    "    if int(b) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return a/b\n",
    "    \n",
    "\n",
    "# Used for creating all possible combinations of the features.\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = iterable\n",
    "    return itertools.chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "\n",
    "\n",
    "def construct_dict(feature_comb):\n",
    "    new_dict = {}\n",
    "    new_dict['education'] = convert(feature_comb[0])\n",
    "    new_dict['job'] = convert(feature_comb[2])\n",
    "    new_dict['marital'] = convert(feature_comb[1])\n",
    "    new_dict['default'] = convert(feature_comb[3])\n",
    "    new_dict['loan'] = convert(feature_comb[4])\n",
    "    new_dict['housing'] = convert(feature_comb[5])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# This was the old metric (reward per call rate).\n",
    "# def compute_expected_reward_feature_set_new(fs_df, no_calls_considered):\n",
    "#     expected_values_calls = []\n",
    "#     len_df = len(fs_df)\n",
    "#     for i in range(1, no_calls_considered + 1):\n",
    "#         expected_values_calls.append({'neg_value':0.0, 'pos_value':0.0, 'count':0, 'expected':0.0})\n",
    "#         for index, row in fs_df.iterrows():\n",
    "#             no_calls = row['campaign']\n",
    "#             if no_calls <= i:\n",
    "#                 if row['y'] == \"yes\":\n",
    "#                     expected_values_calls[i-1]['pos_value'] += ((no_calls_considered+1) - no_calls)\n",
    "#                 else:\n",
    "#                     expected_values_calls[i-1]['neg_value'] += (-no_calls)\n",
    "#             else:\n",
    "#                 expected_values_calls[i-1]['neg_value'] += (-i)\n",
    "#             expected_values_calls[i-1]['count'] += 1\n",
    "#     for loc, item in enumerate(expected_values_calls):\n",
    "#         expected_values_calls[loc]['expected'] = (expected_values_calls[loc]['pos_value'] + expected_values_calls[loc]['neg_value'])/len_df\n",
    "#     return expected_values_calls\n",
    "\n",
    "\n",
    "# This is the new metric (success per call rate).\n",
    "def compute_expected_succ_per_call_rate_feature_set(fs_df, no_calls_considered):\n",
    "    expected_values_call_nums = []\n",
    "    for i in range(1, no_calls_considered + 1):\n",
    "        expected_values_call_nums.append({'succ':0, 'total_calls':0, 'expected':0.0})\n",
    "        for index, row in fs_df.iterrows():\n",
    "            no_calls = row['campaign']\n",
    "            if no_calls <= i:\n",
    "                if row['y'] == \"yes\":\n",
    "                    expected_values_call_nums[i-1]['succ'] += 1\n",
    "                expected_values_call_nums[i-1]['total_calls'] += no_calls\n",
    "            else:\n",
    "                expected_values_call_nums[i-1]['total_calls'] += i\n",
    "    for loc, item in enumerate(expected_values_call_nums):\n",
    "        expected_values_call_nums[loc]['expected'] = div(item['succ'], item['total_calls'])\n",
    "    return expected_values_call_nums\n",
    "\n",
    "\n",
    "def compute_optimal_call_no(results):\n",
    "    max_loc = max(range(len(results)), key=lambda index: results[index]['expected'])\n",
    "    if max_loc == 0 and results[max_loc]['expected'] == 0.0:\n",
    "        return -1\n",
    "    return max_loc\n",
    "\n",
    "\n",
    "# Given a dictionary of what attributes comprise a feature set, we can get all rows corresponding to this feature set.\n",
    "def extract_rows_feature_set(fs_df, feature_labels = {'education':['tertiary', 'unknown'], \n",
    "                                                      'job':['management', 'technician', 'blue-collar'], \n",
    "                                                      'marital':['single'], 'default':['no'], \n",
    "                                                      'housing':['no'], 'loan':['no']}):\n",
    "    for key in feature_labels:\n",
    "        feature_labels_query_str = ''\n",
    "        arr = feature_labels[key]\n",
    "        for label in arr:\n",
    "            feature_labels_query_str += (key + ' == \"'+ label + '\" | ')\n",
    "        feature_labels_query_str = feature_labels_query_str[:-3]\n",
    "        fs_df = fs_df.query(feature_labels_query_str)\n",
    "    return fs_df\n",
    "\n",
    "\n",
    "def find_matching_attribute_comb(row_value, all_combs):\n",
    "    query = None\n",
    "    for comb in all_combs:\n",
    "        for item in comb:\n",
    "            if item == row_value:\n",
    "                query = comb\n",
    "    return query\n",
    "\n",
    "\n",
    "def compute_metric(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += row['campaign']\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "def compute_metric_2(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += min(row['campaign'], )\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} == '{1}'\".format(attrib, value)\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute_range(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    query_strings = []\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} >= {1} & {2} < {3}\".format(attrib, value[0], attrib, value[1])\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "        query_strings.append(v_query)\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals, query_strings\n",
    "\n",
    "\n",
    "def find_combinations(sub_attributes, ratios):\n",
    "    num_iter = len(ratios)\n",
    "    sil_scores = []\n",
    "    # Making use of the K-Means algorithm ... number of centroids are from 2 to n-1.\n",
    "    for clust_num in range(2, num_iter):\n",
    "        kmeans = KMeans(n_clusters = clust_num)\n",
    "        kmeans.fit(ratios.reshape(-1,1))\n",
    "        results = kmeans.labels_\n",
    "        sil_scores.append((silhouette_score(ratios.reshape(-1,1), results, metric='euclidean'), results, clust_num))\n",
    "    # We make use of the silhouette score to determine the ideal number of centroids.\n",
    "    sorted_sil_scores = sorted(sil_scores, key=lambda x: x[0], reverse = True)\n",
    "    # We then use this ideal number of centroids to determine which sub attributes should be aggregated.\n",
    "    joined_sub_attributes = []\n",
    "    for i in range(0, sorted_sil_scores[0][2]):\n",
    "        joined_sub_attributes.append([])\n",
    "    join_list = sorted_sil_scores[0][1]\n",
    "    for index, value in enumerate(join_list):\n",
    "        pos = join_list[index]\n",
    "        joined_sub_attributes[pos].append(sub_attributes[index])\n",
    "    return_joined_sub_attributes = []\n",
    "    for arr in joined_sub_attributes:\n",
    "        similar_els_gp = []\n",
    "        for item in arr:\n",
    "            similar_els_gp.append(str(item))\n",
    "        return_joined_sub_attributes.append(similar_els_gp)\n",
    "#     print(return_joined_sub_attributes)\n",
    "    return return_joined_sub_attributes\n",
    "\n",
    "# The following is the format of the way in which this method should be called.\n",
    "# find_combinations(['a', 'b', 'c', 'd'], np.array([1, 4, 7, 90]), \"job\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44967, 17)\n",
      "CPU times: user 63.3 ms, sys: 19.2 ms, total: 82.6 ms\n",
      "Wall time: 82.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Code that sets up values to construct all possible feature combinations.\n",
    "\n",
    "# Age query strings.\n",
    "# age_query_strings = ['age < 26','age >= 26 & age <=60','age >60']\n",
    "# age_query_strings = ['age >= 10 & age <= 32', 'age >= 33 & age <= 40', 'age >= 50 & age <= 59', 'age >= 60']\n",
    "age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "\n",
    "# Balance query strings.\n",
    "balance_query_strings = ['balance <= 450',' balance > 450']\n",
    "\n",
    "# Max call number to consider.\n",
    "max_calls = 20\n",
    "\n",
    "# Pull and filter all calls <= 20.\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "\n",
    "print(mkt_df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a5989f5c635e>\u001b[0m in \u001b[0;36mcompute_metric_for_each_attribute\u001b[0;34m(all_values, df, attrib)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mv_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{0} == '{1}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mdataset_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mmetric_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mmetric_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m#         print(v_query, metric_val, dataset_query.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a5989f5c635e>\u001b[0m in \u001b[0;36mcompute_metric\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mtotal_calls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mtotal_successes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mtotal_successes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3277\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_block_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDatetimeTZBlock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mget_block_type\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m   3231\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3232\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3233\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3234\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Main code ... orchestrates everything!\n",
    "\n",
    "# Splitting dataframe into data and result dataframes.\n",
    "X = mkt_df_filtered.iloc[:,0:len(mkt_df_filtered.columns)-1]\n",
    "y = mkt_df_filtered.iloc[:,-1]   \n",
    "\n",
    "all_call_limits = [500, 1000, 1500]\n",
    "\n",
    "for call_limit in all_call_limits:\n",
    "    all_non_optimal_ratios = []\n",
    "    all_optimal_ratios = []\n",
    "    for j in range(1,11):\n",
    "        i = 0\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            i += 1\n",
    "\n",
    "            train_df = mkt_df_filtered.iloc[train_index]\n",
    "            test_df = mkt_df_filtered.iloc[test_index]\n",
    "\n",
    "            # At this point, we can run computations for the success rate of each sub attribute and join\n",
    "            # the sub-attributes based on the output of k-means.\n",
    "            poss = []\n",
    "\n",
    "            # Education.\n",
    "            all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "            education_cmbs = find_combinations(all_ed, metric_vals)\n",
    "\n",
    "            # Occupation.\n",
    "            all_jobs = ['student', 'retired', 'unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_jobs, train_df, 'job')\n",
    "            job_cmbs = find_combinations(all_jobs, metric_vals)\n",
    "\n",
    "            # Marital.\n",
    "            all_ms = ['married', 'single', 'divorced', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_ms, train_df, 'marital')\n",
    "            marital_cmbs = find_combinations(all_ms, metric_vals)\n",
    "\n",
    "            # Default\n",
    "            all_def = ['no', 'yes', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_def, train_df, 'default')\n",
    "            default_cmbs = find_combinations(all_def, metric_vals)\n",
    "\n",
    "            # Loan\n",
    "            all_ln = ['no', 'yes', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_ln, train_df, 'loan')\n",
    "            loan_cmbs = find_combinations(all_ln, metric_vals)\n",
    "\n",
    "            # Housing\n",
    "            all_hs = ['no', 'yes', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_hs, train_df, 'housing')\n",
    "            housing_cmbs = find_combinations(all_hs, metric_vals)\n",
    "\n",
    "            poss.append(education_cmbs)\n",
    "            poss.append(marital_cmbs)\n",
    "            poss.append(job_cmbs)\n",
    "            poss.append(default_cmbs)\n",
    "            poss.append(loan_cmbs)\n",
    "            poss.append(housing_cmbs)\n",
    "            all_combs = list(itertools.product(*poss))\n",
    "\n",
    "            # print(\"Number of combinations: \", len(all_combs)* len(age_query_strings) * len(balance_query_strings))\n",
    "\n",
    "            # We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "\n",
    "            num_iter = 0\n",
    "            combs_to_consider = {}\n",
    "            # Setting up looping structures to generate all possibilities.\n",
    "            # All that has to be done now is to change 'df_train' to 'X_train'.\n",
    "            for age_query in age_query_strings:\n",
    "                df_filtered_final = train_df.query(age_query)\n",
    "                for bal_query in balance_query_strings:\n",
    "                    df_filtered_final = df_filtered_final.query(bal_query)\n",
    "                    for comb in all_combs:\n",
    "                        dict_final_query = construct_dict(comb)\n",
    "                        num_iter += 1\n",
    "                        extracted_df = extract_rows_feature_set(df_filtered_final, dict_final_query)\n",
    "                        key = (dict_final_query['education'], dict_final_query['job'], dict_final_query['marital'], dict_final_query['default'], dict_final_query['loan'], dict_final_query['housing'], bal_query, age_query)\n",
    "                        n_rows = extracted_df.shape[0]\n",
    "                        if n_rows >0:\n",
    "                            results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "                            max_loc = compute_optimal_call_no(results)\n",
    "                            rate = results[max_loc]['expected']\n",
    "                            # In this new case max_loc never goes below zero!\n",
    "                            if max_loc != -1:\n",
    "                                combs_to_consider[key] = {'age':age_query, 'bal':bal_query, 'comb':comb, 'consider':True, 'max_loc':max_loc, 'rate':results[max_loc]['expected'], 'n_rows':n_rows}\n",
    "#                             print(comb)\n",
    "#                             print(\"Max Loc is: \", max_loc+1)\n",
    "#                             plot_graph_new(results, max_calls, False, \"Expected Ratio per Call\")\n",
    "        #  When we are finished creating the feature combinations .... we can now use the hold out set for validation of the model!\n",
    "            # Baseline Metrics\n",
    "            num_succ = 0\n",
    "            num_calls = 0\n",
    "            # Optimal Method Metrics\n",
    "            num_succ_optimal = 0\n",
    "            num_calls_optimal = 0\n",
    "            num_bad_cons = 0\n",
    "            num_good_cons = 0\n",
    "\n",
    "            num_fc_small_sample_size = 0\n",
    "\n",
    "            all_possible_calls = []\n",
    "\n",
    "            for loc, row in test_df.iterrows():\n",
    "                # For optimal method.\n",
    "                # We have the exact values for each of the following:\n",
    "                jb_query = convert(find_matching_attribute_comb(str(row['job']), job_cmbs))\n",
    "                mt_query = convert(find_matching_attribute_comb(str(row['marital']), marital_cmbs))\n",
    "                ec_query = convert(find_matching_attribute_comb(str(row['education']), education_cmbs))\n",
    "                house_query = convert(find_matching_attribute_comb(str(row['housing']), housing_cmbs))\n",
    "                loan_query = convert(find_matching_attribute_comb(str(row['loan']), loan_cmbs))\n",
    "                def_query = convert(find_matching_attribute_comb(str(row['default']), default_cmbs))\n",
    "                ##########################\n",
    "                no_calls = row['campaign']\n",
    "                # The balance and age are within ranges so we need to find the matching query.\n",
    "                ##########################\n",
    "                balance = row['balance']\n",
    "                bal_query = None\n",
    "                age = row['age']\n",
    "                age_query = None\n",
    "                for age_q in age_query_strings:\n",
    "                    if eval(age_q):\n",
    "                        age_query = age_q\n",
    "                for bal_q in balance_query_strings:\n",
    "                    if eval(bal_q):\n",
    "                        bal_query = bal_q\n",
    "                key_to_find = (ec_query, jb_query, mt_query, def_query, loan_query, house_query, bal_query, age_query)\n",
    "                if key_to_find in combs_to_consider.keys():\n",
    "                    fs = combs_to_consider[key_to_find]\n",
    "                    # Adding the rate, feature set, the outcome and the number of calls made to a new list .. which will be sorted afterwards.\n",
    "                    if fs['n_rows'] >= 20:\n",
    "                        all_possible_calls.append((fs['rate'], fs, row['y'], row['campaign']))\n",
    "                    else:\n",
    "                        num_fc_small_sample_size += 1\n",
    "\n",
    "            # Actual Testing ..\n",
    "\n",
    "            total_possible_calls = call_limit\n",
    "            all_possible_calls_sorted = sorted(all_possible_calls, key = lambda tup: tup[0], reverse = True)\n",
    "            subset_df = test_df.sample(n = total_possible_calls)\n",
    "\n",
    "            # Baseline \n",
    "            for loc, row in subset_df.iterrows():\n",
    "                if num_calls + row['campaign'] <= total_possible_calls:\n",
    "                    num_calls += row['campaign']\n",
    "                    if row['y'] == \"yes\":\n",
    "                        num_succ += 1\n",
    "            ratio_baseline = div(num_succ, num_calls)\n",
    "            all_non_optimal_ratios.append(ratio_baseline)\n",
    "#             print(\"Ratio for baseline: \", ratio_baseline)\n",
    "\n",
    "            # Optimal\n",
    "            for item in all_possible_calls_sorted:\n",
    "                if item[3] + num_calls_optimal <= total_possible_calls:\n",
    "                    num_calls_optimal += item[3]\n",
    "                    if item[2] == \"yes\":\n",
    "                        num_succ_optimal += 1\n",
    "            ratio_optimal = div(num_succ_optimal, num_calls_optimal)\n",
    "            all_optimal_ratios.append(ratio_optimal)\n",
    "#             print(\"Ratio for optimal: \", ratio_optimal)  \n",
    "#             print(\"Number of FS with small sample sizes: \", num_fc_small_sample_size)\n",
    "#             print(\"\\n\")\n",
    "    mean_non_optimal = statistics.mean(all_non_optimal_ratios)\n",
    "    mean_optimal = statistics.mean(all_optimal_ratios)\n",
    "    std_dev_non_optimal = statistics.stdev(all_non_optimal_ratios)\n",
    "    std_dev_optimal = statistics.stdev(all_optimal_ratios)\n",
    "    \n",
    "#     print(\"Mean Non-Optimal: \", mean_non_optimal)\n",
    "#     print(\"Mean Optimal: \", mean_optimal)\n",
    "#     print(\"Std Dev Non-Optimal: \", std_dev_non_optimal)\n",
    "#     print(\"Std Dev Optimal: \", std_dev_optimal)\n",
    "#     print(all_non_optimal_ratios)\n",
    "#     print(all_optimal_ratios)\n",
    "    \n",
    "    f = open(str(call_limit)+\".txt\",\"w+\")\n",
    "    f.write(\"%s\\n\" % str(mean_non_optimal))\n",
    "    f.write(\"%s\\n\" % str(mean_optimal))\n",
    "    f.write(\"%s\\n\" % str(std_dev_non_optimal))\n",
    "    f.write(\"%s\\n\" % str(std_dev_optimal))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = 45\n",
    "for age_q in age_query_strings:\n",
    "    if eval(age_q):\n",
    "        print(\"Yes!\")\n",
    "        print(age_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age and balance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1326, 17)\n",
      "age < 26 0.11134307585247043\n",
      "(42453, 17)\n",
      "age >= 26 & age <=60 0.03950606355669647\n",
      "(1188, 17)\n",
      "age >60 0.2084717607973422\n"
     ]
    }
   ],
   "source": [
    "# Age.\n",
    "# all_age_query_strings = ['age >= 10 & age <= 32', 'age >= 33 & age <= 40', 'age >= 50 & age <= 59', 'age >= 60']\n",
    "# all_age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "all_age_query_strings = ['age < 26','age >= 26 & age <=60','age >60']\n",
    "for age_query in all_age_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(age_query)\n",
    "    print(df_filtered_final.shape)\n",
    "    print(age_query, compute_metric(df_filtered_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 17)\n",
      "age >= 10 & age <= 19 47 0.14634146341463414\n",
      "(5189, 17)\n",
      "age >= 20 & age <= 29 5189 0.07631601041054488\n",
      "(17989, 17)\n",
      "age >= 30 & age <= 39 17989 0.040097307272879794\n",
      "(11584, 17)\n",
      "age >= 40 & age <= 49 11584 0.03357253501090633\n",
      "(8375, 17)\n",
      "age >= 50 & age <= 59 8375 0.03437390389337075\n",
      "(1229, 17)\n",
      "age >= 60 & age <= 69 1229 0.12427647259107934\n",
      "(424, 17)\n",
      "age >= 70 & age <= 79 424 0.20594965675057209\n",
      "(130, 17)\n",
      "age >= 80 & age <= 100 130 0.1950354609929078\n"
     ]
    }
   ],
   "source": [
    "# Age.\n",
    "all_age_query_strings = ['age >= 10 & age <= 19', 'age >= 20 & age <= 29', 'age >= 30 & age <= 39', 'age >= 40 & age <= 49', 'age >= 50 & age <= 59','age >= 60 & age <= 69', 'age >= 70 & age <= 79', 'age >= 80 & age <= 100']\n",
    "# all_age_query_strings = ['age >= 10 & age <= 100']\n",
    "for age_query in all_age_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(age_query)\n",
    "    print(df_filtered_final.shape)\n",
    "    print(age_query, len(df_filtered_final), compute_metric(df_filtered_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "all_age_query_tuples = [(10, 20), (20, 30), (30, 40), (50, 60), (60, 70), (70, 80), (80, 90), (90, 100)]\n",
    "ratios, all_age_query_strings = compute_metric_for_each_attribute_range(all_age_query_tuples, train_df, 'age')\n",
    "find_combinations(all_age_query_strings, ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Balance\n",
    "all_bal_query_strings = ['balance >= -100000 & balance <= -1', 'balance >= 0 & balance < 250', 'balance >= 250 & balance < 500','balance >= 500 & balance < 750', 'balance >= 750 & balance < 1000', 'balance >= 1000 & balance < 2000', 'balance >= 2000 & balance < 3000', 'balance >= 3000 & balance < 4000','balance >= 4000 & balance < 5000', 'balance >= 5000 & balance < 6000', 'balance >= 6000 & balance < 7000', 'balance >= 7000 & balance < 8000', 'balance >= 8000 & balance < 9000', 'balance >= 9000 & balance < 10000','balance >= 10000 & balance < 11000', 'balance >= 11000 & balance < 12000', 'balance >= 12000 & balance < 13000', 'balance >= 13000 & balance < 14000', 'balance >= 14000 & balance < 15000', 'balance >= 15000 & balance < 16000', 'balance >= 16000 & balance < 17000','balance >= 17000 & balance < 18000', 'balance >= 18000 & balance < 19000', 'balance >= 19000 & balance < 19000', 'balance >= 20000']\n",
    "for bal_query in all_bal_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(bal_query)\n",
    "    print(bal_query, len(df_filtered_final), compute_metric(df_filtered_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance\n",
    "all_bal_query_tuples = [(-10000, 0), (0, 250), (250, 500), (500, 750), (750,1000), (1000, 2000), (2000, 3000), (3000, 4000), (4000, 5000), (5000, 6000), (6000, 7000), (8000, 100000)]\n",
    "ratios, all_bal_query_strings = compute_metric_for_each_attribute_range(all_bal_query_tuples, train_df, 'balance')\n",
    "find_combinations(all_bal_query_strings, ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining where to stop regarding the number of calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to determine the maximum number of calls we should stop at!\n",
    "all_ratios_calls = []\n",
    "for i in range(1,57):\n",
    "    query_str = 'campaign == ' + str(i)\n",
    "    call_query_data = mkt_df_filtered.query(query_str)\n",
    "    succ = 0\n",
    "    calls = 0\n",
    "    for lc, rw in call_query_data.iterrows():\n",
    "        if rw['y'] == \"yes\":\n",
    "            succ += 1\n",
    "        calls += rw['campaign']\n",
    "    all_ratios_calls.append(div(succ, calls))\n",
    "for index, value in enumerate(all_ratios_calls):\n",
    "    print(index+1, value)\n",
    "plot_graph_new(all_ratios_calls, 56, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the success rate by optimizing the maximum calls made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_calls_considered = 20\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls_considered)]\n",
    "\n",
    "result_ratios = [0.0 for i in range (1,max_calls_considered+1)]\n",
    "\n",
    "for i in range(1, max_calls_considered+1):\n",
    "    total_calls = 0\n",
    "    total_succ = 0\n",
    "    #query_str = \"campaign <= {0}\".format(i)\n",
    "    #print(query_str)\n",
    "    #df_filtered_campaign = mkt_df_filtered.query(query_str)\n",
    "    for loc, row in mkt_df_filtered.iterrows():\n",
    "        if row['y']  == \"yes\" and row['campaign'] <= i:\n",
    "            total_succ += 1\n",
    "        total_calls += min(i, row['campaign'])\n",
    "    result_ratios[i-1] = div(total_succ , total_calls)\n",
    "    print(i, result_ratios[i-1], total_succ, total_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_new(result_ratios, 20, True, \"Ratio Per Call #\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "mkt_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered['poutcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered_successes = mkt_df_filtered.query(\"poutcome == 'success'\")\n",
    "print(mkt_df_filtered_successes.shape)\n",
    "mkt_df_filtered_successes['previous'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered_successes['poutcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mkt_df_filtered_successes['campaign'].value_counts(normalize = False)\n",
    "print(res)\n",
    "print(res.values)\n",
    "num_succ = [2561, 1401 , 618, 317, 139, 92, 47, 32, 21, 14, 16, 4, 6, 4, 4, 2, 6, 0, 0 ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_new(num_succ, 20, True, \"Frequency of Contacts Made per Call #\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To conduct further analysis we need to determine:\n",
    "    - The probability that someone was successful for this campaign given that \n",
    "    they were either successful or not for the last campaign.\n",
    "    - Correlate the time spent on the last call and the success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mkt_df_filtered.query(\"y == 'yes'\").shape[0]\n",
    "b = mkt_df_filtered.query(\"poutcome == 'success'\").shape[0]\n",
    "anb = mkt_df_filtered.query(\"y == 'yes' and poutcome == 'success'\").shape[0]\n",
    "print(anb/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4920, 60):\n",
    "    res = mkt_df_filtered.query(\"duration >= {0} and duration <= {1}\".format(str(i-60), str(i)))\n",
    "    print(i, res.shape[0], res.query(\"y == 'yes'\").shape[0])\n",
    "# a = mkt_df_filtered.query(\"duration >= 0 and duration <= 180\").shape[0]\n",
    "# b = mkt_df_filtered.query(\"y == 'yes'\").shape[0]\n",
    "# anb = mkt_df_filtered.query(\"y == 'yes' and duration >= 0 and duration <= 1000\").shape[0]\n",
    "# print(anb/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(mkt_df_filtered['duration'].max()/60)+1\n",
    "int(mkt_df_filtered['duration'].max()/60) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mkt_df_filtered.query(\"y == 'yes' and contact == 'cellular'\").shape[0]\n",
    "b = mkt_df_filtered.query(\"y == 'yes' and contact == 'telephone'\").shape[0]\n",
    "c = mkt_df_filtered.query('y == \"yes\"').shape[0]\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
