{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import operator as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math, itertools\n",
    "import statistics\n",
    "import json\n",
    "import hdbscan\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def load_file(data_file_path):\n",
    "    data_df = pd.read_csv(data_file_path, delimiter=\";\")\n",
    "    return data_df\n",
    "  \n",
    "    \n",
    "def plot_graph_new(results, max_calls, list_passed, title, name = \"1\"):\n",
    "    x_pts = [i+1 for i in range(0, max_calls)]\n",
    "    if list_passed:\n",
    "        y_pts = results\n",
    "    else:    \n",
    "        y_pts = [results[i]['expected'] for i in range(0, max_calls)]\n",
    "    print(y_pts)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_pts, y_pts, linewidth=2)\n",
    "    plt.xlabel(\"Call Number\")\n",
    "    plt.ylabel(\"Success Per Call Rate\")\n",
    "    plt.ylim(0, 0.4)\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))\n",
    "#     plt.axvline(x=0, color =\"black\", linewidth=1)\n",
    "#     plt.axhline(y=0, color =\"black\", linewidth=1)\n",
    "    plt.xticks(np.arange(1, max_calls+1, 1))\n",
    "#     plt.show()\n",
    "    plt.savefig(str(name) + \".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def div(a,b):\n",
    "    if int(b) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return a/b\n",
    "    \n",
    "\n",
    "# Used for creating all possible combinations of the features.\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = iterable\n",
    "    return itertools.chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "\n",
    "\n",
    "def construct_dict(feature_comb):\n",
    "    new_dict = {}\n",
    "    new_dict['education'] = convert(feature_comb[0])\n",
    "    new_dict['job'] = convert(feature_comb[2])\n",
    "    new_dict['marital'] = convert(feature_comb[1])\n",
    "    new_dict['default'] = convert(feature_comb[3])\n",
    "    new_dict['loan'] = convert(feature_comb[4])\n",
    "    new_dict['housing'] = convert(feature_comb[5])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# This is the new metric (success per call rate).\n",
    "def compute_expected_succ_per_call_rate_feature_set(fs_df, no_calls_considered):\n",
    "    expected_values_call_nums = []\n",
    "    for i in range(1, no_calls_considered + 1):\n",
    "        expected_values_call_nums.append({'succ':0, 'total_calls':0, 'expected':0.0})\n",
    "        for index, row in fs_df.iterrows():\n",
    "            no_calls = row['campaign']\n",
    "            if no_calls <= i:\n",
    "                if row['y'] == \"yes\":\n",
    "                    expected_values_call_nums[i-1]['succ'] += 1\n",
    "                expected_values_call_nums[i-1]['total_calls'] += no_calls\n",
    "            else:\n",
    "                expected_values_call_nums[i-1]['total_calls'] += i\n",
    "    for loc, item in enumerate(expected_values_call_nums):\n",
    "        expected_values_call_nums[loc]['expected'] = div(item['succ'], item['total_calls'])\n",
    "    return expected_values_call_nums\n",
    "\n",
    "\n",
    "def compute_optimal_call_no(results):\n",
    "    max_loc = max(range(len(results)), key=lambda index: results[index]['expected'])\n",
    "    if max_loc == 0 and results[max_loc]['expected'] == 0.0:\n",
    "        return -1\n",
    "    return max_loc\n",
    "\n",
    "\n",
    "# Given a dictionary of what attributes comprise a feature set, we can get all rows corresponding to this feature set.\n",
    "def extract_rows_feature_set(fs_df, feature_labels = {'education':['tertiary', 'unknown'], \n",
    "                                                      'job':['management', 'technician', 'blue-collar'], \n",
    "                                                      'marital':['single'], 'default':['no'], \n",
    "                                                      'housing':['no'], 'loan':['no']}):\n",
    "    for key in feature_labels:\n",
    "        feature_labels_query_str = ''\n",
    "        arr = feature_labels[key]\n",
    "        for label in arr:\n",
    "            feature_labels_query_str += (key + ' == \"'+ label + '\" | ')\n",
    "        feature_labels_query_str = feature_labels_query_str[:-3]\n",
    "        fs_df = fs_df.query(feature_labels_query_str)\n",
    "    return fs_df\n",
    "\n",
    "\n",
    "def find_matching_attribute_comb(row_value, all_combs):\n",
    "    query = None\n",
    "    for comb in all_combs:\n",
    "        for item in comb:\n",
    "            if item == row_value:\n",
    "                query = comb\n",
    "    return query\n",
    "\n",
    "\n",
    "def compute_metric(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += row['campaign']\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "def compute_metric_2(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += min(row['campaign'], )\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} == '{1}'\".format(attrib, value)\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute_range(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    query_strings = []\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} >= {1} & {2} < {3}\".format(attrib, value[0], attrib, value[1])\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "        query_strings.append(v_query)\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals, query_strings\n",
    "\n",
    "\n",
    "def find_combinations(sub_attributes, ratios):\n",
    "    num_iter = len(ratios)\n",
    "    sil_scores = []\n",
    "    # Making use of the K-Means algorithm ... number of centroids are from 2 to n-1.\n",
    "    for clust_num in range(2, num_iter):\n",
    "        kmeans = KMeans(n_clusters = clust_num)\n",
    "        kmeans.fit(ratios.reshape(-1,1))\n",
    "        results = kmeans.labels_\n",
    "        sil_scores.append((silhouette_score(ratios.reshape(-1,1), results, metric='euclidean'), results, clust_num))\n",
    "#     print(sil_scores)\n",
    "    # We make use of the silhouette score to determine the ideal number of centroids.\n",
    "    sorted_sil_scores = sorted(sil_scores, key=lambda x: x[0], reverse = True)\n",
    "    # We then use this ideal number of centroids to determine which sub attributes should be aggregated.\n",
    "    joined_sub_attributes = []\n",
    "    for i in range(0, sorted_sil_scores[0][2]):\n",
    "        joined_sub_attributes.append([])\n",
    "    join_list = sorted_sil_scores[0][1]\n",
    "    for index, value in enumerate(join_list):\n",
    "        pos = join_list[index]\n",
    "        joined_sub_attributes[pos].append(sub_attributes[index])\n",
    "    return_joined_sub_attributes = []\n",
    "    for arr in joined_sub_attributes:\n",
    "        similar_els_gp = []\n",
    "        for item in arr:\n",
    "            similar_els_gp.append(str(item))\n",
    "        return_joined_sub_attributes.append(similar_els_gp)\n",
    "#     print(return_joined_sub_attributes)\n",
    "    return return_joined_sub_attributes\n",
    "\n",
    "# The following is the format of the way in which this method should be called.\n",
    "# find_combinations(['a', 'b', 'c', 'd'], np.array([1, 4, 7, 90]), \"job\").\n",
    "\n",
    "def find_all_cust_feature_set(fs, df):\n",
    "    comb = {\n",
    "        'education':fs[0], \n",
    "         'job':fs[1], \n",
    "         'marital':fs[2], \n",
    "         'default':fs[3], \n",
    "         'loan':fs[4], \n",
    "         'housing':fs[5]\n",
    "    }\n",
    "    res_1 = df.query(fs[6])\n",
    "    res_2 = res_1.query(fs[7])\n",
    "    res_final = extract_rows_feature_set(res_2, comb)\n",
    "    return res_final\n",
    "\n",
    "\n",
    "def construct_hull_points(results, max_calls):\n",
    "    pts = []\n",
    "    for x in range(0, max_calls):\n",
    "        s = results[x]['succ']\n",
    "        c = results[x]['total_calls']\n",
    "        pts.append([c,s])\n",
    "#     print(\"Num points is \", len(pts))\n",
    "    pts = np.array(pts)\n",
    "    try:\n",
    "        hull = ConvexHull(pts)\n",
    "        verts = hull.vertices\n",
    "#         print(pts)\n",
    "#         plt.plot(pts[:,0], pts[:,1], 'o')\n",
    "#         for simplex in hull.simplices:\n",
    "#             plt.plot(pts[simplex, 0], pts[simplex, 1], 'k-')\n",
    "        if not np.isin(max_calls - 1, verts):\n",
    "            verts = np.append(max_calls - 1, verts)\n",
    "        verts = np.sort(verts)\n",
    "        return verts.tolist()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def gradient_update(key, fs_pick):\n",
    "    fs = fs_pick[key]\n",
    "    fs_results = fs['results']\n",
    "    hull_pts = fs['hull_points']\n",
    "    loc = fs['loc']\n",
    "    max_loc = fs['max_num_pts']\n",
    "    grad = 0.0\n",
    "    if loc <= max_loc:\n",
    "        if loc == 0:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'], fs_results[hull_pts[loc]]['total_calls'])\n",
    "        else:\n",
    "            grad = div(fs_results[hull_pts[loc]]['succ'] - fs_results[hull_pts[loc-1]]['succ'] , fs_results[hull_pts[loc]]['total_calls'] - fs_results[hull_pts[loc-1]]['total_calls'])\n",
    "        fs_pick[key]['grad'] = grad\n",
    "    else:\n",
    "        fs_pick[key]['finished'] = True\n",
    "\n",
    "        \n",
    "def get_features(row, feature_names):\n",
    "    fs = []\n",
    "    for index, val in enumerate(feature_names):\n",
    "        if int(row[index]) == 1:\n",
    "            fs.append(val)\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for constructing the data required for each of the approaches (Baseline and Gradient Ascent) \n",
    "# to be executed successfully.\n",
    "\n",
    "def construct_feature_combs(train_df):\n",
    "    # At this point, we can run computations for the success rate of each sub attribute and join\n",
    "    # the sub-attributes based on the output of k-means.\n",
    "    poss = []\n",
    "\n",
    "    # Education.\n",
    "    all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "    education_cmbs = find_combinations(all_ed, metric_vals)\n",
    "\n",
    "    # Occupation.\n",
    "    all_jobs = ['student', 'retired', 'unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_jobs, train_df, 'job')\n",
    "    job_cmbs = find_combinations(all_jobs, metric_vals)\n",
    "\n",
    "    # Marital.\n",
    "    all_ms = ['married', 'single', 'divorced']\n",
    "    metric_vals = compute_metric_for_each_attribute(all_ms, train_df, 'marital')\n",
    "    marital_cmbs = find_combinations(all_ms, metric_vals)\n",
    "\n",
    "    # Default\n",
    "    all_def = ['no', 'yes']\n",
    "    default_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Loan\n",
    "    all_ln = ['no', 'yes']\n",
    "    loan_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    # Housing\n",
    "    all_hs = ['no', 'yes']\n",
    "    housing_cmbs = [['no'], ['yes']]\n",
    "\n",
    "    poss.append(education_cmbs)\n",
    "    poss.append(marital_cmbs)\n",
    "    poss.append(job_cmbs)\n",
    "    poss.append(default_cmbs)\n",
    "    poss.append(loan_cmbs)\n",
    "    poss.append(housing_cmbs)\n",
    "    all_combs = list(itertools.product(*poss))\n",
    "\n",
    "    # print(\"Number of combinations: \", len(all_combs)* len(age_query_strings) * len(balance_query_strings))\n",
    "\n",
    "    # We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "    num_iter = 0\n",
    "    combs_to_consider = {}\n",
    "    fs_pick = {}\n",
    "\n",
    "    # Setting up looping structures to generate all possibilities.\n",
    "    for age_query in age_query_strings:\n",
    "        df_filtered_final = train_df.query(age_query)\n",
    "        for bal_query in balance_query_strings:\n",
    "            df_filtered_final_2 = df_filtered_final.query(bal_query)\n",
    "            for comb in all_combs:\n",
    "                dict_final_query = construct_dict(comb)\n",
    "                num_iter += 1\n",
    "                extracted_df = extract_rows_feature_set(df_filtered_final_2, dict_final_query)\n",
    "                key = (dict_final_query['education'], dict_final_query['job'], \n",
    "                       dict_final_query['marital'], dict_final_query['default'], \n",
    "                       dict_final_query['loan'], dict_final_query['housing'], \n",
    "                       bal_query, age_query)\n",
    "                n_rows = extracted_df.shape[0]\n",
    "                if n_rows >= cp:\n",
    "                    results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "#                             max_loc = compute_optimal_call_no(results)\n",
    "#                             if max_loc != -1:\n",
    "                    combs_to_consider[key] = {\n",
    "                                                'max_loc':0,\n",
    "                                                'best_rate':0, \n",
    "                                                'overall_rate':results[max_calls-1]['expected'], \n",
    "                                                'n_rows':n_rows, \n",
    "                                                'results':results,\n",
    "                                                'fs_customers':None\n",
    "                                             }\n",
    "                    fs_pick[key] = {'grad': 0.0, \n",
    "                                    'loc':0, \n",
    "                                    'finished':False, \n",
    "                                    'hull_points':None,\n",
    "                                    'max_num_pts': -1,\n",
    "                                    'results':None,\n",
    "                                    'fs_customers' :None\n",
    "                                   }\n",
    "#               else:\n",
    "#                   print(\"Invalid FS ! -> \", n_rows)\n",
    "    for fs_key in combs_to_consider.keys():\n",
    "        fs_customers = find_all_cust_feature_set(fs_key, test_df)\n",
    "        combs_to_consider[fs_key]['fs_customers'] = fs_customers\n",
    "        res = construct_hull_points(combs_to_consider[fs_key]['results'], max_calls)\n",
    "        fs_pick[fs_key]['results'] = combs_to_consider[fs_key]['results']\n",
    "        fs_pick[fs_key]['fs_customers'] = fs_customers\n",
    "        if res is False:\n",
    "            print(\"Invalid Convex Hull Assignment\")\n",
    "            fs_pick[fs_key]['finished'] = True\n",
    "        else:\n",
    "            fs_pick[fs_key]['hull_points'] = res\n",
    "            fs_pick[fs_key]['max_num_pts'] = len(res) - 1\n",
    "    return combs_to_consider, fs_pick\n",
    "\n",
    "\n",
    "def group_age(row, age_ranges):\n",
    "#     print(age_ranges)\n",
    "    age = int(row['age'])\n",
    "    age_val = None\n",
    "    for index, age_range in enumerate(age_ranges):\n",
    "        if op.ge(age, age_range[0]) and op.le(age, age_range[1]):\n",
    "            age_val = index + 1\n",
    "    if age_val == None:\n",
    "        print(\"Failed Assignment for age: \", age)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'age'] = age_val\n",
    "    return age_val\n",
    "        \n",
    "\n",
    "def group_balance(row, balance_ranges):\n",
    "#     print(balance_ranges)\n",
    "    bal = int(row['balance'])\n",
    "    bal_val = None\n",
    "    for index, balance_range in enumerate(balance_ranges):\n",
    "        if op.ge(bal, balance_range[0]) and op.le(bal, balance_range[1]):\n",
    "            bal_val = index + 1\n",
    "    if bal_val == None:\n",
    "        print(\"Failed Assignment for balance: \", bal)\n",
    "#         mkt_df_filtered_kmeans.loc[loc, 'balance'] = bal_val\n",
    "    return bal_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions representing each approach taken.\n",
    "\n",
    "def call_everyone(test_df):\n",
    "    print(\"Call all Customers Approach\")\n",
    "    call_check_points = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, 96500, 97000, 97500, 98000, 98500, 99000, 99500, 100000, 100500, 101000, 101500, 102000, 102500, 103000, 103500, 104000, 104500, 105000, 105500, 106000, 106500, 107000, 107500, 108000, 108500, 109000, 109500, 110000, 110500, 111000, 111500, 112000, 112500, 113000, 113500, 114000, 114500, 115000, 115500, 116000, 116500, 117000, 117500, 118000, 118500, 119000, 119500, 120000, 120500, 121000, 121500, 122000, 122500, 123000, 123500, 124000, 124500, 125000, 125500, 126000, 126500, 127000, 127500, 128000, 128500, 129000, 129500, 130000, 130500, 131000, 131500, 132000, 132500, 133000, 133500, 134000, 134500, 135000, 135500, 136000, 136500, 137000, 137500, 138000, 138500, 139000, 139500]\n",
    "    result_ratios_p1 = []\n",
    "    cp_loc = 0\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    res = test_df.reindex(np.random.permutation(test_df.index))\n",
    "    for loc, row in res.iterrows():\n",
    "        if num_calls >= call_check_points[cp_loc]:\n",
    "            cp_loc += 1\n",
    "            result_ratios_p1.append((num_succ, num_calls))\n",
    "        num_calls += row['campaign']\n",
    "        if row['y'] == \"yes\":\n",
    "            num_succ += 1\n",
    "    result_ratios_p1.append((num_succ, num_calls))\n",
    "    return result_ratios_p1, num_calls\n",
    "\n",
    "\n",
    "def greedy_approach(combs_to_consider):\n",
    "    print(\"Greedy Approach\")\n",
    "    persons_to_call_overall = {k: v for k, v in sorted(combs_to_consider.items(), key=lambda fs: fs[1]['overall_rate'], reverse = True)}\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    result_ratios_p2 = []\n",
    "    # print(type(persons_to_call_overall))\n",
    "    result_ratios = []\n",
    "    for key in persons_to_call_overall.keys():\n",
    "        for loc, cust in persons_to_call_overall[key]['fs_customers'].iterrows():\n",
    "            num_calls += cust['campaign']\n",
    "            if cust['y'] == \"yes\":\n",
    "                num_succ +=1\n",
    "        result_ratios_p2.append((num_succ, num_calls))\n",
    "    return result_ratios_p2\n",
    "\n",
    "\n",
    "def convex_hull(fs_pick, num_calls):\n",
    "    print(\"Gradient Ascent Approach\")\n",
    "    result_ratios_p4 = []\n",
    "    total_s = 0\n",
    "    total_c = 0\n",
    "    print(\"Performing initial update .. \")\n",
    "    for key in fs_pick.keys():\n",
    "        gradient_update(key, fs_pick)\n",
    "    print(\"Perforiming sort .. \")\n",
    "    # Sort based on gradient.\n",
    "    optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    # Call best feature set, update gradient for this feature set and re-sort all feature sets.\n",
    "    # Rinse and repeat!\n",
    "    print(\"Finished sort .. in while loop \")\n",
    "    while(total_c <= num_calls):\n",
    "        best_loc = 0\n",
    "        while(best_loc < len(optimal_choices) and optimal_choices[best_loc][1]['finished'] == True):\n",
    "            best_loc += 1\n",
    "        if best_loc == len(optimal_choices):\n",
    "            break\n",
    "        fs_key = optimal_choices[best_loc][0]\n",
    "        fs_data = optimal_choices[best_loc][1]\n",
    "        if fs_data['finished'] == False:\n",
    "            loc = fs_data['loc']\n",
    "            if loc == 0:\n",
    "                call_start = 1\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            else:\n",
    "                call_start = fs_data['hull_points'][loc-1] + 2\n",
    "                call_end = fs_data['hull_points'][loc] + 1\n",
    "            for call in range(call_start, call_end + 1, 1):\n",
    "                for loc, row in fs_pick[fs_key]['fs_customers'].iterrows():\n",
    "                    if row['campaign'] == call:\n",
    "                        total_c += 1\n",
    "                        if row['y'] == \"yes\":\n",
    "                            total_s += 1\n",
    "                    elif row['campaign'] > call:\n",
    "                        total_c += 1\n",
    "            result_ratios_p4.append((total_s, total_c))\n",
    "            fs_pick[fs_key]['loc'] += 1\n",
    "            gradient_update(fs_key, fs_pick)\n",
    "            optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "    return result_ratios_p4\n",
    "\n",
    "\n",
    "def upper_bound(test_df):\n",
    "    print(\"Upper Bound Approach\")\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    result_ratios_p5 = []\n",
    "    res_df = test_df.query(\"y == 'yes'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        num_succ += num_cust\n",
    "        num_calls = num_calls + (num_cust * x)\n",
    "        result_ratios_p5.append((num_succ, num_calls))\n",
    "    res_df = test_df.query(\"y == 'no'\")\n",
    "    for x in range(1, max_calls + 1):\n",
    "        res_df2 = res_df.query(\"campaign == {0}\".format(x))\n",
    "        num_cust = len(res_df2)\n",
    "        num_calls = num_calls + (num_cust * x)\n",
    "        result_ratios_p5.append((num_succ, num_calls))\n",
    "    return result_ratios_p5\n",
    "\n",
    "    \n",
    "def compute_ratio_all_users(df, train_indicies):\n",
    "    ratio_values = []\n",
    "#     for loc, row in df.iterrows():\n",
    "#         if row['y'] == \"yes\":\n",
    "#             ratio_values.append((loc, div(1, row['campaign'])))\n",
    "#         else:\n",
    "#             ratio_values.append((loc, 0.0))\n",
    "    for val in train_indicies:\n",
    "        row = df.iloc[val]\n",
    "        if row['y'] == \"yes\":\n",
    "            ratio_values.append((val, div(1, row['campaign'])))\n",
    "        else:\n",
    "            ratio_values.append((val, 0.0))\n",
    "    return ratio_values\n",
    "\n",
    "\n",
    "def compute_freq_percentage(mappings):\n",
    "    total = 0\n",
    "    for user_mapping in mappings.keys():\n",
    "        total += mappings[user_mapping]['freq']\n",
    "    for user_mapping in mappings.keys():\n",
    "        mappings[user_mapping]['percentage'] = div(mappings[user_mapping]['freq'], total)\n",
    "    \n",
    "    \n",
    "def new_approach_ratio_grouping(mkt_df_filtered, train_indicies, test_indicies, group_size, age_groupings, balance_groupings):\n",
    "    # Ensuring that we can binary encode any row in our dataset. We also group age and balance values \n",
    "    # from each row into ranges.\n",
    "    print(\"Step 1\")\n",
    "    for loc, row in mkt_df_filtered.iterrows():\n",
    "        mkt_df_filtered.loc[loc, 'age'] = group_age(row, age_groupings)\n",
    "        mkt_df_filtered.loc[loc, 'balance'] = group_balance(row, balance_groupings)\n",
    "    # Build the customers for each group.\n",
    "    print(\"Step 2\")\n",
    "    ratio_arr = compute_ratio_all_users(mkt_df_filtered, train_indicies)\n",
    "    ratio_arr_sorted = sorted(ratio_arr, key=lambda tup: tup[1], reverse = True)\n",
    "    train_size = len(ratio_arr_sorted)\n",
    "    groupings = {}\n",
    "    for loc in range(0, train_size):\n",
    "        group_key = str(int(loc/group_size))\n",
    "        if group_key not in groupings.keys():\n",
    "            groupings[group_key] = {'indicies':[], 'mappings':{}, 'results':None} \n",
    "        groupings[group_key]['indicies'].append(ratio_arr_sorted[loc][0])\n",
    "    print(len(groupings.keys()))\n",
    "    # For each group, we find the unique feature combinations and store them in a list. \n",
    "    # We also store the results - s/c ratio for call numbers from 1-20.\n",
    "    test_calls = {}\n",
    "    print(\"Step 3\")\n",
    "    for group_key in groupings.keys():\n",
    "        users_df = mkt_df_filtered.iloc[groupings[group_key]['indicies']]\n",
    "        mappings = groupings[group_key]['mappings']\n",
    "        for row in users_df.itertuples():\n",
    "            user_mapping = str((row.job, row.marital, row.education, row.default,\n",
    "                               row.housing, row.loan, row.age, row.balance))\n",
    "            if user_mapping not in mappings.keys():\n",
    "                mappings[user_mapping] = {'freq':0, 'percentage':0.0}\n",
    "            else:\n",
    "                mappings[user_mapping]['freq'] += 1\n",
    "        groupings[group_key]['results'] = compute_expected_succ_per_call_rate_feature_set(users_df, 20)\n",
    "        compute_freq_percentage(mappings)\n",
    "        test_calls[group_key] = {'locs_to_call':[], 'overall_rate':groupings[group_key]['results'][19]}\n",
    "    # For the test set, we need to map each user to the most appropriate cluster.\n",
    "    print(\"Step 4\")\n",
    "    for loc in test_indicies:\n",
    "        row = mkt_df_filtered.iloc[loc]\n",
    "        user_mapping = str((row['job'], row['marital'], row['education'], row['default'],\n",
    "                            row['housing'], row['loan'], row['age'], row['balance']))\n",
    "        best_group_key = None\n",
    "        best_ratio = 0.0\n",
    "        for group_key in groupings.keys():\n",
    "            if user_mapping in groupings[group_key]['mappings']:\n",
    "                if groupings[best_group_key]['mappings'][user_mapping]['percentage'] > best_ratio:\n",
    "                    best_group_key = group_key\n",
    "                    best_ratio = groupings[group_key]['mappings'][user_mapping]['percentage']\n",
    "        test_calls[best_group_key]['locs_to_call'].append(loc)\n",
    "    # Call users ... those with the highest ratios are called first.\n",
    "    test_calls_sorted = {k: v for k, v in sorted(test_calls.items(), key=lambda fs: fs[1]['overall_rate'], reverse = True)}\n",
    "    print(\"Step 5\")\n",
    "    num_succ = 0\n",
    "    num_calls = 0\n",
    "    result_ratios = []\n",
    "    for key in test_calls_sorted.keys():\n",
    "        for cust_loc in test_calls_sorted[key]['locs_to_call']:\n",
    "            row = mkt_df_filtered.loc[cust_loc]\n",
    "            if row['y'] == yes:\n",
    "                num_succ += 1\n",
    "            num_calls += row['campaign']\n",
    "        result_ratios.append((num_succ, num_calls))\n",
    "    \n",
    "    return result_ratios, test_calls, groupings\n",
    "        \n",
    "        \n",
    "def clustering_age_balance_grouped_no_ratio(train_df, test_df, min_cluster_size, balance_groupings, age_groupings):\n",
    "    print(\"In P7\")\n",
    "    group_age(train_df, age_groupings)\n",
    "    group_balance(train_df, balance_groupings)\n",
    "    encoder = OneHotEncoder()\n",
    "    train_df_encoded = encoder.fit_transform(train_df).toarray()\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    clusterer.fit(train_df_encoded)\n",
    "    predictions = clusterer.labels_\n",
    "    group_age(test_df, age_groupings)\n",
    "    group_balance(test_df, balance_groupings)\n",
    "    test_df_encoded = encoder.fit_transform(test_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "#     return None, None, (None, None)\n",
    "    return abstraction_new_approach(predictions, feature_names, train_df, test_df, train_df_encoded, test_df_encoded)\n",
    "\n",
    "\n",
    "def clustering_age_balance_grouped_ratio_gradient_ascent(train_df, test_df, min_cluster_size, balance_groupings, age_groupings, num_calls):\n",
    "    print(\"In P8\")\n",
    "    group_age(train_df, age_groupings)\n",
    "    group_balance(train_df, balance_groupings)\n",
    "    train_df, ratio_df = compute_ratio_all_users(train_df)\n",
    "    encoder = OneHotEncoder()\n",
    "    train_df_encoded = encoder.fit_transform(train_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "    train_df_encoded = np.column_stack((train_df_encoded, ratio_df['ratio'].to_numpy()))\n",
    "    feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    clusterer.fit(train_df_encoded)\n",
    "    predictions = clusterer.labels_\n",
    "    test_df = mkt_df_filtered.iloc[test_index]\n",
    "    group_age(test_df, age_groupings)\n",
    "    group_balance(test_df, balance_groupings)\n",
    "    test_df_encoded = encoder.fit_transform(test_df.drop(columns=['y', 'campaign'])).toarray()\n",
    "#     return None, None, (None, None)\n",
    "    return abstraction_new_approach_gradient_ascent(predictions, feature_names, train_df, test_df, train_df_encoded, test_df_encoded, num_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 17)\n",
      "CPU times: user 41.9 ms, sys: 3.03 ms, total: 44.9 ms\n",
      "Wall time: 44.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Code that sets up values to construct all possible feature combinations.\n",
    "\n",
    "# Age query strings.\n",
    "# age_query_strings = ['age < 26','age >= 26 & age <=60','age >60']\n",
    "# age_query_strings = ['age >= 10 & age <= 32', 'age >= 33 & age <= 40', 'age >= 50 & age <= 59', 'age >= 60']\n",
    "age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "\n",
    "# Balance query strings.\n",
    "balance_query_strings = ['balance <= 450',' balance > 450']\n",
    "\n",
    "# Balance query config.\n",
    "balance_ranges_1 = [(-10000, -1), (0, 550), (551, 105000)]\n",
    "balance_ranges_2 = [(-10000, 450), (451, 105000)]\n",
    "\n",
    "# Age query config.\n",
    "age_ranges_1 = [(18,30), (31,40), (41,50), (51,60), (61,100)]\n",
    "age_ranges_2 = [(10, 34), (35, 45), (46, 100)]\n",
    "\n",
    "# Max call number to consider.\n",
    "max_calls = 20\n",
    "\n",
    "# Pull and filter all calls <= 20.\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "mkt_df_filtered = mkt_df_filtered[['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance', 'campaign', 'y']]\n",
    "mkt_df_filtered_kmeans = mkt_df_filtered[['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance']]\n",
    "print(mkt_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mkt_df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mkt_df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Main code ... orchestrates everything!\n",
    "\n",
    "# Splitting dataframe into data and result dataframes.\n",
    "X = mkt_df_filtered.iloc[:,0:len(mkt_df_filtered.columns)-1]\n",
    "y = mkt_df_filtered.iloc[:,-1]\n",
    "\n",
    "# cut_points = [10, 20]\n",
    "\n",
    "cut_points = [20]\n",
    "\n",
    "for cp in cut_points:\n",
    "\n",
    "    for j in range(1,2):\n",
    "        \n",
    "        phase_batch = {}\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        i = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            i += 1\n",
    "            \n",
    "            print(\"At fold number: \", i)\n",
    "\n",
    "            train_df = mkt_df_filtered.iloc[train_index]\n",
    "            test_df = mkt_df_filtered.iloc[test_index]\n",
    "            \n",
    "#             combs_to_consider, fs_pick = construct_feature_combs(train_df)\n",
    "            \n",
    "            \n",
    "#             # Testing Phase 1 -> Baseline test with shuffling of all customers and calling them ..\n",
    "#             result_ratios_p1, num_calls = call_everyone(test_df)\n",
    "\n",
    "\n",
    "#             # Testing Phase 2 -> Order how we call customers - based on the overall s/c ratio ..\n",
    "#             result_ratios_p2 = greedy_approach(combs_to_consider)\n",
    "\n",
    "\n",
    "#             # Testing Phase 4 -> Convex Hull - Gradient Ascent Approach ..\n",
    "#             result_ratios_p4 = convex_hull(fs_pick, num_calls)\n",
    "            \n",
    "            \n",
    "#             # Testing Phase 5 -> If we were godlike and knew all ..\n",
    "#             result_ratios_p5 = upper_bound(test_df)\n",
    "            print(len(train_index))\n",
    "            a,b,c = new_approach_ratio_grouping(mkt_df_filtered, train_index, test_index, 500, age_ranges_2, balance_ranges_2)\n",
    "            \n",
    "            break\n",
    "\n",
    "            # Add all results together for this fold.\n",
    "            \n",
    "            phase_batch_key = str(j) + \"_\" + str(i)\n",
    "            phase_batch[phase_batch_key] = {'p1':result_ratios_p1, 'p2':result_ratios_p2, \n",
    "                                            'p4':result_ratios_p4, 'p5':result_ratios_p5,\n",
    "                                            'p6':result_ratios_p6, 'p7':result_ratios_p7, \n",
    "                                            'p8':result_ratios_p8, 'p9':None}\n",
    "            \n",
    "            \n",
    "# 'all_marital_unknown_res_cp_' + str(cp) +'.json'\n",
    "\n",
    "        with open('ALL_FIX_Ratio_No_RatioGA' + str(cp) +'.json', 'w') as fp:\n",
    "                json.dump(phase_batch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(train_df.drop(columns=['y', 'campaign', 'age', 'balance']))\n",
    "res1 = enc.transform([test_df.drop(columns=['y', 'campaign', 'age', 'balance']).iloc[1]]).toarray()\n",
    "res2 = enc.transform([test_df.drop(columns=['y', 'campaign', 'age', 'balance']).iloc[20]]).toarray()\n",
    "# all_rows_enc = encoder.fit_transform(train_df.drop(columns=['y', 'campaign', 'age', 'balance'])).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(mkt_df_filtered)\n",
    "for i in range(0, num_rows):\n",
    "    row = mkt_df_filtered.iloc[i]\n",
    "    mkt_df_filtered.iloc[i, 6] = group_age(row, age_ranges_2)\n",
    "    mkt_df_filtered.iloc[i, 7] = group_balance(row, balance_ranges_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_index))\n",
    "for loc in test_index:\n",
    "    row = mkt_df_filtered.iloc[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in mkt_df_filtered.itertuples():\n",
    "    y = mkt_df_filtered.loc[row.Index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc, row in mkt_df_filtered.iterrows():\n",
    "    y = row\n",
    "    x = loc\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered.loc[45210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"((1, u'Central Plant 1', u'http://egauge.com/'), (2, u'Central Plant 2', u'http://egauge2.com/'))\"\n",
    "eval(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "results = euclidean_distances(res1, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS BODY OF CODE TO VERIFY RESULTS ... A KFOLD OBJECT MUST EXIST ALREADY.\n",
    "\n",
    "%%time\n",
    "\n",
    "# Main code ... orchestrates everything!\n",
    "\n",
    "# Splitting dataframe into data and result dataframes.\n",
    "X = mkt_df_filtered.iloc[:,0:len(mkt_df_filtered.columns)-1]\n",
    "y = mkt_df_filtered.iloc[:,-1]\n",
    "\n",
    "# cut_points = [10, 20]\n",
    "\n",
    "cut_points = [20]\n",
    "\n",
    "for cp in cut_points:\n",
    "\n",
    "    for j in range(1,2):\n",
    "        \n",
    "        phase_batch = {}\n",
    "        i = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            i += 1\n",
    "            \n",
    "            print(\"At fold number: \", i)\n",
    "\n",
    "            train_df = mkt_df_filtered.iloc[train_index]\n",
    "            test_df = mkt_df_filtered.iloc[test_index]\n",
    "            \n",
    "            combs_to_consider, fs_pick = construct_feature_combs(train_df)\n",
    "            \n",
    "            \n",
    "            # Testing Phase 1 -> Baseline test with shuffling of all customers and calling them ..\n",
    "            result_ratios_p1, num_calls = call_everyone(test_df)\n",
    "\n",
    "\n",
    "            # Testing Phase 2 -> Order how we call customers - based on the overall s/c ratio ..\n",
    "            result_ratios_p2 = greedy_approach(combs_to_consider)\n",
    "\n",
    "\n",
    "            # Testing Phase 4 -> Convex Hull - Gradient Ascent Approach ..\n",
    "            result_ratios_p4 = convex_hull(fs_pick, num_calls)\n",
    "            \n",
    "            \n",
    "            # Testing Phase 5 -> If we were godlike and knew all ..\n",
    "            result_ratios_p5 = upper_bound(test_df)\n",
    "            \n",
    "            \n",
    "            # --- Preparing Data For Clustering ---\n",
    "            # We run any type of clustering and then pass the predictions to the clustering function\n",
    "            # which will compute the results.\n",
    "#             construct_data_clustering(mkt_df_filtered_kmeans)\n",
    "            \n",
    "#             train_df = mkt_df_filtered_kmeans.iloc[train_index]\n",
    "#             test_df = mkt_df_filtered_kmeans.iloc[test_index]\n",
    "\n",
    "#             encoder = OneHotEncoder()\n",
    "#             mkt_df_filtered_kmeans_encoded = encoder.fit_transform(mkt_df_filtered_kmeans).toarray()\n",
    "\n",
    "#             feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "            \n",
    "#             # Testing Phase 6 -> KMeans Clustering\n",
    "            \n",
    "#             num_centroids = 450\n",
    "#             km = KMeans(n_clusters=num_centroids, max_iter = 300)\n",
    "#             fit_params = km.fit(mkt_df_filtered_kmeans_encoded)\n",
    "#             predictions = km.predict(mkt_df_filtered_kmeans_encoded)\n",
    "            \n",
    "#             result_ratios_p6 = clustering(predictions)\n",
    "            \n",
    "            \n",
    "#             # Testing Phase 6 -> KMeans Clustering with Gradient Ascent\n",
    "#             result_ratios_p65 = clustering_gradient_ascent(predictions, num_calls)\n",
    "            \n",
    "#             # Testing Phase 7 -> HDBScan\n",
    "#             clusterer = hdbscan.HDBSCAN(min_cluster_size=50)\n",
    "#             clusterer.fit(mkt_df_filtered_kmeans_encoded)\n",
    "#             predictions = clusterer.labels_\n",
    "            \n",
    "#             result_ratios_p7 = clustering(predictions)\n",
    "            \n",
    "#             # Testing Phase 8 -> HDBScan with Gradient Ascent\n",
    "#             result_ratios_p8 = clustering_gradient_ascent(predictions, num_calls)\n",
    "            \n",
    "#             # Testing Phase 9 -> HDBScan Corrected ...\n",
    "#             construct_data_clustering_ratio(train_df)\n",
    "#             encoder = OneHotEncoder()\n",
    "#             train_df_encoded = encoder.fit_transform(train_df).toarray()\n",
    "#             clusterer = hdbscan.HDBSCAN(min_cluster_size=20)\n",
    "#             clusterer.fit(train_df_encoded)\n",
    "#             predictions = clusterer.labels_\n",
    "#             feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])\n",
    "              \n",
    "            # Testing Phase 10 - New Approach\n",
    "            print(\"P9\")\n",
    "            train_df = mkt_df_filtered.iloc[train_index]\n",
    "            test_df = mkt_df_filtered.iloc[test_index]\n",
    "            result_ratios_p9, groupings, hits = new_approach_train_set_cluster_only(train_df, test_df)\n",
    "            \n",
    "            \n",
    "            print(\"P10\")\n",
    "            train_df = mkt_df_filtered.iloc[train_index]\n",
    "            test_df = mkt_df_filtered.iloc[test_index]\n",
    "            result_ratios_p10, groupings2, hits2 = new_approach_train_set_cluster_only_2(train_df, test_df)\n",
    "\n",
    "#             result_ratios_p1 = None\n",
    "#             result_ratios_p2 = None\n",
    "#             result_ratios_p3 = None\n",
    "#             result_ratios_p4 = None\n",
    "#             result_ratios_p5 = None\n",
    "            result_ratios_p6 = None\n",
    "            result_ratios_p65 = None\n",
    "            result_ratios_p7 = None\n",
    "            result_ratios_p8 = None\n",
    "            # Add all results together for this fold.\n",
    "            \n",
    "            phase_batch_key = str(j) + \"_\" + str(i)\n",
    "            phase_batch[phase_batch_key] = {'p1':result_ratios_p1, 'p2':result_ratios_p2, \n",
    "                                            'p4':result_ratios_p4, 'p5':result_ratios_p5,\n",
    "                                            'p6':result_ratios_p6, 'p65':result_ratios_p65,\n",
    "                                            'p7':result_ratios_p7, 'p8':result_ratios_p8,\n",
    "                                            'p9':result_ratios_p9, 'p10':result_ratios_p10}\n",
    "            \n",
    "            \n",
    "# 'all_marital_unknown_res_cp_' + str(cp) +'.json'\n",
    "\n",
    "        with open('DB_Trial_Ratio_No_Ratio_NEW_VALIDATE' + str(cp) +'.json', 'w') as fp:\n",
    "                json.dump(phase_batch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hits2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result_ratios_p10[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecing together now approach .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = mkt_df_filtered.iloc[train_index]\n",
    "test_df = mkt_df_filtered.iloc[test_index]\n",
    "print(train_df.columns)\n",
    "train_df, ratio_df = construct_data_clustering_ratio(train_df)\n",
    "print(train_df.columns)\n",
    "encoder = OneHotEncoder()\n",
    "train_df_encoded = encoder.fit_transform(train_df.drop(columns=['y', 'campaign', 'age', 'balance'])).toarray()\n",
    "train_df_encoded = np.column_stack((train_df_encoded, ratio_df['ratio'].to_numpy()))\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=20)\n",
    "# clusterer.fit(train_df_encoded)\n",
    "# predictions = clusterer.labels_\n",
    "# feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=25)\n",
    "clusterer.fit(train_df_encoded)\n",
    "predictions = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = {}\n",
    "for index, group in enumerate(predictions):\n",
    "    if str(group) not in groupings.keys():\n",
    "        groupings[str(group)] = {'indicies':[], 'unique_keys':{}, 'results':None}\n",
    "    groupings[str(group)]['indicies'].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = encoder.get_feature_names(['job', 'marital', 'education', 'default', 'housing', 'loan', 'age', 'balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groupings.keys():\n",
    "    for index in groupings[group]['indicies']:\n",
    "        cust_info = train_df.iloc[index]\n",
    "        cust_features = get_features(train_df_encoded[index], feature_names)\n",
    "        if str(cust_features) not in groupings[group]['unique_keys'].keys():\n",
    "            groupings[group]['unique_keys'][str(cust_features)] = {'#_ocurr': 1}\n",
    "        else:\n",
    "            groupings[group]['unique_keys'][str(cust_features)]['#_ocurr'] += 1\n",
    "    results = compute_expected_succ_per_call_rate_feature_set(train_df.iloc[groupings[group]['indicies']], 20)\n",
    "    groupings[group]['results'] = results\n",
    "    print(len(groupings[group]['indicies']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = mkt_df_filtered.iloc[test_index]\n",
    "construct_data_clustering(test_df)\n",
    "test_df_encoded = encoder.fit_transform(test_df.drop(columns=['y', 'campaign'])).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_features = get_features(test_df_encoded[100], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "final_call_arr = []\n",
    "counts = []\n",
    "\n",
    "for index in range(0, 8993, 1):\n",
    "    cust_features = get_features(test_df_encoded[index], feature_names)\n",
    "    hits = 0\n",
    "    key_hits_refs = []\n",
    "    for group in groupings.keys():\n",
    "        if str(cust_features) in groupings[group]['unique_keys'].keys():\n",
    "            hits += 1\n",
    "            key_hits_refs.append((groupings[group]['results'][19]['expected'], str(cust_features)))\n",
    "    if hits == 1:\n",
    "        final_call_arr.append((key_hits_refs[0][0], key_hits_refs[0][1], index))\n",
    "    elif hits >1:\n",
    "        # We need to sort key_hits_refs first in order of highest ratio ..\n",
    "        sorted_key_hits = sorted(key_hits_refs, key=lambda tup: tup[0], reverse = True)\n",
    "        final_call_arr.append((key_hits_refs[0][0], key_hits_refs[0][1], index))\n",
    "    counts.append(hits)\n",
    "\n",
    "sorted_final_call = sorted(final_call_arr, key=lambda tup: tup[0], reverse = True)\n",
    "\n",
    "total_s = 0\n",
    "total_c = 0\n",
    "curr_key = None\n",
    "vals = []\n",
    "prev_key = sorted_final_call[0][1]\n",
    "for rec in sorted_final_call:\n",
    "    index = rec[2]\n",
    "    row = test_df.iloc[index]\n",
    "    if row['y'] == \"yes\":\n",
    "        total_s += 1\n",
    "    total_c += row['campaign']\n",
    "    if rec[1] != prev_key:\n",
    "        vals.append((total_s, total_c))\n",
    "        prev_key = rec[1]\n",
    "\n",
    "print(Counter(counts).keys()) # equals to list(set(words))\n",
    "print(Counter(counts).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_final_call = sorted(final_call_arr, key=lambda tup: tup[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_s = 0\n",
    "total_c = 0\n",
    "curr_key = None\n",
    "vals = []\n",
    "prev_key = sorted_final_call[0][1]\n",
    "for rec in sorted_final_call:\n",
    "    index = rec[2]\n",
    "    row = test_df.iloc[index]\n",
    "    if row['y'] == \"yes\":\n",
    "        total_s += 1\n",
    "    total_c += row['campaign']\n",
    "    if rec[1] != prev_key:\n",
    "        vals.append((total_s, total_c))\n",
    "        prev_key = rec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100, 3900, 100):\n",
    "    print(vals[i][0]/vals[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 4\")\n",
    "result_ratios_p4 = []\n",
    "total_s = 0\n",
    "total_c = 0\n",
    "\n",
    "for key in fs_pick.keys():\n",
    "    gradient_update(key, fs_pick, combs_to_consider)\n",
    "# Sort based on gradient.\n",
    "optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]\n",
    "# Call best feature set, update gradient for this feature set and re-sort all feature sets.\n",
    "# Rinse and repeat!\n",
    "while(total_c <= num_calls - 10):\n",
    "    best_loc = 0\n",
    "    while(best_loc < len(optimal_choices) and optimal_choices[best_loc][1]['finished'] == True):\n",
    "        best_loc += 1\n",
    "    if best_loc == len(optimal_choices):\n",
    "        break\n",
    "    fs_key = optimal_choices[best_loc][0]\n",
    "    fs_data = optimal_choices[best_loc][1]\n",
    "    if fs_data['finished'] == False:\n",
    "        loc = fs_data['loc']\n",
    "        if loc == 0:\n",
    "            call_start = 1\n",
    "            call_end = fs_data['hull_points'][loc] + 1\n",
    "        else:\n",
    "            call_start = fs_data['hull_points'][loc-1] + 2\n",
    "            call_end = fs_data['hull_points'][loc] + 1\n",
    "        for call in range(call_start, call_end + 1, 1):\n",
    "            for loc, row in combs_to_consider[fs_key]['fs_customers'].iterrows():\n",
    "                if row['campaign'] == call:\n",
    "                    total_c += 1\n",
    "                    if row['y'] == \"yes\":\n",
    "                        total_s += 1\n",
    "                elif row['campaign'] > call:\n",
    "                    total_c += 1\n",
    "        result_ratios_p4.append((total_s, total_c))\n",
    "        fs_pick[fs_key]['loc'] += 1\n",
    "        gradient_update(fs_key, fs_pick, combs_to_consider)\n",
    "        optimal_choices = [(k,v) for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['grad'], reverse = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered['default'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_to_call_overall = {k: v for k, v in sorted(combs_to_consider.items(), key=lambda fs: fs[1]['overall_rate'], reverse = True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_indicies = [0, 9, 24, 49, 74 , 99]\n",
    "i = 0\n",
    "for key in persons_to_call_overall.keys():\n",
    "    print(i, key[3])\n",
    "#     if i in key_indicies:\n",
    "#         print(i, persons_to_call_overall[key]['overall_rate'], key[3]) \n",
    "#         print(\"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### i = 0\n",
    "for key in persons_to_call_overall.keys():\n",
    "    print(i)\n",
    "    if i >= 113 and i<=115:\n",
    "#         print(key[3], key[4], key[5])\n",
    "        print(persons_to_call_overall[key]['results'][19])\n",
    "        plot_graph_new(persons_to_call_overall[key]['results'], 20, False, \"Customer Segment \" + str(i+1), \"worst_\"+str(i))     \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_new(persons_to_call_overall[key]['results'], 20, False, \"Customer Segments 116 - 118  \", \"worst_agg\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key in persons_to_call_overall.keys():\n",
    "    print(persons_to_call_overall[key]['results'][19])\n",
    "    print(key[3], key[4], key[5])\n",
    "    if i > 2:\n",
    "        break\n",
    "    plot_graph_new(persons_to_call_overall[key]['results'], 20, False, \"Customer Segment \" + str(i+1), \"best_\"+str(i))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_to_call_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in persons_to_call_overall:\n",
    "    if persons_to_call_overall[k]['n_rows'] == 956:\n",
    "        thi_fs = k\n",
    "        for res in persons_to_call_overall[k]['results']:\n",
    "            print(res)\n",
    "        plot_graph_new(persons_to_call_overall[k]['results'], 20, False, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = construct_hull_points(combs_to_consider[thi_fs]['results'], max_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thi_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ed = ['primary', 'secondary', 'tertiary', 'unknown']\n",
    "metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "print(metric_vals)\n",
    "print(len(all_combs)* len(age_query_strings) * len(balance_query_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 5\")\n",
    "num_succ = 0\n",
    "num_calls = 0\n",
    "\n",
    "result_ratios_p5 = []\n",
    "res_df = test_df.query(\"y == 'yes'\")\n",
    "print(len(res_df))\n",
    "for i in range(1, 21):\n",
    "    res_df2 = res_df.query(\"campaign == {0}\".format(i))\n",
    "    num_cust = len(res_df2)\n",
    "    num_succ += num_cust\n",
    "    num_calls = num_calls + (num_cust * i)\n",
    "    result_ratios_p5.append((num_succ, num_calls))\n",
    "\n",
    "res_df = test_df.query(\"y == 'no'\")\n",
    "print(len(res_df))\n",
    "for i in range(1, 21):\n",
    "    res_df2 = res_df.query(\"campaign == {0}\".format(i))\n",
    "    num_cust = len(res_df2)\n",
    "    num_calls = num_calls + (num_cust * i)\n",
    "    result_ratios_p5.append((num_succ, num_calls))\n",
    "print(result_ratios_p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cmbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ratios_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in optimal_choices:\n",
    "    print(optimal_choices[ch]['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cmbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (('secondary', 'primary'),\n",
    "  ('student', 'retired'),\n",
    "  ('married', 'single', 'divorced'),\n",
    "  ('no',),\n",
    "  ('no', 'yes'),\n",
    "  ('no',),\n",
    "  'balance <= 450',\n",
    "  'age >= 10 & age <= 34')\n",
    "fs_pick[key]['loc'] = 0\n",
    "fs_pick[key]['finished'] = False\n",
    "print(fs_pick[key]['hull_points'])\n",
    "total_s = 0\n",
    "total_c = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fs_pick[key]\n",
    "fs_results = combs_to_consider[key]['results']\n",
    "hull_pts = fs['hull_points']\n",
    "loc = fs['loc']\n",
    "max_loc = fs['max_num_pts']\n",
    "grad = 0.0\n",
    "if loc <= max_loc:\n",
    "    if loc == 0:\n",
    "        grad = div(fs_results[hull_pts[loc]]['succ'], fs_results[hull_pts[loc]]['total_calls'])\n",
    "    else:\n",
    "        grad = div(fs_results[hull_pts[loc]]['succ'] - fs_results[hull_pts[loc-1]]['succ'] , fs_results[hull_pts[loc]]['total_calls'] - fs_results[hull_pts[loc-1]]['total_calls'])\n",
    "    fs_pick[key]['grad'] = grad\n",
    "else:\n",
    "    fs_pick[key]['finished'] = True\n",
    "print(\"At Loc:\", loc)\n",
    "print(grad)\n",
    "\n",
    "\n",
    "fs = fs_pick[key]\n",
    "if fs['finished'] == False:\n",
    "    loc = fs['loc']\n",
    "    if loc == 0:\n",
    "        call_start = 1\n",
    "        call_end = fs['hull_points'][loc] + 1\n",
    "    else:\n",
    "        call_start = fs['hull_points'][loc-1] + 2\n",
    "        call_end = fs['hull_points'][loc] + 1\n",
    "    print(\"Start and end:\", call_start, call_end)\n",
    "    for call in range(call_start, call_end + 1, 1):\n",
    "        for loc, row in combs_to_consider[key]['fs_customers'].iterrows():\n",
    "            if row['campaign'] == call:\n",
    "                total_c += 1\n",
    "                if row['y'] == \"yes\":\n",
    "                    total_s += 1\n",
    "            elif row['campaign'] > call:\n",
    "                total_c += 1\n",
    "    fs_pick[key]['loc'] += 1\n",
    "print(total_s, total_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (('tertiary', 'unknown'), ('student', 'retired'), ('married', 'single', 'divorced'), ('no', 'yes'), ('no', 'yes'), ('yes', 'unknown'), 'balance <= 450', 'age >= 10 & age <= 34')\n",
    "del combs_to_consider[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = (('secondary', 'primary'), ('unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur'), ('married', 'single', 'divorced'), ('no',), ('no', 'yes'), ('yes', 'unknown'), 'balance <= 450', 'age >= 35 & age <= 45')\n",
    "key = (('secondary', 'primary'),\n",
    "  ('student', 'retired'),\n",
    "  ('married', 'single', 'divorced'),\n",
    "  ('no',),\n",
    "  ('no', 'yes'),\n",
    "  ('no',),\n",
    "  'balance <= 450',\n",
    "  'age >= 10 & age <= 34')\n",
    "for index, row in combs_to_consider[key]['fs_customers'].iterrows():\n",
    "    print(row['y'], row['campaign'])\n",
    "print(fs_pick[key])\n",
    "plot_graph_new(combs_to_consider[key]['results'], max_calls, False, \"Expected Ratio per Call\")\n",
    "print(combs_to_consider[key]['results'])\n",
    "res = construct_hull_points(combs_to_consider[fs_key]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_s = 0\n",
    "num_c = 0\n",
    "for i, row in test_df.iterrows():\n",
    "    if row['y'] == \"yes\":\n",
    "        num_s += 1\n",
    "    num_c += row['campaign']\n",
    "print(num_s, num_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_s = 0\n",
    "num_c = 0\n",
    "for key in combs_to_consider.keys():\n",
    "    for i, row in combs_to_consider[key]['fs_customers'].iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            num_s += 1\n",
    "        num_c += row['campaign']\n",
    "print(num_s, num_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cells that follow are used for checking/fine tuning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = 45\n",
    "for age_q in age_query_strings:\n",
    "    if eval(age_q):\n",
    "        print(\"Yes!\")\n",
    "        print(age_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age and balance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age.\n",
    "# all_age_query_strings = ['age >= 10 & age <= 32', 'age >= 33 & age <= 40', 'age >= 50 & age <= 59', 'age >= 60']\n",
    "# all_age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "# all_age_query_strings = ['age < 26','age >= 26 & age <=60','age >60']\n",
    "all_age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "for age_query in all_age_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(age_query)\n",
    "    s = 0\n",
    "    c = 0\n",
    "    for index, row in df_filtered_final.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            s += 1\n",
    "        c += row['campaign']\n",
    "    print(s/c)\n",
    "    print(df_filtered_final.shape)\n",
    "    print(age_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age.\n",
    "all_age_query_strings = ['age >= 10 & age <= 19', 'age >= 20 & age <= 29', 'age >= 30 & age <= 39', 'age >= 40 & age <= 49', 'age >= 50 & age <= 59','age >= 60 & age <= 69', 'age >= 70 & age <= 79', 'age >= 80 & age <= 89', 'age >= 90 & age <= 100']\n",
    "all_age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "# age_query_strings = ['age >= 10 & age <= 33', 'age >= 34 & age <= 45', 'age >= 46']\n",
    "for age_query in all_age_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(age_query)\n",
    "    print(df_filtered_final.shape[0])\n",
    "    print(age_query, len(df_filtered_final), compute_metric(df_filtered_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "all_age_query_tuples = [(10, 20), (20, 30), (30, 40), (50, 60), (60, 70), (70, 80), (80, 90), (90, 100)]\n",
    "ratios, all_age_query_strings = compute_metric_for_each_attribute_range(all_age_query_tuples, mkt_df_filtered, 'age')\n",
    "print(ratios)\n",
    "# find_combinations(all_age_query_strings, ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Balance\n",
    "all_bal_query_strings = ['balance >= -100000 & balance <= -1', 'balance >= 0 & balance < 1000', 'balance >= 1000 & balance < 2000', 'balance >= 2000 & balance < 3000', 'balance >= 3000 & balance < 4000','balance >= 4000 & balance < 5000', 'balance >= 5000 & balance < 6000', 'balance >= 6000 & balance < 7000', 'balance >= 7000 & balance < 8000', 'balance >= 8000 & balance < 9000', 'balance >= 9000 & balance < 10000','balance >= 10000 & balance < 11000', 'balance >= 11000 & balance < 12000', 'balance >= 12000 & balance < 13000', 'balance >= 13000 & balance < 14000', 'balance >= 14000 & balance < 15000', 'balance >= 15000 & balance < 16000', 'balance >= 16000 & balance < 17000','balance >= 17000 & balance < 18000', 'balance >= 18000 & balance < 19000', 'balance >= 19000 & balance < 19000', 'balance >= 20000']\n",
    "all_bal_query_strings = ['balance >= -100000 & balance <= -1', 'balance >= 0 & balance <= 2000', 'balance > 2000 & balance <= 4000', 'balance > 4000 & balance <= 6000', 'balance > 6000 & balance <= 8000', 'balance > 8000 & balance <= 10000', 'balance > 10000 & balance <= 12000', 'balance > 12000 & balance <= 14000', 'balance > 14000 & balance <= 16000', 'balance > 16000 & balance <= 18000', 'balance > 18000 & balance <= 20000' , 'balance >= 20000']\n",
    "all_bal_query_strings = ['balance >= -100000 & balance <= -1', 'balance >= 0 & balance <= 5000', 'balance > 5000 & balance <= 10000', 'balance > 10000 & balance <= 15000','balance > 15000 & balance <= 20000', 'balance >20000']\n",
    "for bal_query in all_bal_query_strings:\n",
    "    df_filtered_final = mkt_df_filtered.query(bal_query)\n",
    "    print(bal_query, len(df_filtered_final), compute_metric(df_filtered_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance\n",
    "all_bal_query_tuples = [(-10000, 0), (0, 250), (250, 500), (500, 750), (750,1000), (1000, 2000), (2000, 3000), (3000, 4000), (4000, 5000), (5000, 6000), (6000, 7000), (8000, 100000)]\n",
    "ratios, all_bal_query_strings = compute_metric_for_each_attribute_range(all_bal_query_tuples, train_df, 'balance')\n",
    "find_combinations(all_bal_query_strings, ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining where to stop regarding the number of calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to determine the maximum number of calls we should stop at!\n",
    "all_ratios_calls = []\n",
    "for i in range(1,57):\n",
    "    query_str = 'campaign == ' + str(i)\n",
    "    call_query_data = mkt_df_filtered.query(query_str)\n",
    "    succ = 0\n",
    "    calls = 0\n",
    "    for lc, rw in call_query_data.iterrows():\n",
    "        if rw['y'] == \"yes\":\n",
    "            succ += 1\n",
    "        calls += rw['campaign']\n",
    "    all_ratios_calls.append(div(succ, calls))\n",
    "for index, value in enumerate(all_ratios_calls):\n",
    "    print(index+1, value)\n",
    "plot_graph_new(all_ratios_calls, 56, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the success rate by optimizing the maximum calls made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_calls_considered = 20\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls_considered)]\n",
    "\n",
    "result_ratios = [0.0 for i in range (1,max_calls_considered+1)]\n",
    "\n",
    "for i in range(1, max_calls_considered+1):\n",
    "    total_calls = 0\n",
    "    total_succ = 0\n",
    "    #query_str = \"campaign <= {0}\".format(i)\n",
    "    #print(query_str)\n",
    "    #df_filtered_campaign = mkt_df_filtered.query(query_str)\n",
    "    for loc, row in mkt_df_filtered.iterrows():\n",
    "        if row['y']  == \"yes\" and row['campaign'] <= i:\n",
    "            total_succ += 1\n",
    "        total_calls += min(i, row['campaign'])\n",
    "    result_ratios[i-1] = div(total_succ , total_calls)\n",
    "    print(i, result_ratios[i-1], total_succ, total_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_new(result_ratios, 20, True, \"Ratio Per Call #\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "mkt_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered['poutcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered_successes = mkt_df_filtered.query(\"poutcome == 'success'\")\n",
    "print(mkt_df_filtered_successes.shape)\n",
    "mkt_df_filtered_successes['previous'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df_filtered_successes['poutcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mkt_df_filtered_successes['campaign'].value_counts(normalize = False)\n",
    "print(res)\n",
    "print(res.values)\n",
    "num_succ = [2561, 1401 , 618, 317, 139, 92, 47, 32, 21, 14, 16, 4, 6, 4, 4, 2, 6, 0, 0 ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_new(num_succ, 20, True, \"Frequency of Contacts Made per Call #\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To conduct further analysis we need to determine:\n",
    "    - The probability that someone was successful for this campaign given that \n",
    "    they were either successful or not for the last campaign.\n",
    "    - Correlate the time spent on the last call and the success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mkt_df_filtered.query(\"y == 'yes'\").shape[0]\n",
    "b = mkt_df_filtered.query(\"poutcome == 'success'\").shape[0]\n",
    "anb = mkt_df_filtered.query(\"y == 'yes' and poutcome == 'success'\").shape[0]\n",
    "print(anb/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4920, 60):\n",
    "    res = mkt_df_filtered.query(\"duration >= {0} and duration <= {1}\".format(str(i-60), str(i)))\n",
    "    print(i, res.shape[0], res.query(\"y == 'yes'\").shape[0])\n",
    "# a = mkt_df_filtered.query(\"duration >= 0 and duration <= 180\").shape[0]\n",
    "# b = mkt_df_filtered.query(\"y == 'yes'\").shape[0]\n",
    "# anb = mkt_df_filtered.query(\"y == 'yes' and duration >= 0 and duration <= 1000\").shape[0]\n",
    "# print(anb/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(mkt_df_filtered['duration'].max()/60)+1\n",
    "int(mkt_df_filtered['duration'].max()/60) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mkt_df_filtered.query(\"y == 'yes' and contact == 'cellular'\").shape[0]\n",
    "b = mkt_df_filtered.query(\"y == 'yes' and contact == 'telephone'\").shape[0]\n",
    "c = mkt_df_filtered.query('y == \"yes\"').shape[0]\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irritability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ_ratios = []\n",
    "for i in range(1,21):\n",
    "    num_calls = 0\n",
    "    num_succ = 0\n",
    "    for loc, row in mkt_df_filtered.iterrows():\n",
    "        if row['campaign'] <= i:\n",
    "            num_calls += row['campaign']\n",
    "            if row['y'] == \"yes\":\n",
    "                num_succ += 1\n",
    "    succ_ratios.append(div(num_succ, num_calls))\n",
    "succ_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range (1,21)], succ_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_all = []\n",
    "successes_all = []\n",
    "for i in range(1,21):\n",
    "    time_taken = 0\n",
    "    num_succ = 0\n",
    "    num_ppl = 0\n",
    "    for loc, row in mkt_df_filtered.iterrows():\n",
    "        num_ppl += 1\n",
    "        if row['campaign'] <= i:\n",
    "            time_taken += int(row['duration'])\n",
    "            if row['y'] == \"yes\":\n",
    "                num_succ += 1\n",
    "    time_taken_all.append(time_taken/num_ppl)\n",
    "    # successes_all.append(num_succ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1,21)], successes_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1,21)], time_taken_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_succ = [0 for i in range(1,21)]\n",
    "new_succ[0] = successes_all[0]\n",
    "for i in range(1,20):\n",
    "    new_succ[i] = successes_all[i] - successes_all[i-1]\n",
    "new_succ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dur = [0 for i in range(1,21)]\n",
    "new_dur[0] = time_taken_all[0]\n",
    "for i in range(1,20):\n",
    "    new_dur[i] = time_taken_all[i] - time_taken_all[i-1]\n",
    "for i in range(0,20):\n",
    "    new_dur[i] /= 60\n",
    "new_dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_all = []\n",
    "successes_all = []\n",
    "\n",
    "for i in range(1,21):\n",
    "    time_taken = 0\n",
    "    num_succ = 0\n",
    "    for loc, row in mkt_df_filtered.iterrows():\n",
    "        if row['campaign'] == i:\n",
    "            time_taken += int(row['duration']/60)\n",
    "            if row['y'] == \"yes\":\n",
    "                num_succ += 1\n",
    "    time_taken_all.append(time_taken)\n",
    "    successes_all.append(num_succ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successes_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(0,20):\n",
    "    res.append(div(successes_all[i],time_taken_all[i]))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1,21)], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "\n",
    "%%time\n",
    "\n",
    "# Main code ... orchestrates everything!\n",
    "\n",
    "# Splitting dataframe into data and result dataframes.\n",
    "X = mkt_df_filtered.iloc[:,0:len(mkt_df_filtered.columns)-1]\n",
    "y = mkt_df_filtered.iloc[:,-1]\n",
    "\n",
    "cut_points = [0]\n",
    "\n",
    "for cp in cut_points:\n",
    "\n",
    "    for j in range(1,2):\n",
    "        \n",
    "        phase_batch = {}\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        i = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            i += 1\n",
    "\n",
    "            train_df = mkt_df_filtered.iloc[train_index]\n",
    "            test_df = mkt_df_filtered.iloc[test_index]\n",
    "\n",
    "            # At this point, we can run computations for the success rate of each sub attribute and join\n",
    "            # the sub-attributes based on the output of k-means.\n",
    "            poss = []\n",
    "\n",
    "            # Education.\n",
    "            all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "            education_cmbs = find_combinations(all_ed, metric_vals)\n",
    "\n",
    "            # Occupation.\n",
    "            all_jobs = ['student', 'retired', 'unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_jobs, train_df, 'job')\n",
    "            job_cmbs = find_combinations(all_jobs, metric_vals)\n",
    "\n",
    "            # Marital.\n",
    "            all_ms = ['married', 'single', 'divorced', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_ms, train_df, 'marital')\n",
    "            marital_cmbs = find_combinations(all_ms, metric_vals)\n",
    "\n",
    "            # Default\n",
    "            all_def = ['no', 'yes', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_def, train_df, 'default')\n",
    "            default_cmbs = find_combinations(all_def, metric_vals)\n",
    "\n",
    "            # Loan\n",
    "            all_ln = ['no', 'yes', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_ln, train_df, 'loan')\n",
    "            loan_cmbs = find_combinations(all_ln, metric_vals)\n",
    "\n",
    "            # Housing\n",
    "            all_hs = ['no', 'yes', 'unknown']\n",
    "            metric_vals = compute_metric_for_each_attribute(all_hs, train_df, 'housing')\n",
    "            housing_cmbs = find_combinations(all_hs, metric_vals)\n",
    "\n",
    "            poss.append(education_cmbs)\n",
    "            poss.append(marital_cmbs)\n",
    "            poss.append(job_cmbs)\n",
    "            poss.append(default_cmbs)\n",
    "            poss.append(loan_cmbs)\n",
    "            poss.append(housing_cmbs)\n",
    "            all_combs = list(itertools.product(*poss))\n",
    "\n",
    "            # print(\"Number of combinations: \", len(all_combs)* len(age_query_strings) * len(balance_query_strings))\n",
    "\n",
    "            # We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "            num_iter = 0\n",
    "            combs_to_consider = {}\n",
    "            fs_pick = {}\n",
    "\n",
    "            # Setting up looping structures to generate all possibilities.\n",
    "            for age_query in age_query_strings:\n",
    "                df_filtered_final = train_df.query(age_query)\n",
    "                for bal_query in balance_query_strings:\n",
    "                    df_filtered_final_2 = df_filtered_final.query(bal_query)\n",
    "                    for comb in all_combs:\n",
    "                        dict_final_query = construct_dict(comb)\n",
    "                        num_iter += 1\n",
    "                        extracted_df = extract_rows_feature_set(df_filtered_final_2, dict_final_query)\n",
    "                        key = (dict_final_query['education'], dict_final_query['job'], \n",
    "                               dict_final_query['marital'], dict_final_query['default'], \n",
    "                               dict_final_query['loan'], dict_final_query['housing'], \n",
    "                               bal_query, age_query)\n",
    "                        n_rows = extracted_df.shape[0]\n",
    "                        if n_rows > cp:\n",
    "                            results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "                            max_loc = compute_optimal_call_no(results)\n",
    "                            if max_loc != -1:\n",
    "                                combs_to_consider[key] = {\n",
    "                                                            'max_loc':max_loc + 1,\n",
    "                                                            'best_rate':results[max_loc]['expected'], \n",
    "                                                            'overall_rate':results[max_calls-1]['expected'], \n",
    "                                                            'n_rows':n_rows, \n",
    "                                                            'results':results,\n",
    "                                                            'fs_customers':None,\n",
    "                                                            'valid': True\n",
    "                                                         }\n",
    "                                fs_pick[key] = {'current_ratio': 0.0, 'call_num':0, 'finished':False}\n",
    "                            else:\n",
    "                                print(\"Invalid FS ! -> \", n_rows)\n",
    "            \n",
    "            for fs_key in combs_to_consider.keys():\n",
    "                fs_customers = find_all_cust_feature_set(fs_key, test_df)\n",
    "                combs_to_consider[fs_key]['fs_customers'] = fs_customers\n",
    "\n",
    "            \n",
    "            # Testing Phase 1: Baseline test with shuffling of customers.\n",
    "\n",
    "            call_check_points = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, 96500, 97000, 97500, 98000, 98500, 99000, 99500, 100000, 100500, 101000, 101500, 102000, 102500, 103000, 103500, 104000, 104500, 105000, 105500, 106000, 106500, 107000, 107500, 108000, 108500, 109000, 109500, 110000, 110500, 111000, 111500, 112000, 112500, 113000, 113500, 114000, 114500, 115000, 115500, 116000, 116500, 117000, 117500, 118000, 118500, 119000, 119500, 120000, 120500, 121000, 121500, 122000, 122500, 123000, 123500, 124000, 124500, 125000, 125500, 126000, 126500, 127000, 127500, 128000, 128500, 129000, 129500, 130000, 130500, 131000, 131500, 132000, 132500, 133000, 133500, 134000, 134500, 135000, 135500, 136000, 136500, 137000, 137500, 138000, 138500, 139000, 139500]\n",
    "            result_ratios_p1 = []\n",
    "            cp_loc = 0\n",
    "            num_succ = 0\n",
    "            num_calls = 0\n",
    "            res = test_df.reindex(np.random.permutation(test_df.index))\n",
    "            for loc, row in res.iterrows():\n",
    "                if num_calls >= call_check_points[cp_loc]:\n",
    "                    cp_loc += 1\n",
    "                    result_ratios_p1.append((num_succ, num_calls))\n",
    "                num_calls += row['campaign']\n",
    "                if row['y'] == \"yes\":\n",
    "                    num_succ += 1\n",
    "            result_ratios_p1.append((num_succ, num_calls))\n",
    "\n",
    "\n",
    "            # Testing Phase 2: Order how we call customer based on the overall s/c ratio.\n",
    "            # In this instance we call everyone.\n",
    "\n",
    "            persons_to_call_overall = {k: v for k, v in sorted(combs_to_consider.items(), key=lambda fs: fs[1]['overall_rate'], reverse = True)}\n",
    "            num_succ = 0\n",
    "            num_calls = 0\n",
    "            result_ratios_p2 = []\n",
    "            # print(type(persons_to_call_overall))\n",
    "            result_ratios = []\n",
    "            for key in persons_to_call_overall.keys():\n",
    "                for loc, cust in persons_to_call_overall[key]['fs_customers'].iterrows():\n",
    "                    num_calls += cust['campaign']\n",
    "                    if cust['y'] == \"yes\":\n",
    "                        num_succ +=1\n",
    "                result_ratios_p2.append((num_succ, num_calls))\n",
    "\n",
    "\n",
    "            # Testing Phase 3: Order how we call customers based on the maximum s/c ratio.\n",
    "            # The maximum s/c ratio is linked to a call number. Use this to determine number of successes.\n",
    "\n",
    "#             persons_to_call_max = {k: v for k, v in sorted(combs_to_consider.items(), key=lambda fs: fs[1]['best_rate'], reverse = True)}\n",
    "#             num_succ = 0\n",
    "#             num_calls = 0\n",
    "#             result_ratios_p3 = []\n",
    "#             # print(type(persons_to_call_max))\n",
    "#             result_ratios_p3 = []\n",
    "#             for key in persons_to_call_max.keys():\n",
    "#                 max_calls_fs = persons_to_call_max[key]['max_loc']\n",
    "#                 for loc, cust in persons_to_call_max[key]['fs_customers'].iterrows():\n",
    "#                     cust_calls = cust['campaign']\n",
    "#                     if cust_calls <= max_calls_fs:\n",
    "#                         num_calls += cust_calls\n",
    "#                         if cust['y'] == \"yes\":\n",
    "#                             num_succ +=1\n",
    "#                     else:\n",
    "#                         num_calls += max_calls_fs\n",
    "#                 result_ratios_p3.append((num_succ, num_calls))\n",
    "\n",
    "\n",
    "            # Testing Phase 4: Order how we call customers from feature sets.\n",
    "            # Use the approach of calculating gradient at each call point and use this to order\n",
    "            # how the calls are made.\n",
    "\n",
    "            result_ratios_p4 = []\n",
    "            total_s = 0\n",
    "            total_c = 0\n",
    "\n",
    "            for k in range(1,21):\n",
    "                for key in combs_to_consider.keys():\n",
    "                    fs_ref = combs_to_consider[key]['results']\n",
    "                    pos = fs_pick[key]['call_num']\n",
    "                    if pos < 20:\n",
    "                        fs_pick[key]['call_num'] += 1\n",
    "                        if pos == 0:\n",
    "                            fs_pick[key]['current_ratio'] = div(fs_ref[pos]['succ'], fs_ref[pos]['total_calls'])\n",
    "                        else:\n",
    "                            fs_pick[key]['current_ratio'] = div((fs_ref[pos]['succ'] - fs_ref[pos-1]['succ']), (fs_ref[pos]['total_calls'] - fs_ref[pos-1]['total_calls']))\n",
    "                    else:\n",
    "                        fs_pick[key]['finished'] = True\n",
    "\n",
    "                optimal_choices = {k: v for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['current_ratio'], reverse = True)}\n",
    "\n",
    "                for key in optimal_choices.keys():\n",
    "                    if optimal_choices[key]['finished'] == False:\n",
    "                        for loc, row in combs_to_consider[key]['fs_customers'].iterrows():\n",
    "                            if row['campaign'] == (optimal_choices[key]['call_num']):\n",
    "                                total_c += 1\n",
    "                                if row['y'] == \"yes\":\n",
    "                                    total_s += 1\n",
    "                            elif row['campaign'] > (optimal_choices[key]['call_num']):\n",
    "                                total_c += 1\n",
    "                        result_ratios_p4.append((total_s, total_c, k))\n",
    "\n",
    "#             result_ratios_p4 = []\n",
    "#             total_s = 0\n",
    "#             total_c = 0\n",
    "\n",
    "#             for k in range(1,21):\n",
    "#                 for key in combs_to_consider.keys():\n",
    "#                     fs_ref = combs_to_consider[key]['results']\n",
    "#                     # Sort of a patch to the results so subtracting would be easier.\n",
    "#                     fs_ref.insert(0, {'succ':0, 'total_calls':0, 'expected':0.0})\n",
    "#                     e_pos = fs_pick[key]['end'] # This should initially be 1\n",
    "#                     s_pos = fs_pick[key]['end'] - 1 # This should initially be 0\n",
    "#                     curr_ratio = 0.0\n",
    "#                     if e_pos < 21:\n",
    "#                         curr_ratio = div((fs_ref[e_pos]['succ'] - fs_ref[s_pos]['succ']), (fs_ref[e_pos]['total_calls'] - fs_ref[s_pos]['total_calls']))\n",
    "#                         if curr_ratio != 0.0:\n",
    "#                             e_pos += 1\n",
    "#                             s_pos += 1\n",
    "#                         else:\n",
    "#                             while e_pos < 21 and curr_ratio == 0.0:\n",
    "#                                 e_pos += 1\n",
    "#                                 curr_ratio = div((fs_ref[e_pos]['succ'] - fs_ref[s_pos]['succ']), (fs_ref[e_pos]['total_calls'] - fs_ref[s_pos]['total_calls']))\n",
    "#                         fs_pick[key]['end'] = e_pos\n",
    "#                         fs_pick[key]['start'] = s_pos\n",
    "#                         fs_pick[key]['current_ratio'] = curr_ratio\n",
    "#                         if e_pos >= 21:\n",
    "#                             fs_pick[key]['finished'] = True\n",
    "#                     else:\n",
    "#                         fs_pick[key]['finished'] = True\n",
    "\n",
    "#                 optimal_choices = {k: v for k, v in sorted(fs_pick.items(), key=lambda val: val[1]['current_ratio'], reverse = True)}\n",
    "# #                 print(optimal_choices)\n",
    "#                 for key in optimal_choices.keys():\n",
    "#                     if optimal_choices[key]['finished'] == False:\n",
    "#                         start = optimal_choices[key]['start']\n",
    "#                         end = optimal_choices[key]['end']\n",
    "#                         for s in range(start, end, 1):\n",
    "#                             for loc, row in combs_to_consider[key]['fs_customers'].iterrows():\n",
    "#                                 if row['campaign'] == s:\n",
    "#                                     total_c += 1\n",
    "#                                     if row['y'] == \"yes\":\n",
    "#                                         total_s += 1\n",
    "#                                 elif row['campaign'] > s:\n",
    "#                                     total_c += 1\n",
    "#                         result_ratios_p4.append((total_s, total_c, k))\n",
    "                    \n",
    "            \n",
    "            # Testing Phase 5: Using a Decision Tree.\n",
    "            # Use the copy of the mkt dataframe for this portion.\n",
    "            \n",
    "#             result_ratios_p5 = []\n",
    "#             total_succ = 0\n",
    "#             total_calls = 0\n",
    "#             cp_loc = 0\n",
    "            \n",
    "#             train_x = train_df_cpy.iloc[:,0:len(train_df_cpy.columns)-1]\n",
    "#             train_y = train_df_cpy.iloc[:,-1]\n",
    "\n",
    "#             test_x = test_df_cpy.iloc[:,0:len(test_df_cpy.columns)-1]\n",
    "#             test_y = test_df_cpy.iloc[:,-1]\n",
    "            \n",
    "#             clf = DecisionTreeClassifier()\n",
    "#             clf = clf.fit(train_x, train_y)\n",
    "#             test_y_pred = clf.predict(test_x)\n",
    "            \n",
    "#             for index, outcome in enumerate(test_y_pred):\n",
    "#                 num_calls_made = int(test_x.iloc[index]['campaign'])\n",
    "#                 actual_result = int(test_y.iloc[index]) == 1\n",
    "#                 if total_calls >= call_check_points[cp_loc]:\n",
    "#                     cp_loc += 1\n",
    "#                     result_ratios_p5.append((total_succ, total_calls))\n",
    "#                 if outcome == 1:\n",
    "#                     total_calls += num_calls_made\n",
    "#                     if actual_result:\n",
    "#                         total_succ += 1\n",
    "#             result_ratios_p5.append((total_succ, total_calls))\n",
    "            \n",
    "            # Final Part .... Put results in dict and append to array.\n",
    "            \n",
    "            phase_batch_key = str(j) + \"_\" + str(i)\n",
    "            phase_batch[phase_batch_key] = {'p1':result_ratios_p1, 'p2':result_ratios_p2, 'p4':result_ratios_p4}\n",
    "            \n",
    "    with open('idk_results_cp_' + str(cp) +'.json', 'w') as fp:\n",
    "        json.dump(phase_batch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cmbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
