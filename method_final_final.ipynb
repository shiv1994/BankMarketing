{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math, itertools\n",
    "import statistics\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "# from ipynb.fs.full.helper_fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def load_file(data_file_path):\n",
    "    data_df = pd.read_csv(data_file_path, delimiter=\";\")\n",
    "    return data_df\n",
    "  \n",
    "    \n",
    "def plot_graph_new(results, max_calls, list_passed, title):\n",
    "    x_pts = [i+1 for i in range(0, max_calls)]\n",
    "    if list_passed:\n",
    "        y_pts = results\n",
    "    else:    \n",
    "        y_pts = [results[i]['expected'] for i in range(0, max_calls)]\n",
    "    plt.title(title)\n",
    "    plt.plot(x_pts, y_pts)\n",
    "    plt.axvline(x=0, color =\"black\")\n",
    "    plt.axhline(y=0, color =\"black\")\n",
    "    plt.xticks(np.arange(1, max_calls+1, 1))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# def plot_graph_both_axes(ratios, no_calls):\n",
    "    \n",
    "\n",
    "def div(a,b):\n",
    "    if int(b) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return a/b\n",
    "    \n",
    "\n",
    "# Used for creating all possible combinations of the features.\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = iterable\n",
    "    return itertools.chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "\n",
    "\n",
    "def construct_dict(feature_comb):\n",
    "    new_dict = {}\n",
    "    new_dict['education'] = convert(feature_comb[0])\n",
    "    new_dict['job'] = convert(feature_comb[2])\n",
    "    new_dict['marital'] = convert(feature_comb[1])\n",
    "    new_dict['default'] = convert(feature_comb[3])\n",
    "    new_dict['loan'] = convert(feature_comb[4])\n",
    "    new_dict['housing'] = convert(feature_comb[5])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# This was the old metric (reward per call rate).\n",
    "# def compute_expected_reward_feature_set_new(fs_df, no_calls_considered):\n",
    "#     expected_values_calls = []\n",
    "#     len_df = len(fs_df)\n",
    "#     for i in range(1, no_calls_considered + 1):\n",
    "#         expected_values_calls.append({'neg_value':0.0, 'pos_value':0.0, 'count':0, 'expected':0.0})\n",
    "#         for index, row in fs_df.iterrows():\n",
    "#             no_calls = row['campaign']\n",
    "#             if no_calls <= i:\n",
    "#                 if row['y'] == \"yes\":\n",
    "#                     expected_values_calls[i-1]['pos_value'] += ((no_calls_considered+1) - no_calls)\n",
    "#                 else:\n",
    "#                     expected_values_calls[i-1]['neg_value'] += (-no_calls)\n",
    "#             else:\n",
    "#                 expected_values_calls[i-1]['neg_value'] += (-i)\n",
    "#             expected_values_calls[i-1]['count'] += 1\n",
    "#     for loc, item in enumerate(expected_values_calls):\n",
    "#         expected_values_calls[loc]['expected'] = (expected_values_calls[loc]['pos_value'] + expected_values_calls[loc]['neg_value'])/len_df\n",
    "#     return expected_values_calls\n",
    "\n",
    "\n",
    "# This is the new metric (success per call rate).\n",
    "def compute_expected_succ_per_call_rate_feature_set(fs_df, no_calls_considered):\n",
    "    expected_values_call_nums = []\n",
    "    for i in range(1, no_calls_considered + 1):\n",
    "        expected_values_call_nums.append({'succ':0, 'total_calls':0, 'expected':0.0})\n",
    "        for index, row in fs_df.iterrows():\n",
    "            no_calls = row['campaign']\n",
    "            if no_calls <= i:\n",
    "                if row['y'] == \"yes\":\n",
    "                    expected_values_call_nums[i-1]['succ'] += 1\n",
    "                expected_values_call_nums[i-1]['total_calls'] += no_calls\n",
    "            else:\n",
    "                expected_values_call_nums[i-1]['total_calls'] += i\n",
    "    for loc, item in enumerate(expected_values_call_nums):\n",
    "        expected_values_call_nums[loc]['expected'] = div(item['succ'], item['total_calls'])\n",
    "    return expected_values_call_nums\n",
    "\n",
    "\n",
    "def compute_optimal_call_no(results):\n",
    "    max_loc = max(range(len(results)), key=lambda index: results[index]['expected'])\n",
    "    if max_loc == 0 and results[max_loc]['expected'] == 0.0:\n",
    "        return -1\n",
    "    return max_loc\n",
    "\n",
    "\n",
    "# Given a dictionary of what attributes comprise a feature set, we can get all rows corresponding to this feature set.\n",
    "def extract_rows_feature_set(fs_df, feature_labels = {'education':['tertiary', 'unknown'], \n",
    "                                                      'job':['management', 'technician', 'blue-collar'], \n",
    "                                                      'marital':['single'], 'default':['no'], \n",
    "                                                      'housing':['no'], 'loan':['no']}):\n",
    "    for key in feature_labels:\n",
    "        feature_labels_query_str = ''\n",
    "        arr = feature_labels[key]\n",
    "        for label in arr:\n",
    "            feature_labels_query_str += (key + ' == \"'+ label + '\" | ')\n",
    "        feature_labels_query_str = feature_labels_query_str[:-3]\n",
    "        fs_df = fs_df.query(feature_labels_query_str)\n",
    "    return fs_df\n",
    "\n",
    "\n",
    "def find_matching_attribute_comb(row_value, all_combs):\n",
    "    query = None\n",
    "    for comb in all_combs:\n",
    "        for item in comb:\n",
    "            if item == row_value:\n",
    "                query = comb\n",
    "    return query\n",
    "\n",
    "\n",
    "def find_matching_attribute_eval_age(row_value, values):\n",
    "    query = None\n",
    "    for index, comb in enumerate(values):\n",
    "        res = comb.format(age = row_value)\n",
    "        if eval(res):\n",
    "            return index\n",
    "\n",
    "\n",
    "def find_matching_attribute_eval_balance(row_value, values):\n",
    "    query = None\n",
    "    for index, comb in enumerate(values):\n",
    "        res = comb.format(balance = row_value)\n",
    "        if eval(res):\n",
    "            return index\n",
    "\n",
    "\n",
    "def compute_metric(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += row['campaign']\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "\n",
    "def compute_metric_2(df):\n",
    "    total_calls = 0\n",
    "    total_successes = 0\n",
    "    for loc, row in df.iterrows():\n",
    "        if row['y'] == \"yes\":\n",
    "            total_successes += 1\n",
    "        total_calls += min(row['campaign'], )\n",
    "    return div(total_successes, total_calls)\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} == '{1}'\".format(attrib, value)\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals\n",
    "\n",
    "\n",
    "def compute_metric_for_each_attribute_range(all_values, df, attrib):\n",
    "    metric_vals = np.zeros(shape=(len(all_values),1))\n",
    "    query_strings = []\n",
    "    for index, value in enumerate(all_values):\n",
    "        v_query = \"{0} >= {1} & {2} < {3}\".format(attrib, value[0], attrib, value[1])\n",
    "        dataset_query = df.query(v_query)\n",
    "        metric_val = compute_metric(dataset_query)\n",
    "        metric_vals[index] = metric_val\n",
    "        query_strings.append(v_query)\n",
    "#         print(v_query, metric_val, dataset_query.shape)\n",
    "    return metric_vals, query_strings\n",
    "\n",
    "\n",
    "def find_combinations(sub_attributes, ratios):\n",
    "    num_iter = len(ratios)\n",
    "    sil_scores = []\n",
    "    # Making use of the K-Means algorithm ... number of centroids are from 2 to n-1.\n",
    "    for clust_num in range(2, num_iter):\n",
    "        kmeans = KMeans(n_clusters = clust_num)\n",
    "        kmeans.fit(ratios.reshape(-1,1))\n",
    "        results = kmeans.labels_\n",
    "        sil_scores.append((silhouette_score(ratios.reshape(-1,1), results, metric='euclidean'), results, clust_num))\n",
    "    # We make use of the silhouette score to determine the ideal number of centroids.\n",
    "    sorted_sil_scores = sorted(sil_scores, key=lambda x: x[0], reverse = True)\n",
    "    # We then use this ideal number of centroids to determine which sub attributes should be aggregated.\n",
    "    joined_sub_attributes = []\n",
    "    for i in range(0, sorted_sil_scores[0][2]):\n",
    "        joined_sub_attributes.append([])\n",
    "    join_list = sorted_sil_scores[0][1]\n",
    "    for index, value in enumerate(join_list):\n",
    "        pos = join_list[index]\n",
    "        joined_sub_attributes[pos].append(sub_attributes[index])\n",
    "    return_joined_sub_attributes = []\n",
    "    for arr in joined_sub_attributes:\n",
    "        similar_els_gp = []\n",
    "        for item in arr:\n",
    "            similar_els_gp.append(str(item))\n",
    "        return_joined_sub_attributes.append(similar_els_gp)\n",
    "#     print(return_joined_sub_attributes)\n",
    "    return return_joined_sub_attributes\n",
    "\n",
    "# The following is the format of the way in which this method should be called.\n",
    "# find_combinations(['a', 'b', 'c', 'd'], np.array([1, 4, 7, 90]), \"job\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Code that sets up values to construct all possible feature combinations.\n",
    "\n",
    "# Age query strings.\n",
    "# age_query_strings = ['age < 26','age >= 26 & age <=60','age >60']\n",
    "# age_query_strings = ['age >= 10 & age <= 32', 'age >= 33 & age <= 40', 'age >= 50 & age <= 59', 'age >= 60']\n",
    "\n",
    "# These strings are used for queries.\n",
    "age_query_strings = ['age >= 10 & age <= 34', 'age >= 35 & age <= 45', 'age >= 46']\n",
    "balance_query_strings = ['balance <= 450','balance > 450']\n",
    "\n",
    "# These strings are used for injecting values into the statement when using eval.\n",
    "age_query_format_strings = ['{age} >= 10 and {age} <= 34', '{age} >= 35 and {age} <= 45', '{age} >= 46']\n",
    "balance_query_format_strings = ['{balance} <= 450','{balance} > 450']\n",
    "\n",
    "# Max call number to consider.\n",
    "max_calls = 20\n",
    "\n",
    "# Pull and filter all calls <= 20.\n",
    "current_dir = os.getcwd()\n",
    "mkt_df = load_file(current_dir + '/bank-full.csv')\n",
    "mkt_df_filtered = mkt_df[(mkt_df['campaign']>=1) & (mkt_df['campaign']<=max_calls)]\n",
    "\n",
    "print(mkt_df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "baseline_ov = []\n",
    "opt_no_res_ov = []\n",
    "opt_res_ov = []\n",
    "\n",
    "train_df = mkt_df_filtered\n",
    "test_df = mkt_df_filtered\n",
    "\n",
    "# At this point, we can run computations for the success rate of each sub attribute and join\n",
    "# the sub-attributes based on the output of k-means.\n",
    "poss = []\n",
    "\n",
    "# Education\n",
    "all_ed = ['tertiary', 'secondary', 'primary', 'unknown']\n",
    "metric_vals = compute_metric_for_each_attribute(all_ed, train_df, 'education')\n",
    "education_cmbs = find_combinations(all_ed, metric_vals)\n",
    "\n",
    "# Occupation\n",
    "all_jobs = ['student', 'retired', 'unemployed', 'admin.', 'management', 'self-employed', 'technician', 'unknown', 'services', 'housemaid', 'blue-collar', 'entrepreneur']\n",
    "metric_vals = compute_metric_for_each_attribute(all_jobs, train_df, 'job')\n",
    "job_cmbs = find_combinations(all_jobs, metric_vals)\n",
    "\n",
    "# Marital\n",
    "all_ms = ['married', 'single', 'divorced', 'unknown']\n",
    "metric_vals = compute_metric_for_each_attribute(all_ms, train_df, 'marital')\n",
    "marital_cmbs = find_combinations(all_ms, metric_vals)\n",
    "\n",
    "# Default\n",
    "all_def = ['no', 'yes', 'unknown']\n",
    "metric_vals = compute_metric_for_each_attribute(all_def, train_df, 'default')\n",
    "default_cmbs = find_combinations(all_def, metric_vals)\n",
    "\n",
    "# Loan\n",
    "all_ln = ['no', 'yes', 'unknown']\n",
    "metric_vals = compute_metric_for_each_attribute(all_ln, train_df, 'loan')\n",
    "loan_cmbs = find_combinations(all_ln, metric_vals)\n",
    "\n",
    "# Housing\n",
    "all_hs = ['no', 'yes', 'unknown']\n",
    "metric_vals = compute_metric_for_each_attribute(all_hs, train_df, 'housing')\n",
    "housing_cmbs = find_combinations(all_hs, metric_vals)\n",
    "\n",
    "poss.append(education_cmbs)\n",
    "poss.append(marital_cmbs)\n",
    "poss.append(job_cmbs)\n",
    "poss.append(default_cmbs)\n",
    "poss.append(loan_cmbs)\n",
    "poss.append(housing_cmbs)\n",
    "all_combs = list(itertools.product(*poss))\n",
    "\n",
    "print(\"Number of combinations: \", len(all_combs)* len(age_query_strings) * len(balance_query_strings))\n",
    "\n",
    "# We can now go ahead and genreate the feature sets based on what was done previously.\n",
    "\n",
    "num_iter = 0\n",
    "combs_to_consider = {}\n",
    "# Setting up looping structures to generate all possibilities.\n",
    "# All that has to be done now is to change 'df_train' to 'X_train'.\n",
    "for age_query in age_query_strings:\n",
    "    df_filtered_final = train_df.query(age_query)\n",
    "    for bal_query in balance_query_strings:\n",
    "        df_filtered_final_2 = df_filtered_final.query(bal_query)\n",
    "        for comb in all_combs:\n",
    "            dict_final_query = construct_dict(comb)\n",
    "            num_iter += 1\n",
    "            extracted_df = extract_rows_feature_set(df_filtered_final_2, dict_final_query)\n",
    "            key = (dict_final_query['education'], dict_final_query['job'], dict_final_query['marital'], dict_final_query['default'], dict_final_query['loan'], dict_final_query['housing'], age_query, bal_query)\n",
    "            n_rows = extracted_df.shape[0]\n",
    "            if n_rows >0:\n",
    "                results = compute_expected_succ_per_call_rate_feature_set(extracted_df, max_calls)\n",
    "                max_loc = compute_optimal_call_no(results)\n",
    "                if max_loc != -1:\n",
    "                    combs_to_consider[key] = {'comb':comb, 'max_loc':max_loc + 1, 'best_rate':results[max_loc]['expected'], 'overall_rate':results[max_calls-1]['expected'], 'n_rows':n_rows}\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Invalid FS !\")\n",
    "\n",
    "all_possible_calls = []\n",
    "\n",
    "num_missed = 0\n",
    "\n",
    "for loc, row in test_df.iterrows():\n",
    "    # Preprocessing step for optimal method.\n",
    "    # We have the exact values for each of the following:\n",
    "    jb_query = convert(find_matching_attribute_comb(str(row['job']), job_cmbs))\n",
    "    mt_query = convert(find_matching_attribute_comb(str(row['marital']), marital_cmbs))\n",
    "    ec_query = convert(find_matching_attribute_comb(str(row['education']), education_cmbs))\n",
    "    house_query = convert(find_matching_attribute_comb(str(row['housing']), housing_cmbs))\n",
    "    loan_query = convert(find_matching_attribute_comb(str(row['loan']), loan_cmbs))\n",
    "    def_query = convert(find_matching_attribute_comb(str(row['default']), default_cmbs))\n",
    "    # These matches are different.\n",
    "    bal_query = balance_query_strings[find_matching_attribute_eval_balance(int(row['balance']), balance_query_format_strings)]\n",
    "    age_query = age_query_strings[find_matching_attribute_eval_age(int(row['age']), age_query_format_strings)]\n",
    "    no_calls = row['campaign']\n",
    "    key_to_find = (ec_query, jb_query, mt_query, def_query, loan_query, house_query, age_query, bal_query)\n",
    "    if key_to_find in combs_to_consider.keys():\n",
    "        fs = combs_to_consider[key_to_find]\n",
    "        all_possible_calls.append((fs['best_rate'], fs['overall_rate'], row['y'], row['campaign'], fs, key_to_find))\n",
    "    else:\n",
    "        num_missed += 1\n",
    "\n",
    "# Baseline with shuffling of customers.\n",
    "call_check_points = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, 96500, 97000, 97500, 98000, 98500, 99000, 99500, 100000, 100500, 101000, 101500, 102000, 102500, 103000, 103500, 104000, 104500, 105000, 105500, 106000, 106500, 107000, 107500, 108000, 108500, 109000, 109500, 110000, 110500, 111000, 111500, 112000, 112500, 113000, 113500, 114000, 114500, 115000, 115500, 116000, 116500, 117000, 117500, 118000, 118500, 119000, 119500, 120000, 120500, 121000, 121500, 122000, 122500, 123000, 123500, 124000, 124500, 125000, 125500, 126000, 126500, 127000, 127500, 128000, 128500, 129000, 129500, 130000, 130500, 131000, 131500, 132000, 132500, 133000, 133500, 134000, 134500, 135000, 135500, 136000, 136500, 137000, 137500, 138000, 138500, 139000, 139500]\n",
    "s_c_ratio_baseline_cp = []\n",
    "num_succ = 0\n",
    "num_calls = 0\n",
    "cp_loc = 0\n",
    "res = test_df.reindex(np.random.permutation(test_df.index))\n",
    "for loc, row in res.iterrows():\n",
    "    if num_calls >= call_check_points[cp_loc]:\n",
    "        cp_loc += 1\n",
    "        s_c_ratio_baseline_cp.append(div(num_succ, num_calls))\n",
    "    num_calls += row['campaign']\n",
    "    if row['y'] == \"yes\":\n",
    "        num_succ += 1\n",
    "s_c_ratio_baseline_cp.append(div(num_succ, num_calls))\n",
    "\n",
    "# Optimal with no restrictions on #calls for each feature set.\n",
    "all_possible_calls_sorted_overall = sorted(all_possible_calls, key = lambda tup: tup[1], reverse = True)\n",
    "s_c_ratio_opt_no_res = []\n",
    "num_calls = 0\n",
    "num_succ = 0\n",
    "prev_fs = all_possible_calls_sorted_overall[0][5]\n",
    "for item in all_possible_calls_sorted_overall:\n",
    "    if prev_fs != item[5]:\n",
    "        s_c_ratio_opt_no_res.append((div(num_succ, num_calls), num_calls))\n",
    "        prev_fs = item[5]\n",
    "    num_calls += item[3]\n",
    "    if item[2] == \"yes\":\n",
    "        num_succ += 1\n",
    "s_c_ratio_opt_no_res.append((div(num_succ, num_calls), num_calls))\n",
    "\n",
    "# Optimal with restrictions on #calls for each feature set.\n",
    "all_possible_calls_sorted_best = sorted(all_possible_calls, key = lambda tup: tup[0], reverse = True)\n",
    "s_c_ratio_opt_res = []\n",
    "num_calls = 0\n",
    "num_succ = 0\n",
    "num_bad = 0\n",
    "saved = 0\n",
    "prev_fs = all_possible_calls_sorted_best[0][5]\n",
    "for item in all_possible_calls_sorted_best:\n",
    "    user_outcome = item[2]\n",
    "    user_calls = item[3]\n",
    "    # max_no_calls_fs = item[4]['max_loc']\n",
    "    max_no_calls_fs = 5\n",
    "    if user_calls >= 6:\n",
    "        saved += 1\n",
    "    if prev_fs != item[5]:\n",
    "        s_c_ratio_opt_res.append((div(num_succ, num_calls), num_calls))\n",
    "        prev_fs = item[5]\n",
    "    if user_outcome == \"yes\" and user_calls <= max_no_calls_fs:\n",
    "        num_succ += 1\n",
    "        num_calls += user_calls\n",
    "    elif user_outcome == \"yes\" and user_calls > max_no_calls_fs:\n",
    "        num_calls += max_no_calls_fs\n",
    "        num_bad += 1\n",
    "    elif user_outcome == \"no\" and user_calls <= max_no_calls_fs:\n",
    "        num_calls += user_calls\n",
    "    elif user_outcome == \"no\" and user_calls > max_no_calls_fs:\n",
    "        num_calls += max_no_calls_fs\n",
    "s_c_ratio_opt_res.append((div(num_succ, num_calls), num_calls))\n",
    "\n",
    "print(\"Potentially Bad: \", num_bad)\n",
    "print(\"Saved excessive calls: \", saved)\n",
    "\n",
    "baseline_ov.append(s_c_ratio_baseline_cp)\n",
    "opt_no_res_ov.append(s_c_ratio_opt_no_res)\n",
    "opt_res_ov.append(s_c_ratio_opt_res)\n",
    "        \n",
    "x = np.array(baseline_ov)\n",
    "y = np.array(opt_no_res_ov)\n",
    "z = np.array(opt_res_ov)\n",
    "\n",
    "np.save(\"baseline_ov.npy\", x)\n",
    "np.save(\"opt_no_res_ov.npy\", y)\n",
    "np.save(\"opt_res_ov.npy\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_c_ratio_opt_res\n",
    "# Ordering all calls with max_calls in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_c_ratio_opt_no_res\n",
    "# Ordering all calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
